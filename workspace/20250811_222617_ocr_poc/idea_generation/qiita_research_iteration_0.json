{
  "articles_found": 20,
  "ideas_generated": 7,
  "insights": {
    "common_technologies": {
      "AWS": 20,
      "Azure": 6,
      "React": 2,
      "Docker": 1,
      "PostgreSQL": 1,
      "MySQL": 1,
      "JavaScript": 1,
      "GCP": 1
    },
    "python_libraries": {
      "Python": 20,
      "pip": 11,
      "PIL": 11,
      "numpy": 10,
      "pandas": 9,
      "cv2": 9,
      "matplotlib": 8,
      "opencv": 8,
      "torch": 5,
      "transformers": 3,
      "Pillow": 3,
      "keras": 3,
      "jupyter": 3,
      "tensorflow": 3,
      "conda": 2,
      "seaborn": 2,
      "scikit-learn": 2,
      "fastapi": 2,
      "gradio": 2,
      "pytorch": 2,
      "google colab": 2,
      "plotly": 1,
      "flask": 1,
      "django": 1,
      "streamlit": 1,
      "anaconda": 1
    },
    "implementation_patterns": [],
    "code_examples": [
      {
        "code": "# 仮想環境の作成\n$ python -m venv langchain_env\n\n# 仮想環境の有効化（Windows）\n$ langchain_env/Scripts/activate",
        "article_title": "PDFデータを活用したLangChainでのRAG構築",
        "article_url": "https://qiita.com/camcam/items/ae9ac4860968389804bd",
        "likes": 15,
        "type": "general"
      },
      {
        "code": "プログラム上に`gpt-3.5-turbo`が記述されているコード（コメントアウトしている）がありますが、もちろんこれを実行するとエラーが出ました。`gpt-3.5-tureb`には画像認識の処理はありませんものね。\n\n# OCR\n次にオープンソースのOCRを利用するためのプログラムを書きます。\n今回は`Tesseract`というライブラリを使うことにしました。Windowsでのインストールができるという事で、それをPythonのプログラムで操作することにしました。\nインストールは、以下の投稿記事を参考にしました。\n\nhttps://qiita.com/henjiganai/items/7a5",
        "article_title": "OCRとOpenAIを比較してみた",
        "article_url": "https://qiita.com/ogi_kimura/items/f6b8e3426349767e8f7b",
        "likes": 4,
        "type": "general"
      },
      {
        "code": "結果、以下のような画像が生成されます。\n\n790467491_880.png\n![790467491_880.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/118c242e-ee59-660d-d48f-84b1b6fb0737.png)\n1_462.png\n![1_462.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/3d6f2aee-8c47-1c38-8a81-6ca2f80d3ab8.png)",
        "article_title": "「シーケンス認識によるAI-OCRモデル」を活用しMNISTから生成した数字列を読み取る",
        "article_url": "https://qiita.com/cosumi77/items/83256a5a59e7784bb894",
        "likes": 3,
        "type": "general"
      },
      {
        "code": "結果がこちら。解像度を与えると若干改善するものの、やっぱり難しいです。てか、ここまである程度うまくいくのやばいな。要素数が少なければあとはうまくやれそうですが、大量にあるときには難しいです。\n\n![kanashii.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/2c5ea18a-cadf-9e3f-1d9c-4ea5979c39fc.png)\n\n### 食べログさんのやつはどうか\n\nhttps://tech-blog.tabelog.com/entry/ai-menu-ocr\n\nこっちはめちゃく",
        "article_title": "確信度を出してくれるOCRを作ってみる！",
        "article_url": "https://qiita.com/sakasegawa/items/c7173648aea662451dbd",
        "likes": 50,
        "type": "general"
      },
      {
        "code": "そのまま実行するとWebカメラの起動までに１分程度かかる。調べたところ以下の2行を追加するとすぐに起動するようになる。(詳細不明)",
        "article_title": "Azure AI Document IntelligenceとAzure Open AIでレシート画像から出費を集計する(第1回:Custom Visionによるレシート抽出)",
        "article_url": "https://qiita.com/iruca22/items/0d40d258faffdcceca11",
        "likes": 2,
        "type": "general"
      },
      {
        "code": "このサンプルコードでは https://github.com/sonoisa/clip-japanese/tree/main/sample_images に用意した16枚の画像を利用します。",
        "article_title": "【日本語CLIP基礎】画像とテキストの類似度計算、画像やテキストの埋め込み計算、類似画像検索",
        "article_url": "https://qiita.com/sonoisa/items/d6db2f130fa9a4ce0c2c",
        "likes": 51,
        "type": "general"
      },
      {
        "code": "@startuml\n<style>\nroot {\n  Padding 0\n  Margin 0\n}\nfooter {\n  FontColor black\n  FontSize 15\n}\n</style>\nfooter ゆずソフト作『千恋＊万花』より。「求肥」の意味を調べたい。\ntitle <img:https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2666576/40a33293-2d6a-fc97-6593-5bd631e046bb.png>\n@enduml",
        "article_title": "PyOCR+Tesseract+画像処理でノベルゲームのテキストを抽出する",
        "article_url": "https://qiita.com/MasKoaTS/items/b82d758b158bb7b50d5e",
        "likes": 8,
        "type": "general"
      },
      {
        "code": "※tesseractやPOPPLERは事前にダウンロード、インストールしておく必要あり。\n\n\n## 各コードごとの解説",
        "article_title": "Markitdownで取れないテキストをOCR機能を抽出しよう！！",
        "article_url": "https://qiita.com/makotodaxi5/items/96c36c3fc09b30761bff",
        "likes": 2,
        "type": "general"
      },
      {
        "code": "pip install --no-index --find-links=pylib [導入するライブラリ名]",
        "article_title": "AI-OCRを自作しました(2025.2)",
        "article_url": "https://qiita.com/jupiter-san/items/d7b3e2a70c8624c43c45",
        "likes": 13,
        "type": "general"
      },
      {
        "code": "このプログラムでは、「config\\user_prompt.txt」で定義したプロンプトに、店名と商品名をつなげて、プロンプトとしてgptモデルへ送信する。\nなお、今回のプロンプトは以下の通りとし、プロンプトで与えられた選択肢から適切なものを<genre>タグ内に記載して返答することを強要する。Pythonコードは応答を受け取ると<genre>タグ内の文字を抽出し、戻り値として返答する。",
        "article_title": "Azure AI Document IntelligenceとAzure Open AIでレシート画像から出費を集計する(第3回:AzureOpenAIでの出費の分類とDB格納)",
        "article_url": "https://qiita.com/iruca22/items/f5cbdda373220b9e04ed",
        "likes": 2,
        "type": "general"
      },
      {
        "code": "このプログラムで注意したところ、苦労したところについて説明します。\n\n##### プロンプト\n請求書の画像データから「会社名」「請求日」「請求金額」「摘要名」を抜き出して、それらをPDFのファイル名に付与することを考えました。\nただ、`ChatGPT`に対してどういうプロンプトにして良いかわかりませんでした。そういう場合は、本人に聞いてみよう～という事で、`ChatGPT`にどのようなプロンプトにして良いかを質問しちゃいました。その結果が以下のプロンプトになります。`{text}`は、請求書画像を基に`ChatGPT`から返却された文章です。このプロンプトでは、その文章をインプットにして再度`",
        "article_title": "電子化対応を支援する！請求書自動整理ツールのご紹介",
        "article_url": "https://qiita.com/ogi_kimura/items/d1578a8bedc53404f8ef",
        "likes": 2,
        "type": "general"
      },
      {
        "code": "query_prompt = f\"\"\"\nあなたは高度な自然言語処理と情報検索の専門家です。企業のESGレポートや統合報告書に関連する質問を、文書検索システムでの検索効率と回答精度を高めるように「答え」が変わらないように質問を変えてください。\n\n## 元の質問\n{question}\n\n## 質問の内容\n1.日本の企業に関する質問です。\n2.各企業が出している資料を参照に答えます。\n3.参照する資料は以下のものです。\n- 統合報告書\n- 統合レポート\n- ステナビリティデータブック\n- \n## 指示\n絶対にハルシネーションを避けてください。\n文章を変えた質問を３つ作成してください。\n元の質問と答え",
        "article_title": "RAGを知って、一ヵ月で「第3回金融データ活用チャレンジ」ゴールドメダル(暫定)取得まで",
        "article_url": "https://qiita.com/tomo418/items/49567b821097081d6918",
        "likes": 21,
        "type": "general"
      },
      {
        "code": "このコードでは、画像を **Base64形式** にエンコードしてCloud Vision APIにリクエストを送信しています。**Base64** とは、バイナリデータ（この場合は画像ファイル）をテキスト形式に変換するエンコーディング方式です。画像ファイルはバイナリデータとして処理されるため、APIに送信する際にはテキスト形式に変換する必要があります。コード内では、`encoded_image = base64.b64encode(image_file.read()).decode(\"utf-8\")` を使って、画像をBase64形式に変換しています。\n\nもしリクエストが成功すれば、以下のよ",
        "article_title": "手書きメモや領収書を自動で整理、OCRとChatGPTで簡単にデータ化する方法",
        "article_url": "https://qiita.com/nishifeoda/items/c1db897df5e53778d297",
        "likes": 3,
        "type": "general"
      }
    ],
    "python_code_examples": [
      {
        "code": "import numpy as np\nimport os\nfrom PIL import Image\n# 個人番号届出書のマイナ部分のみ切り出し\npath = os.getcwd()\n# オリジナルスキャンデータの保存場所\nin_path = os.path.join(path,\"AIocr\",\"myno\",\"ori\")\n# 加工後のデータの保管場所\nout_path = os.path.join(path,\"AIocr\",\"myno\",\"mno\")\n#社員CDが写っている部分だけ抽出\n#  (300, 1375    ) (300+900, 1375    )\n#  (300, 1375+180) (300+900, 1375+180)\ny = 1375\nh = 180\nx = 300\nw = 900\nfor curDir,dirs,files in os.walk(in_path):\n    for file in files:\n        img = Image.open(os.path.join(in_path,file))\n        # グレイスケール変換\n    ",
        "article_title": "AI-OCRを自作しました(2025.2)",
        "article_url": "https://qiita.com/jupiter-san/items/d7b3e2a70c8624c43c45",
        "likes": 13,
        "python_code_score": 8,
        "type": "python_specific"
      },
      {
        "code": "import pytesseract\nfrom PIL import Image\nimport pandas as pd\n\ndef image_to_text(image_path):\n    # 画像を読み込む\n    img = Image.open(image_path)\n    # TesseractでOCRを実行\n    custom_config = r'--oem 1 --psm 6'\n    text = pytesseract.image_to_string(img, config=custom_config, lang='jpn')\n    return text\n\nif __name__ == \"__main__\":\n        image_path = 'C:/Users/ogiki/Desktop/data/大阪ばんざい.jpg'\n        text = image_to_text(image_path)\n        print(text)\n        # ファイル保存\n        csv_path = 'output_ocr.csv'\n",
        "article_title": "OCRとOpenAIを比較してみた",
        "article_url": "https://qiita.com/ogi_kimura/items/f6b8e3426349767e8f7b",
        "likes": 4,
        "python_code_score": 7,
        "type": "python_specific"
      },
      {
        "code": "#実行前に実行フォルダ直下にcreate_imagesフォルダを作成してください\n\n#各種インポート\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport cv2\nimport os\nimport re\nimport glob\n\nfrom pathlib import Path\nfrom collections import Counter\nfrom PIL import Image\n\nfrom keras.datasets import mnist\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n#作成する画像数\ncreate_data_count = 1000\n\n#MNISTの画像サイズ\nmnist_picture_width = 28\nmnist_picture_height = 28\n\n#数値列の最大桁数\nstr_max_length = 10\n\n#作成した画",
        "article_title": "「シーケンス認識によるAI-OCRモデル」を活用しMNISTから生成した数字列を読み取る",
        "article_url": "https://qiita.com/cosumi77/items/83256a5a59e7784bb894",
        "likes": 3,
        "python_code_score": 7,
        "type": "python_specific"
      },
      {
        "code": "#---------------------------------------------------------\n#データ準備\n#---------------------------------------------------------\n\n# Get list of all the images\nimages = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n\n#ファイル名の末尾の識別子を除去し、ラベルとして取得\n# 【変更後】\nlabels = [re.split(\"_[0-9]*.png\",img.split(os.path.sep)[-1])[0] for img in images]  \n# 【変更前】\n# labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n\n# 空白を表す文字sを右に追加する（サンプルコードは固定文字数だが今回のケースは文字数が変動する）\nlabels = [label.ljust(str_ma",
        "article_title": "「シーケンス認識によるAI-OCRモデル」を活用しMNISTから生成した数字列を読み取る",
        "article_url": "https://qiita.com/cosumi77/items/83256a5a59e7784bb894",
        "likes": 3,
        "python_code_score": 7,
        "type": "python_specific"
      },
      {
        "code": "class ExtractedText(BaseModel):\n    text: str = Field(..., description=\"The content of the extracted text.\")\n    coordinates: List[int] = Field(\n        ...,\n        description=(\n            \"Bounding box coordinates represented as a list of two points [x1, y1, x2, y2].\\n\"\n            \"- x1, y1: The top-left corner of the bounding box.\\n\"\n            \"- x2, y2: The bottom-right corner of the bounding box.\"\n        )\n    )\n\n\n\nclass OCRResultWithCoordinates(BaseModel):\n    extracted_texts: List[E",
        "article_title": "確信度を出してくれるOCRを作ってみる！",
        "article_url": "https://qiita.com/sakasegawa/items/c7173648aea662451dbd",
        "likes": 50,
        "python_code_score": 6,
        "type": "python_specific"
      },
      {
        "code": "from pydantic import BaseModel\nfrom typing import List\nimport base64\nfrom collections import Counter\nimport re\nfrom google.api_core.client_options import ClientOptions\nfrom google.cloud import documentai\nimport openai\nimport MeCab\nfrom pdf2image import convert_from_path\nimport google.generativeai as genai\nimport json\n\n\nclass OCRResult(BaseModel):\n    extracted_texts: List[str]\n\n# サンプル画像と対応する正解テキスト\nsample_images = ['data/image1.png', 'data/image2.png', 'data/image3.png', 'data/image4.png', 'data/",
        "article_title": "確信度を出してくれるOCRを作ってみる！",
        "article_url": "https://qiita.com/sakasegawa/items/c7173648aea662451dbd",
        "likes": 50,
        "python_code_score": 6,
        "type": "python_specific"
      },
      {
        "code": "from typing import List, Dict\nfrom pydantic import BaseModel, Field\nimport gradio as gr\nimport base64\nimport cv2\nimport numpy as np\nfrom google.cloud import documentai\nfrom google.api_core.client_options import ClientOptions\nimport openai\nimport google.generativeai as genai\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport Levenshtein\nfrom dataclasses import dataclass, field\n\nclass ExtractedElement(BaseModel):\n    id: int = Field(..., description=\"ID number of the extracted element\"",
        "article_title": "確信度を出してくれるOCRを作ってみる！",
        "article_url": "https://qiita.com/sakasegawa/items/c7173648aea662451dbd",
        "likes": 50,
        "python_code_score": 6,
        "type": "python_specific"
      },
      {
        "code": "import os                                            # OS操作用\nfrom markitdown import MarkItDown                   # 任意テキスト→Markdown\nfrom pdfminer.high_level import extract_text        # PDFテキスト抽出\nfrom pdf2image import convert_from_path             # PDF→画像\nimport pytesseract                                  # OCR\nimport pandas as pd                                 # Excel処理\nfrom docx import Document                           # Word処理\npytesseract.pytesseract.tesseract_cmd = r\"<TESSERACT_PATH>\" # t",
        "article_title": "Markitdownで取れないテキストをOCR機能を抽出しよう！！",
        "article_url": "https://qiita.com/makotodaxi5/items/96c36c3fc09b30761bff",
        "likes": 2,
        "python_code_score": 5,
        "type": "python_specific"
      },
      {
        "code": "import os\nfrom markitdown import MarkItDown\nfrom pdfminer.high_level import extract_text\nfrom pdf2image import convert_from_path\nimport pytesseract\nimport pandas as pd\nfrom docx import Document\n\npytesseract.pytesseract.tesseract_cmd = r\"<TESSERACT_PATH>\"\n\nmd_engine = MarkItDown()",
        "article_title": "Markitdownで取れないテキストをOCR機能を抽出しよう！！",
        "article_url": "https://qiita.com/makotodaxi5/items/96c36c3fc09b30761bff",
        "likes": 2,
        "python_code_score": 5,
        "type": "python_specific"
      },
      {
        "code": "def save_md(path, content):\n    md_path = os.path.splitext(path)[0] + \".md\"\n    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(content)\n    return md_path",
        "article_title": "Markitdownで取れないテキストをOCR機能を抽出しよう！！",
        "article_url": "https://qiita.com/makotodaxi5/items/96c36c3fc09b30761bff",
        "likes": 2,
        "python_code_score": 5,
        "type": "python_specific"
      },
      {
        "code": "import easyocr\nimport cv2\nfrom matplotlib import pyplot as plt\n\n# リーダー初期化（日本語指定）\nreader = easyocr.Reader(['ja'])\n\n# 画像読み込み\nimage = cv2.imread('receipt.jpg')\n\n# OCR実行\nresults = reader.readtext(image)\n\n# 結果表示\nfor (bbox, text, prob) in results:\n    print(f'Text: {text}, Confidence: {prob:.2f}')\n    top_left = tuple(map(int, bbox[0]))\n    bottom_right = tuple(map(int, bbox[2]))\n    cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\nplt.sh",
        "article_title": "猿でもわかるAIプログラミングシリーズ 🐵💻 AIでレシートの文字を読み取ってみた！（OCR入門）",
        "article_url": "https://qiita.com/BNR-Gigi/items/247be5c2f3e5efc452c1",
        "likes": 2,
        "python_code_score": 5,
        "type": "python_specific"
      },
      {
        "code": "def preprocess_image(image_path):\n    image = cv2.imread(image_path)\n    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n    denoised = cv2.fastNlMeansDenoising(gray, h=30)\n    _, thresholded = cv2.threshold(denoised, 0, 255, \n                                 cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    return thresholded\n\nprocessed_image = preprocess_image('receipt.jpg')",
        "article_title": "猿でもわかるAIプログラミングシリーズ 🐵💻 AIでレシートの文字を読み取ってみた！（OCR入門）",
        "article_url": "https://qiita.com/BNR-Gigi/items/247be5c2f3e5efc452c1",
        "likes": 2,
        "python_code_score": 5,
        "type": "python_specific"
      }
    ],
    "article_summaries": [
      {
        "title": "PDFデータを活用したLangChainでのRAG構築",
        "url": "https://qiita.com/camcam/items/ae9ac4860968389804bd",
        "tags": [
          "Python",
          "rag",
          "生成AI",
          "LangChain",
          "LLM"
        ],
        "likes": 15,
        "stocks": 12,
        "quality_score": 25,
        "semantic_similarity": 0.7970733046531677,
        "summary": "# 第1章 はじめに\n\n## 1.1 本記事の概要と目的\n本記事では、大規模言語モデル（LLM）をより効果的に活用する手法として注目されている「RAG（Retrieval-Augmented Generation）」の概要と、Python向けフレームワークであるLangChainを使った実装方法について解説します。特に、PDFデータを外部情報源として扱う具体的な方法を取り上げ、「データ検索と回答生..."
      },
      {
        "title": "OCRとOpenAIを比較してみた",
        "url": "https://qiita.com/ogi_kimura/items/f6b8e3426349767e8f7b",
        "tags": [
          "Python",
          "OCR",
          "OpenAI",
          "ChatGPT",
          "LangChain"
        ],
        "likes": 4,
        "stocks": 4,
        "quality_score": 24,
        "semantic_similarity": 0.8282209038734436,
        "summary": "<img width=A%><img width=100% src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/f920c76e-3c56-75b0-ea7d-4ea0fd880818.png\">\n\n# はじめに\n　情報システム部にいると、「OCRを試してみたい」とか「紙の帳票はやめないが、効率化を図りた..."
      },
      {
        "title": "エンジニアにも知って欲しいChatGPT基本テクニック",
        "url": "https://qiita.com/b-mente/items/93ea3d9a4fc33a76b949",
        "tags": [
          "Python",
          "初心者",
          "AI",
          "ChatGPT",
          "CodeInterpreter"
        ],
        "likes": 1853,
        "stocks": 1991,
        "quality_score": 24,
        "semantic_similarity": 0.7712546586990356,
        "summary": "# はじめに\n\nChatGPTブームがひと段落した感がありますが、周りのエンジニアでChatGPTを活用している姿をあまり見みません。\n\n基本的なテクニックを理解すれば、エンジニアこそChatGPTを活用できると思うので、普段使用しているテクニックをいくつかピックアップして紹介します。\n\n## プロンプトの記載方法\n\n### Markdown記法で指示する\n\n色々なところで紹介されていますが、回答..."
      },
      {
        "title": "「シーケンス認識によるAI-OCRモデル」を活用しMNISTから生成した数字列を読み取る",
        "url": "https://qiita.com/cosumi77/items/83256a5a59e7784bb894",
        "tags": [
          "Python",
          "AI",
          "OCR",
          "MNIST"
        ],
        "likes": 3,
        "stocks": 1,
        "quality_score": 22,
        "semantic_similarity": 0.8348196744918823,
        "summary": "# はじめに\nこんにちは。cosumi77と申します。Qiita初投稿です。\n\n普段はド田舎でSEとしてvb.netの開発に従事しておりますが、PythonやAIについてはつい最近まで「なにそれ？美味しいの？」状態でした。\n本記事は、そんな私がナウい（笑）言語を駆使して課題に取り組みましたので、それを紹介するものとなります。\n\n「こんな~~クソ~~記事をネット上に公開して誰が一体読むのか…？」とい..."
      },
      {
        "title": "確信度を出してくれるOCRを作ってみる！",
        "url": "https://qiita.com/sakasegawa/items/c7173648aea662451dbd",
        "tags": [
          "OCR",
          "LMM",
          "GPT-4o"
        ],
        "likes": 50,
        "stocks": 27,
        "quality_score": 22,
        "semantic_similarity": 0.8220961689949036,
        "summary": "こんにちは！逆瀬川 ( https://x.com/gyakuse ) です！\n\nこのアドベントカレンダーでは生成AIのアプリケーションを実際に作り、どのように作ればいいのか、ということをわかりやすく書いていければと思います。アプリケーションだけではなく、プロダクト開発に必要なモデルの調査方法、training方法、基礎知識等にも触れていければと思います。12月5日の朝にこれを書いていますが、4日..."
      },
      {
        "title": "Azure AI Document IntelligenceとAzure Open AIでレシート画像から出費を集計する(第1回:Custom Visionによるレシート抽出)",
        "url": "https://qiita.com/iruca22/items/0d40d258faffdcceca11",
        "tags": [
          "Python",
          "Azure",
          "AI",
          "OCR",
          "ChatGPT"
        ],
        "likes": 2,
        "stocks": 2,
        "quality_score": 22,
        "semantic_similarity": 0.8110144138336182,
        "summary": "# 1.概要・目的\n## 1.1 目的\n①昨今市中には文字認識(OCR)サービスが多くあるが、Azureベースのシステムへの組み込みも意識して、今回はAzure単体で、レシートから出費をどのように集計できるか試す。\n②集計の際に出費の分類を入力することが手間であるため、Azure OpenAIで自動的に推測させる。\n\n今回は最終的に、以下のようなレシート画像33枚から、自動的にデータをDBに蓄積し..."
      },
      {
        "title": "RAGの鬼門＝PDFの表を突破できるか？ PDF を マークダウンに変換するツール \"Marker\" を使ってみた",
        "url": "https://qiita.com/yuji-arakawa/items/6d0299c505315bc3cdb0",
        "tags": [
          "Python",
          "PDF",
          "AI",
          "rag",
          "LLM"
        ],
        "likes": 28,
        "stocks": 14,
        "quality_score": 22,
        "semantic_similarity": 0.802364706993103,
        "summary": "## TL;DR:\n\n- LLM で PDF から情報を抽出する際、Marker を使ってマークダウンに変換してLLMのコンテキスト情報として渡す方法と pdfminer.six でテキストを抽出して LLM へ渡す方法を比較\n- 表のデータ抽出においては、Marker を使ったマークダウン変換の方が優位性あり\n- ただし、Marker による変換は完璧ではなく、表の構造によっては正しく変換できな..."
      },
      {
        "title": "画像内個人情報を黒消ししてみた(電話番号編)",
        "url": "https://qiita.com/mingchun_zhao/items/3bbcafbd1026380b47eb",
        "tags": [
          "Python",
          "Security",
          "電話番号",
          "QiitaEngineerFesta2022",
          "黒消し"
        ],
        "likes": 13,
        "stocks": 5,
        "quality_score": 22,
        "semantic_similarity": 0.795191764831543,
        "summary": "## はじめに\n\n画像内の個人情報を黒消ししてみます(Pythonで実装)。\n画像内の電話番号と郵便番号を見つけ、黒塗りするシンプルなものです。\n\n※ 画像内個人情報を黒消ししてみた(名前編)は[こちら](https://qiita.com/mingchun_zhao/items/1510da4a01d049c927dd)\n\n## 黒消し結果から\n\n|処理前の画像|処理後の画像|\n|:--:|:-..."
      },
      {
        "title": "【日本語CLIP基礎】画像とテキストの類似度計算、画像やテキストの埋め込み計算、類似画像検索",
        "url": "https://qiita.com/sonoisa/items/d6db2f130fa9a4ce0c2c",
        "tags": [
          "画像処理",
          "自然言語処理",
          "機械学習",
          "DeepLearning",
          "clip"
        ],
        "likes": 51,
        "stocks": 31,
        "quality_score": 22,
        "semantic_similarity": 0.7934056520462036,
        "summary": "# 前置き\n\n本記事は、[日本語CLIPモデル](https://huggingface.co/sonoisa/clip-vit-b-32-japanese-v1)に関するシリーズ記事の2本目です。\n日本語CLIPモデルとは何なのかについては、1本目の記事「[【日本語モデル付き】2022年にマルチモーダル処理をする人にお勧めしたい事前学習済みモデル](https://qiita.com/sonoi..."
      },
      {
        "title": "OBSを使ってゲーム実況画面にスコアなどを表示してみよう（その２）",
        "url": "https://qiita.com/Magiqych/items/1e65b7b884e425af6959",
        "tags": [
          "Python",
          "機械学習",
          "React",
          "アイドルマスター",
          "デレステ"
        ],
        "likes": 5,
        "stocks": 2,
        "quality_score": 22,
        "semantic_similarity": 0.78825843334198,
        "summary": "この記事は[OBSを使ってゲーム実況画面にスコアなどを表示してみよう（その１）](https://qiita.com/Magiqych/items/8546b9968e4616731a8a)の続きです\n\n:::note info\n旧題「デレステ配信オーバーレイに使用した技術と実装」\n:::\n:::note info\n著作権情報　THE IDOLM@STER™& ©Bandai Namco Ente..."
      },
      {
        "title": "PyOCR+Tesseract+画像処理でノベルゲームのテキストを抽出する",
        "url": "https://qiita.com/MasKoaTS/items/b82d758b158bb7b50d5e",
        "tags": [
          "Python",
          "画像処理",
          "tesseract-ocr",
          "pyocr"
        ],
        "likes": 8,
        "stocks": 3,
        "quality_score": 21,
        "semantic_similarity": 0.8368127346038818,
        "summary": "# はじめに\n私が趣味でノベルゲームをプレイするとき、たまに意味を知らない単語や難読単語、実物を画像で調べたくなる単語（作中で話題に上がった料理など）が登場することがあります。\n\nこのようなとき、通常は手動でブラウザを開いて検索ワードを入力するのですが、本記事では、ゆずソフト作のノベルゲーム『千恋＊万花』（[Steam全年齢版リンク](https://store.steampowered.com/..."
      },
      {
        "title": "Markitdownで取れないテキストをOCR機能を抽出しよう！！",
        "url": "https://qiita.com/makotodaxi5/items/96c36c3fc09b30761bff",
        "tags": [
          "Python",
          "tesseract-ocr",
          "MarkItDown"
        ],
        "likes": 2,
        "stocks": 2,
        "quality_score": 21,
        "semantic_similarity": 0.8284538984298706,
        "summary": "## Markitdownは便利だけどたまに使いづらい。\n\nLLMライクに資料を前処理するときにMarkitdownは結構使いやすい。しかし、MarkItDown 単体で PDF から Markdown を生成すると、PDF の内容によってはテキスト抽出が不完全になることがある。特に、テキスト層が存在しない画像 PDF やレイアウトが複雑な表形式、段組み構造では文字の順序が乱れる、空白が消える、文..."
      },
      {
        "title": "AI-OCRを自作しました(2025.2)",
        "url": "https://qiita.com/jupiter-san/items/d7b3e2a70c8624c43c45",
        "tags": [
          "AI",
          "OCR",
          "CNN"
        ],
        "likes": 13,
        "stocks": 12,
        "quality_score": 21,
        "semantic_similarity": 0.8066489100456238,
        "summary": "# 1.　はじめに\n　この記事では、さまざまな制約下にもめげず AI-OCR を自作した過程を記述しています。生成AIで何でも可能なこのご時世にそんなモノ好きはいないでしょうが、もし、仮にAI-OCRを自作される機会がありましたら、参考となりましたら幸いです。\n\n# 2.　背景\n## 2.1.　動機\n　教育訓練講座（SAMURAI ENGINEER　AIデータサイエンスコース）を半年間受講し、機械..."
      },
      {
        "title": "Azure AI Document IntelligenceとAzure Open AIでレシート画像から出費を集計する(第3回:AzureOpenAIでの出費の分類とDB格納)",
        "url": "https://qiita.com/iruca22/items/f5cbdda373220b9e04ed",
        "tags": [
          "Python",
          "Azure",
          "AI",
          "OCR",
          "ChatGPT"
        ],
        "likes": 2,
        "stocks": 0,
        "quality_score": 21,
        "semantic_similarity": 0.804154098033905,
        "summary": "# 0.概要\n①昨今市中には文字認識(OCR)サービスが多くあるが、Azureベースのシステムへの組み込みも意識して、今回はAzure単体で、レシートから出費をどのように集計できるか試す。\n②集計の際に出費の分類を入力することが手間であるため、Azure OpenAIで自動的に推測させる。\n\n第2回では、Document Intelligenceの事前構築済みモデル（レシートモデル）を特に重点的に..."
      },
      {
        "title": "画像内個人情報を黒消ししてみた(名前編)",
        "url": "https://qiita.com/mingchun_zhao/items/1510da4a01d049c927dd",
        "tags": [
          "Python",
          "Security",
          "個人情報",
          "QiitaEngineerFesta2022",
          "黒消し"
        ],
        "likes": 6,
        "stocks": 4,
        "quality_score": 21,
        "semantic_similarity": 0.8012495040893555,
        "summary": "## はじめに\n\n画像内の個人情報を黒消ししてみます(Pythonで実装)。\n画像内の人名や地名を見つけ、黒塗りするシンプルなものです。\n\n※ 画像内個人情報を黒消ししてみた(電話番号編)は[こちら](https://qiita.com/mingchun_zhao/items/3bbcafbd1026380b47eb)\n\n## 黒消し結果から\n\n|処理前の画像|処理後の画像|\n|:--:|:--:..."
      },
      {
        "title": "電子化対応を支援する！請求書自動整理ツールのご紹介",
        "url": "https://qiita.com/ogi_kimura/items/d1578a8bedc53404f8ef",
        "tags": [
          "Python",
          "ExcelVBA",
          "OpenAI",
          "電子帳簿保存法",
          "ChatGPT"
        ],
        "likes": 2,
        "stocks": 5,
        "quality_score": 21,
        "semantic_similarity": 0.7996953725814819,
        "summary": "<img width=A%><img width=100% src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/f920c76e-3c56-75b0-ea7d-4ea0fd880818.png\">\n\n# 最近思うこと\n　2024年1月に電子帳簿保存法が改定され、領収書や請求書を紙ではなく電子データのまま保..."
      },
      {
        "title": "RAGを知って、一ヵ月で「第3回金融データ活用チャレンジ」ゴールドメダル(暫定)取得まで",
        "url": "https://qiita.com/tomo418/items/49567b821097081d6918",
        "tags": [
          "Python",
          "rag",
          "LLM"
        ],
        "likes": 21,
        "stocks": 23,
        "quality_score": 21,
        "semantic_similarity": 0.7900566458702087,
        "summary": "## はじめに\n今回初めてQiitaに投稿します。\n理由はタイトルにも書きましたが、RAGを学んでコンペに参加しましたのでその備忘録と、\n最後ゴールドは取れましたが11位と入賞（10位まで）に入れませんでした。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/4010716/881167b4-cda..."
      },
      {
        "title": "2023年 Python / データ分析関連の人気Qiita記事150選",
        "url": "https://qiita.com/kunishou/items/5e29e06e6669b05dc022",
        "tags": [
          "Python",
          "機械学習",
          "データ分析",
          "AI",
          "生成AI"
        ],
        "likes": 106,
        "stocks": 167,
        "quality_score": 21,
        "semantic_similarity": 0.7610898017883301,
        "summary": "# はじめに\nどうもこんにちは。kunishouです。2023年も残すところ今日、明日のみ。皆さん年の瀬をいかがお過ごしでしょうか？\n\n今年も昨年と同様、仕事も勉強もしなくていい日が数日続き、すでにソワソワしてきました。というわけで毎年恒例の（と言いつつ今年で2回目の） Python / データ分析関連の人気 Qiita 記事 150選を今年も投稿することにしました！本記事では、2023年にQii..."
      },
      {
        "title": "手書きメモや領収書を自動で整理、OCRとChatGPTで簡単にデータ化する方法",
        "url": "https://qiita.com/nishifeoda/items/c1db897df5e53778d297",
        "tags": [
          "Python",
          "C#",
          "OCR",
          "CloudVisionAPI",
          "ChatGPT"
        ],
        "likes": 3,
        "stocks": 3,
        "quality_score": 20,
        "semantic_similarity": 0.8468772768974304,
        "summary": "## 1. はじめに\n今回は、画像化した手書きのメモや領収書をテキスト化し、データとして整理できるシステムの開発方法について紹介します。このシステムを作ろうと思ったきっかけは、先日ある研究者の方と話していた時のことです。\n\nその研究者は、スマホで撮影した手書きのメモや書籍のページ、領収書、請求書、契約書などをPCにたくさん保存しているものの、それらを整理するのが苦手だと言っていました。また、これら..."
      },
      {
        "title": "猿でもわかるAIプログラミングシリーズ 🐵💻 AIでレシートの文字を読み取ってみた！（OCR入門）",
        "url": "https://qiita.com/BNR-Gigi/items/247be5c2f3e5efc452c1",
        "tags": [
          "機械学習",
          "AI",
          "バイナリテック"
        ],
        "likes": 2,
        "stocks": 3,
        "quality_score": 20,
        "semantic_similarity": 0.8403991460800171,
        "summary": "\n## 1. はじめに  \n「毎月の経費精算でレシートの数字をひたすら入力するのが面倒...」  \n「紙の資料をデジタル化したいけど手作業は非効率...」  \n\nこんな悩みを解決するのが**OCR（Optical Character Recognition）技術**です。  \n本記事ではPythonを使い、AIでレシートの文字を自動認識するシステムをゼロから構築します。  \n\n実際に私がプロジェク..."
      }
    ]
  },
  "top_articles": [
    {
      "rendered_body": "<h1 data-sourcepos=\"1:1-1:22\">\n<span id=\"第1章-はじめに\" class=\"fragment\"></span><a href=\"#%E7%AC%AC1%E7%AB%A0-%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\"><i class=\"fa fa-link\"></i></a>第1章 はじめに</h1>\n<h2 data-sourcepos=\"3:1-3:34\">\n<span id=\"11-本記事の概要と目的\" class=\"fragment\"></span><a href=\"#11-%E6%9C%AC%E8%A8%98%E4%BA%8B%E3%81%AE%E6%A6%82%E8%A6%81%E3%81%A8%E7%9B%AE%E7%9A%84\"><i class=\"fa fa-link\"></i></a>1.1 本記事の概要と目的</h2>\n<p data-sourcepos=\"4:1-4:457\">本記事では、大規模言語モデル（LLM）をより効果的に活用する手法として注目されている「RAG（Retrieval-Augmented Generation）」の概要と、Python向けフレームワークであるLangChainを使った実装方法について解説します。特に、PDFデータを外部情報源として扱う具体的な方法を取り上げ、「データ検索と回答生成の流れ」 を順を追って説明します。</p>\n<p data-sourcepos=\"6:1-6:43\">本記事の目的は、次の3点です。</p>\n<ul data-sourcepos=\"7:1-10:0\">\n<li data-sourcepos=\"7:1-7:50\">RAGの基本概念・メリットを理解する</li>\n<li data-sourcepos=\"8:1-8:83\">LangChainを使ったPDFデータの登録・検索・回答生成を実装する</li>\n<li data-sourcepos=\"9:1-10:0\">実装の注意点や精度向上のコツをつかむ</li>\n</ul>\n<p data-sourcepos=\"11:1-11:195\">この記事を参考にしていただくことで、PDFドキュメントを活用したRAG構築のアイデアを形にするためのヒントを得られることを目指しています。</p>\n<p data-sourcepos=\"13:1-13:226\">なお、使用しているコード及びデータは<a href=\"https://github.com/syu-tam/rag\" rel=\"nofollow noopener\" target=\"_blank\">こちらのGitリポジトリ</a>に公開しています。必要に応じてダウンロードやクローンしてご利用ください。</p>\n<h2 data-sourcepos=\"16:1-16:19\">\n<span id=\"12-想定読者\" class=\"fragment\"></span><a href=\"#12-%E6%83%B3%E5%AE%9A%E8%AA%AD%E8%80%85\"><i class=\"fa fa-link\"></i></a>1.2 想定読者</h2>\n<p data-sourcepos=\"17:1-17:63\">本記事は、次のような方を対象にしています。</p>\n<ul data-sourcepos=\"19:1-22:0\">\n<li data-sourcepos=\"19:1-19:65\">大規模言語モデル（LLM）の活用に関心がある方</li>\n<li data-sourcepos=\"20:1-20:62\">LangChainを使ったシステム開発に興味がある方</li>\n<li data-sourcepos=\"21:1-22:0\">PDFデータを活用して情報検索や回答生成を試してみたい方</li>\n</ul>\n<p data-sourcepos=\"23:1-23:186\">Pythonでの開発経験がある程度あり、機械学習や自然言語処理に触れたことがある方であれば、スムーズに進められる内容となっています。</p>\n<h2 data-sourcepos=\"25:1-25:19\">\n<span id=\"13-前提知識\" class=\"fragment\"></span><a href=\"#13-%E5%89%8D%E6%8F%90%E7%9F%A5%E8%AD%98\"><i class=\"fa fa-link\"></i></a>1.3 前提知識</h2>\n<ul data-sourcepos=\"27:1-36:0\">\n<li data-sourcepos=\"27:1-30:0\">\n<p data-sourcepos=\"27:3-27:17\">Pythonの基礎</p>\n<ul data-sourcepos=\"28:5-30:0\">\n<li data-sourcepos=\"28:5-28:159\">Pythonスクリプトの実行、仮想環境の作成（venvやcondaなど）、pipによるライブラリのインストールを理解している。</li>\n<li data-sourcepos=\"29:5-30:0\">変数、関数、リスト、辞書などの基本的な文法を理解している</li>\n</ul>\n</li>\n<li data-sourcepos=\"31:1-33:0\">\n<p data-sourcepos=\"31:3-31:42\">LLM（大規模言語モデル)の概要</p>\n<ul data-sourcepos=\"32:5-33:0\">\n<li data-sourcepos=\"32:5-33:0\">GPTやBERTなどのモデルに関して、テキスト生成の仕組みをおおまかに把握している。</li>\n</ul>\n</li>\n<li data-sourcepos=\"34:1-36:0\">\n<p data-sourcepos=\"34:3-34:35\">Embeddings(埋め込み)の概念</p>\n<ul data-sourcepos=\"35:5-36:0\">\n<li data-sourcepos=\"35:5-36:0\">単語や文をベクトル空間に埋め込む手法（Word2VecやBERTのような考え方）に馴染みがある。</li>\n</ul>\n</li>\n</ul>\n<p data-sourcepos=\"37:1-37:126\">もし不明点があれば、随時公式ドキュメントや他の入門記事を参照いただくとスムーズです。</p>\n<p data-sourcepos=\"39:1-39:124\">また、以下の2冊の書籍はLLMについて網羅的にまとめられており、非常に参考になりました。</p>\n<ul data-sourcepos=\"40:1-42:0\">\n<li data-sourcepos=\"40:1-40:981\"><a href=\"https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297136333/ref=sr_1_2_sspa?adgrpid=156801342469&amp;dib=eyJ2IjoiMSJ9.iUa4odO5qgcS966iULz3HtcTc2uqQu0lVWd7hosPI6MA14Y-B61aYYPUS92JRkE7ne1PM9ASU07HNCHOLA7b1AsP5nf8Gh2K4u3cUCYWpGMiXa29adm6yymKO0P-1WR6VnXtocbGKS0d8INl1SPdfa5u1aOdbFvoXbp9E2Sut-FgGxy0C62OoSxk-c-gySknbxhccbn5FJI_YueYoZpgzhHgaRPkZz1fPt5my_oHa6Kl79IQQ_j4gJ8CdY1RLlSIBn-j8TDYLwQsYIgQzSgEe6fwgmFjZlM5VrBO_srMSL4.vLuK9aFlV34yjO3H2Gtu1sQMjDi3V6hnVZJjQABhgfY&amp;dib_tag=se&amp;gad_source=1&amp;hvadid=685864338618&amp;hvdev=c&amp;hvlocphy=9166121&amp;hvnetw=g&amp;hvqmt=e&amp;hvrand=18040040681330066797&amp;hvtargid=kwd-2270348275400&amp;hydadcr=1792_13657028&amp;jp-ad-ap=0&amp;keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1+%E8%A8%80%E8%AA%9E+%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&amp;mcid=753301ec7f8030b2bd433cf43109b8ff&amp;qid=1738131408&amp;sr=8-2-spons&amp;sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&amp;psc=1\" rel=\"nofollow noopener\" target=\"_blank\">大規模言語モデル入門</a></li>\n<li data-sourcepos=\"41:1-42:0\"><a href=\"https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80%E2%85%A1%E3%80%9C%E7%94%9F%E6%88%90%E5%9E%8BLLM%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%A8%E8%A9%95%E4%BE%A1-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297143933/ref=sr_1_3_sspa?adgrpid=156801342469&amp;dib=eyJ2IjoiMSJ9.iUa4odO5qgcS966iULz3HtcTc2uqQu0lVWd7hosPI6MA14Y-B61aYYPUS92JRkE7ne1PM9ASU07HNCHOLA7b1AsP5nf8Gh2K4u3cUCYWpGMiXa29adm6yymKO0P-1WR6VnXtocbGKS0d8INl1SPdfa5u1aOdbFvoXbp9E2Sut-FgGxy0C62OoSxk-c-gySknbxhccbn5FJI_YueYoZpgzhHgaRPkZz1fPt5my_oHa6Kl79IQQ_j4gJ8CdY1RLlSIBn-j8TDYLwQsYIgQzSgEe6fwgmFjZlM5VrBO_srMSL4.vLuK9aFlV34yjO3H2Gtu1sQMjDi3V6hnVZJjQABhgfY&amp;dib_tag=se&amp;gad_source=1&amp;hvadid=685864338618&amp;hvdev=c&amp;hvlocphy=9166121&amp;hvnetw=g&amp;hvqmt=e&amp;hvrand=18040040681330066797&amp;hvtargid=kwd-2270348275400&amp;hydadcr=1792_13657028&amp;jp-ad-ap=0&amp;keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1+%E8%A8%80%E8%AA%9E+%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&amp;mcid=753301ec7f8030b2bd433cf43109b8ff&amp;qid=1738131408&amp;sr=8-3-spons&amp;sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&amp;psc=1\" rel=\"nofollow noopener\" target=\"_blank\">大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価</a></li>\n</ul>\n<h1 data-sourcepos=\"43:1-43:51\">\n<span id=\"第2章-ragretrieval-augmented-generationとは\" class=\"fragment\"></span><a href=\"#%E7%AC%AC2%E7%AB%A0-ragretrieval-augmented-generation%E3%81%A8%E3%81%AF\"><i class=\"fa fa-link\"></i></a>第2章 RAG(Retrieval-Augmented Generation)とは</h1>\n<h2 data-sourcepos=\"44:1-44:19\">\n<span id=\"21-ragの概要\" class=\"fragment\"></span><a href=\"#21-rag%E3%81%AE%E6%A6%82%E8%A6%81\"><i class=\"fa fa-link\"></i></a>2.1 RAGの概要</h2>\n<p data-sourcepos=\"45:1-46:192\">RAG (Retrieval-Augmented Generation) は、検索拡張生成と訳される技術で、近年注目を集めている生成AIの応用手法の一つです。<br>\n生成AI、特に大規模言語モデル (LLM) は、膨大な知識を学習していますが、その知識は学習データに含まれる情報に基づいています。そのため、</p>\n<ul data-sourcepos=\"47:1-50:0\">\n<li data-sourcepos=\"47:1-47:35\">最新情報に対応できない</li>\n<li data-sourcepos=\"48:1-48:71\">専門的な知識や特定のドメイン知識が不足している</li>\n<li data-sourcepos=\"49:1-50:0\">事実に基づかない内容 (ハルシネーション) を生成してしまう</li>\n</ul>\n<p data-sourcepos=\"51:1-51:39\">といった課題がありました。</p>\n<p data-sourcepos=\"53:1-54:30\">RAGは、これらの課題を克服するために、LLMが外部の知識源を参照しながら回答を生成する仕組みを取り入れています。具体的には、質問内容に関連する情報を検索し、その情報をLLMに与えることで、より正確で信頼性の高い回答生成を可能にします。<br>\nRAGを活用することで、</p>\n<ul data-sourcepos=\"56:1-59:0\">\n<li data-sourcepos=\"56:1-56:44\">常に最新の情報に基づいた回答</li>\n<li data-sourcepos=\"57:1-57:77\">専門知識や社内データなど、特定の知識に基づいた回答</li>\n<li data-sourcepos=\"58:1-59:0\">根拠のある、ハルシネーションを抑制した回答</li>\n</ul>\n<p data-sourcepos=\"60:1-60:45\">を生成することが期待できます。</p>\n<h2 data-sourcepos=\"62:1-62:22\">\n<span id=\"22-ragの仕組み\" class=\"fragment\"></span><a href=\"#22-rag%E3%81%AE%E4%BB%95%E7%B5%84%E3%81%BF\"><i class=\"fa fa-link\"></i></a>2.2 RAGの仕組み</h2>\n<p data-sourcepos=\"63:1-63:192\">RAGの仕組みは、大きく分けて 「検索 (Retrieval)」（下図①から③） と 「生成 (Generation)」（下図④から⑥） の2つのステップから構成されます。</p>\n<p data-sourcepos=\"65:1-65:211\">参考：<a href=\"https://www.dir.co.jp/world/entry/solution/rag\" rel=\"nofollow noopener\" target=\"_blank\">生成AIのビジネス活用で注目されるRAG（検索拡張生成）とは？ - 仕組みや活用例、精度向上のノウハウなどを紹介</a></p>\n<p data-sourcepos=\"67:1-67:131\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3985295%2Fa0ad0a94-cb31-021a-c8a3-c7aed48f1676.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=2d80414a473c789794a7e2d3be46b234\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3985295%2Fa0ad0a94-cb31-021a-c8a3-c7aed48f1676.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=2d80414a473c789794a7e2d3be46b234\" alt=\"20240723092206.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3985295%2Fa0ad0a94-cb31-021a-c8a3-c7aed48f1676.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=85dbf1b915e9ad7889efb494ca8a22f7 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3985295/a0ad0a94-cb31-021a-c8a3-c7aed48f1676.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"69:1-69:41\"><strong>＜検索 (Retrieval) ステップ＞</strong></p>\n<p data-sourcepos=\"71:1-71:218\">RAGシステムは、まず ユーザーからの質問（上図①) を受け付けます。すると、システムは質問内容に関連する情報を 外部の知識源 から探し出す検索を行います。</p>\n<p data-sourcepos=\"73:1-73:245\">本記事では、知識源として PDFデータ を活用します。PDFデータは、あらかじめ ベクトルデータベース という、意味に基づいた検索 が得意な特殊なデータベースに登録しておきます。</p>\n<p data-sourcepos=\"75:1-75:67\">検索ステップでは、以下の3つの処理を行います。</p>\n<p data-sourcepos=\"77:1-79:147\"><strong>1. 質問文をベクトル化(上図①)：</strong> 質問文を 埋め込みモデル (例：OpenAI Embeddings、HuggingFace Transformers) というAIモデルで、意味 を捉えた数値データ (ベクトル) に変換します。<br>\n<strong>2. ベクトル検索（上図②)：</strong> ベクトル化された質問文と、ベクトルデータベース(例：FAISS、Chroma、Pinecone)に登録されたPDFドキュメントのベクトルを比較し、意味的に近い ドキュメントを検索します。<br>\n<strong>3. 関連情報を取得（上図③)：</strong> 検索されたドキュメントの中から、質問に関連性の高い箇所を抽出します。</p>\n<hr data-sourcepos=\"81:1-81:3\">\n<p data-sourcepos=\"82:1-82:42\"><strong>＜生成 (Generation) ステップ＞</strong></p>\n<p data-sourcepos=\"84:1-84:108\">検索ステップで取得した関連情報と、質問文を組み合わせて、LLMに入力します。</p>\n<p data-sourcepos=\"86:1-86:331\">LLMは、与えられた情報 (関連情報と質問文) をもとに、質問に対する回答文を生成します。この際、LLMは検索された関連情報を参照しながら回答を生成するため、外部知識に基づいた、より正確で根拠のある回答を生成することが可能になります。</p>\n<p data-sourcepos=\"88:1-88:67\">生成ステップでは、以下の2つの処理を行います。</p>\n<p data-sourcepos=\"90:1-91:175\"><strong>1. プロンプト作成(上図④)：</strong> 質問文と検索された関連情報を組み合わせ、LLMへの指示文 (プロンプト) を作成します。<br>\n<strong>2. 回答生成と出力(上図⑤＆⑥)：</strong> 作成されたプロンプトをLLMに入力し、回答文を生成させた後。ユーザーに解答を表示します。</p>\n<p data-sourcepos=\"93:1-93:262\">このように、RAGは 「検索」 と 「生成」 のステップを組み合わせることで、LLMが持つ生成能力と、外部知識源の情報を効果的に統合し、より高度な質問応答システムを構築するための技術です。</p>\n<h1 data-sourcepos=\"95:1-95:28\">\n<span id=\"第3章-langchainとは\" class=\"fragment\"></span><a href=\"#%E7%AC%AC3%E7%AB%A0-langchain%E3%81%A8%E3%81%AF\"><i class=\"fa fa-link\"></i></a>第3章 LangChainとは　</h1>\n<h2 data-sourcepos=\"96:1-96:25\">\n<span id=\"31-langchainの概要\" class=\"fragment\"></span><a href=\"#31-langchain%E3%81%AE%E6%A6%82%E8%A6%81\"><i class=\"fa fa-link\"></i></a>3.1 LangChainの概要</h2>\n<p data-sourcepos=\"97:1-97:396\">LangChainは、大規模言語モデル（LLM）の活用をより効果的にするためのPythonライブラリおよびフレームワークです。このフレームワークは、テキスト生成、データ検索、エージェントの管理といったタスクを柔軟に組み合わせて、さまざまなアプリケーションを構築できるよう設計されています。</p>\n<p data-sourcepos=\"99:1-99:1228\">Langchainについての詳細な実装は、<a href=\"https://www.amazon.co.jp/LangChain%E5%AE%8C%E5%85%A8%E5%85%A5%E9%96%80-%E7%94%9F%E6%88%90AI%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E9%96%8B%E7%99%BA%E3%81%8C%E3%81%AF%E3%81%8B%E3%81%A9%E3%82%8B%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%93%8D%E3%82%8A%E6%96%B9-%E7%94%B0%E6%9D%91-%E6%82%A0-ebook/dp/B0CL49K74N/ref=sr_1_3?adgrpid=155650325217&amp;dib=eyJ2IjoiMSJ9.cb2DSolyrUa1Vyx5yimjomJ1vXv1THhF84w7hqgn1EWm79jYA-dlRkZFRriUBiDcCN0wWd_8FlJQUoA6PABkQc8jDeAvpRCy2hEvYb2D6jBtphdV55ES3cLn7HRE7rsHByWpT0fGiXMms5XqNDkAr-dYdd51-7TIYV5vpNiYMbvDPzFYWjjsuDWOWashvkcVtj8hymSMKIBIuwNySVfURa7akWg3i8r4CMMLMMrF0Rk.4rI-motw7xKRBs6batXzXS7BUIJgQW0EZ3LCP-EeBmo&amp;dib_tag=se&amp;gad_source=1&amp;hvadid=687716088906&amp;hvdev=c&amp;hvlocphy=9166121&amp;hvnetw=g&amp;hvqmt=e&amp;hvrand=3850225481485846046&amp;hvtargid=kwd-2278701652466&amp;hydadcr=27486_14701136&amp;jp-ad-ap=0&amp;keywords=lang+chain+%E6%9C%AC&amp;mcid=df081cd160633ae2851a77addccc61f5&amp;qid=1738130620&amp;sr=8-3\" rel=\"nofollow noopener\" target=\"_blank\">LangChain完全入門　生成AIアプリケーション開発がはかどる大規模言語モデルの操り方</a>の本で非常にわかりやすくまとめられており、参考になりました。</p>\n<h2 data-sourcepos=\"101:1-101:25\">\n<span id=\"32-langchainの特徴\" class=\"fragment\"></span><a href=\"#32-langchain%E3%81%AE%E7%89%B9%E5%BE%B4\"><i class=\"fa fa-link\"></i></a>3.2 LangChainの特徴</h2>\n<p data-sourcepos=\"102:1-102:60\">LangChainは、次のような特徴を持っています。</p>\n<p data-sourcepos=\"104:1-104:43\"><strong>1. LLMを基盤にしたタスク管理</strong></p>\n<ul data-sourcepos=\"105:1-107:0\">\n<li data-sourcepos=\"105:1-107:0\">テキスト生成や質問応答だけでなく、検索やデータ操作を含む高度なワークフローを構築できます。</li>\n</ul>\n<p data-sourcepos=\"108:1-108:25\"><strong>2. モジュール性</strong></p>\n<ul data-sourcepos=\"109:1-110:0\">\n<li data-sourcepos=\"109:1-110:0\">各コンポーネント（例：データ読み込み、埋め込み生成、検索処理）を独立して管理できるため、目的に応じた柔軟な構成が可能です。</li>\n</ul>\n<p data-sourcepos=\"111:1-111:43\"><strong>3. 豊富なインテグレーション</strong></p>\n<ul data-sourcepos=\"112:1-113:0\">\n<li data-sourcepos=\"112:1-113:0\">OpenAIやHugging Faceのモデルだけでなく、FAISSやChromaなどのベクトルデータベース、PDFリーダーなど、さまざまな外部ツールと連携可能です。</li>\n</ul>\n<p data-sourcepos=\"114:1-114:40\"><strong>4. エージェントのサポート</strong></p>\n<ul data-sourcepos=\"115:1-116:0\">\n<li data-sourcepos=\"115:1-116:0\">「エージェント」と呼ばれる機能を活用することで、ユーザーの指示に基づいて動的にタスクを切り替えたり、複数の機能を組み合わせて実行できます。</li>\n</ul>\n<h2 data-sourcepos=\"117:1-117:25\">\n<span id=\"33-ユースケース\" class=\"fragment\"></span><a href=\"#33-%E3%83%A6%E3%83%BC%E3%82%B9%E3%82%B1%E3%83%BC%E3%82%B9\"><i class=\"fa fa-link\"></i></a>3.3 ユースケース</h2>\n<p data-sourcepos=\"118:1-118:72\">LangChainは以下のようなシナリオで活用されています。</p>\n<ul data-sourcepos=\"120:1-132:0\">\n<li data-sourcepos=\"120:1-122:0\">\n<p data-sourcepos=\"120:3-121:86\"><strong>質問応答システム</strong><br>\n外部データを元に、LLMを活用した精度の高いQAシステムを構築。</p>\n</li>\n<li data-sourcepos=\"123:1-125:0\">\n<p data-sourcepos=\"123:3-124:87\"><strong>ドキュメント要約</strong><br>\n長い文書をチャンク化して要約を生成するワークフローを実装。</p>\n</li>\n<li data-sourcepos=\"126:1-128:0\">\n<p data-sourcepos=\"126:3-127:90\"><strong>カスタムチャットボット</strong><br>\n特定のドメインデータを利用した対話型アプリケーションの開発。</p>\n</li>\n<li data-sourcepos=\"129:1-132:0\">\n<p data-sourcepos=\"129:3-130:99\"><strong>データ統合ツール</strong><br>\n様々なデータソースを取り込み、ベクトル検索とLLMを組み合わせた分析。</p>\n</li>\n</ul>\n<h1 data-sourcepos=\"133:1-133:72\">\n<span id=\"第4章-構築手順---langchain--pdfデータ-rag実装ステップ\" class=\"fragment\"></span><a href=\"#%E7%AC%AC4%E7%AB%A0-%E6%A7%8B%E7%AF%89%E6%89%8B%E9%A0%86---langchain--pdf%E3%83%87%E3%83%BC%E3%82%BF-rag%E5%AE%9F%E8%A3%85%E3%82%B9%E3%83%86%E3%83%83%E3%83%97\"><i class=\"fa fa-link\"></i></a>第4章 構築手順 - LangChain × PDFデータ RAG実装ステップ</h1>\n<h2 data-sourcepos=\"134:1-134:19\">\n<span id=\"41-環境構築\" class=\"fragment\"></span><a href=\"#41-%E7%92%B0%E5%A2%83%E6%A7%8B%E7%AF%89\"><i class=\"fa fa-link\"></i></a>4.1 環境構築</h2>\n<p data-sourcepos=\"135:1-135:166\">LangChainを使ったRAG実装を始めるために、Python環境を整えます。本記事では、Python 3.11.9を使用して動作確認を行っています。</p>\n<hr data-sourcepos=\"137:1-137:3\">\n<p data-sourcepos=\"138:1-138:31\"><strong>＜仮想環境の作成＞</strong></p>\n<div class=\"code-frame\" data-lang=\"bash\" data-sourcepos=\"139:1-145:3\"><div class=\"highlight\"><pre><code><span class=\"c\"># 仮想環境の作成</span>\n<span class=\"nv\">$ </span>python <span class=\"nt\">-m</span> venv langchain_env\n\n<span class=\"c\"># 仮想環境の有効化（Windows）</span>\n<span class=\"nv\">$ </span>langchain_env/Scripts/activate\n</code></pre></div></div>\n<hr data-sourcepos=\"146:1-146:3\">\n<p data-sourcepos=\"147:1-147:46\"><strong>＜ライブラリのインストール＞</strong></p>\n<div class=\"code-frame\" data-lang=\"bash\" data-sourcepos=\"149:1-151:3\"><div class=\"highlight\"><pre><code><span class=\"nv\">$ </span>pip <span class=\"nb\">install </span>PyMuPDF langchain langchain-openai langchain-chroma python-dotenv\n</code></pre></div></div>\n<p data-sourcepos=\"152:1-152:60\">以下の表は各ライブラリの簡単な説明です。</p>\n<table data-sourcepos=\"154:1-160:143\">\n<thead>\n<tr data-sourcepos=\"154:1-154:128\">\n<th data-sourcepos=\"154:2-154:34\">ライブラリ名</th>\n<th data-sourcepos=\"154:36-154:127\">説明</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"156:1-156:160\">\n<td data-sourcepos=\"156:2-156:24\">PyMuPDF</td>\n<td data-sourcepos=\"156:26-156:159\">PDFやその他のドキュメント形式を操作するためのライブラリ。<code>fitz</code>モジュールを通じて利用可能。</td>\n</tr>\n<tr data-sourcepos=\"157:1-157:157\">\n<td data-sourcepos=\"157:2-157:24\">langchain</td>\n<td data-sourcepos=\"157:26-157:156\">大規模言語モデル（LLM）を活用したアプリケーションを構築するためのフレームワーク。</td>\n</tr>\n<tr data-sourcepos=\"158:1-158:136\">\n<td data-sourcepos=\"158:2-158:24\">langchain-openai</td>\n<td data-sourcepos=\"158:26-158:135\">LangChainでOpenAIのAPIを利用するための拡張ライブラリ。</td>\n</tr>\n<tr data-sourcepos=\"159:1-159:145\">\n<td data-sourcepos=\"159:2-159:24\">langchain-chroma</td>\n<td data-sourcepos=\"159:26-159:144\">LangChainとChroma（ベクトルデータベース）を連携させるためのライブラリ。</td>\n</tr>\n<tr data-sourcepos=\"160:1-160:143\">\n<td data-sourcepos=\"160:2-160:24\">python-dotenv</td>\n<td data-sourcepos=\"160:26-160:142\">\n<code>.env</code>ファイルから環境変数を読み込むためのライブラリ。</td>\n</tr>\n</tbody>\n</table>\n<hr data-sourcepos=\"162:1-162:3\">\n<p data-sourcepos=\"163:1-164:340\"><strong>＜APIキーの設定＞</strong><br>\nプロジェクトのルートディレクトリに <code>.env</code> ファイルを作成し、使用するLLMのAPIキーやエンドポイントを設定しておきます。本記事では、Azure OpenAIのGPT-4o miniを使用する例を示します。他のサービスを利用する場合は、適宜設定内容を変更してください。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"165:1-171:3\">\n<div class=\"code-lang\"><span class=\"bold\">.env</span></div>\n<div class=\"highlight\"><pre><code><span class=\"n\">OPENAI_API_KEY</span><span class=\"o\">=</span><span class=\"n\">your_openai_api_key_here</span>\n<span class=\"n\">API_ENDPOINT</span><span class=\"o\">=</span><span class=\"n\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">your</span><span class=\"o\">-</span><span class=\"n\">resource</span><span class=\"o\">-</span><span class=\"n\">name</span><span class=\"p\">.</span><span class=\"n\">openai</span><span class=\"p\">.</span><span class=\"n\">azure</span><span class=\"p\">.</span><span class=\"n\">com</span><span class=\"o\">/</span>\n<span class=\"n\">API_VERSION</span><span class=\"o\">=</span><span class=\"mi\">2023</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">15</span>  <span class=\"c1\"># Azure OpenAIの場合の例\n</span><span class=\"n\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span><span class=\"o\">=</span><span class=\"n\">your_chat_completion_deployment_id</span>\n<span class=\"n\">DEPLOYMENT_ID_FOR_EMBEDDING</span><span class=\"o\">=</span><span class=\"n\">your_embedding_deployment_id</span>\n</code></pre></div>\n</div>\n<p data-sourcepos=\"172:1-172:63\"><strong><code>.env</code> ファイルの設定項目(Azure OpenAIの場合）</strong></p>\n<table data-sourcepos=\"174:1-180:143\">\n<thead>\n<tr data-sourcepos=\"174:1-174:137\">\n<th data-sourcepos=\"174:2-174:38\">項目名</th>\n<th data-sourcepos=\"174:40-174:136\">説明</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"176:1-176:141\">\n<td data-sourcepos=\"176:2-176:32\">OPENAI_API_KEY</td>\n<td data-sourcepos=\"176:34-176:140\">OpenAIまたはAzure OpenAIで発行されたAPIキー。</td>\n</tr>\n<tr data-sourcepos=\"177:1-177:141\">\n<td data-sourcepos=\"177:2-177:32\">API_ENDPOINT</td>\n<td data-sourcepos=\"177:34-177:140\">Azure OpenAIサービスのエンドポイントURL。</td>\n</tr>\n<tr data-sourcepos=\"178:1-178:159\">\n<td data-sourcepos=\"178:2-178:31\">API_VERSION</td>\n<td data-sourcepos=\"178:33-178:158\">使用するAPIのバージョン。Azureでは一般的に <code>2023-05-15</code>など を指定します。</td>\n</tr>\n<tr data-sourcepos=\"179:1-179:147\">\n<td data-sourcepos=\"179:2-179:35\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</td>\n<td data-sourcepos=\"179:37-179:146\">Chat Completion用にデプロイされたモデルのID。</td>\n</tr>\n<tr data-sourcepos=\"180:1-180:143\">\n<td data-sourcepos=\"180:2-180:32\">DEPLOYMENT_ID_FOR_EMBEDDING</td>\n<td data-sourcepos=\"180:34-180:142\">Embedding用にデプロイされたモデルのID。</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"182:2-182:54\"><strong>⚠️ 注意事項: <code>.env</code> ファイルの管理</strong></p>\n<blockquote data-sourcepos=\"183:1-183:228\">\n<p data-sourcepos=\"183:2-183:228\"><code>.env</code> ファイルにはAPIキーやエンドポイントURLなどの<strong>機密情報</strong>が含まれます。そのため、<strong>必ず <code>.gitignore</code> に追加し、Gitでバージョン管理しないようにしてください。</strong></p>\n</blockquote>\n<h2 data-sourcepos=\"185:1-185:31\">\n<span id=\"42-外部データの登録\" class=\"fragment\"></span><a href=\"#42-%E5%A4%96%E9%83%A8%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E7%99%BB%E9%8C%B2\"><i class=\"fa fa-link\"></i></a>4.2 外部データの登録</h2>\n<h3 data-sourcepos=\"186:1-186:28\">\n<span id=\"1-pdfデータの準備\" class=\"fragment\"></span><a href=\"#1-pdf%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%BA%96%E5%82%99\"><i class=\"fa fa-link\"></i></a>1. PDFデータの準備</h3>\n<p data-sourcepos=\"187:1-187:352\">まず、RAGシステムの知識源とするPDFファイルを用意します。本記事では、動作確認用のPDFファイルとして、国税庁が公開した「令和6年分 年末調整のしかた」を活用しました。<a href=\"https://www.nta.go.jp/publication/pamph/gensen/nencho2024/01.htm\" rel=\"nofollow noopener\" target=\"_blank\">こちら</a>からダウンロードできます。</p>\n<p data-sourcepos=\"189:1-189:442\">活用したいPDFファイルを選んだら、テキストやグラフ、表の情報の抽出を行います。RAG構築において、この情報をいかに正確に抽出できるかが非常に重要となるため、詳しく解説していきます。本記事では、PDFデータの抽出方法を6つ紹介し、それぞれのメリットとデメリット、そして用途に応じた選択のポイントをまとめます。</p>\n<hr data-sourcepos=\"191:1-191:3\">\n<p data-sourcepos=\"192:1-192:45\"><strong>＜Ⅰ. Pythonライブラリの活用＞</strong></p>\n<p data-sourcepos=\"194:2-195:17\"><code>PyMuPDF（fitz）</code>や<code>pdfminer</code>、<code>PyPDF2</code>などの Pythonライブラリを利用して、プログラムを通じてテキストや構造情報を抽出する方法です。<br>\n<strong>メリット</strong></p>\n<ul data-sourcepos=\"196:1-199:0\">\n<li data-sourcepos=\"196:1-196:50\">大量のPDFを自動で一括処理できる。</li>\n<li data-sourcepos=\"197:1-197:80\">Pythonスクリプトで独自の前処理・後処理を組み込み可能。</li>\n<li data-sourcepos=\"198:1-199:0\">オープンソースのライブラリなら無料で利用可能。</li>\n</ul>\n<p data-sourcepos=\"200:1-200:19\"><strong>デメリット</strong></p>\n<ul data-sourcepos=\"201:1-204:0\">\n<li data-sourcepos=\"201:1-201:62\">スクリプトを書くための知識が求められる。</li>\n<li data-sourcepos=\"202:1-202:92\">PDFのレイアウトや文字コードの扱いによって抽出精度に差がでる。</li>\n<li data-sourcepos=\"203:1-204:0\">グラフや表の読み取り精度が低い。</li>\n</ul>\n<p data-sourcepos=\"205:1-205:34\"><strong>こんなときにおすすめ</strong></p>\n<ul data-sourcepos=\"206:1-208:0\">\n<li data-sourcepos=\"206:1-206:92\">RAG用に大量のPDFデータを定期的に収集・更新する必要があるとき。</li>\n<li data-sourcepos=\"207:1-208:0\">自由度の高い前処理や後処理（特定のセクションだけ抽出、マーカー付きテキストだけ取り出すなど）が必要なとき。</li>\n</ul>\n<hr data-sourcepos=\"209:1-209:3\">\n<p data-sourcepos=\"210:1-211:310\"><strong>＜Ⅱ. 生成AIサービスの活用＞</strong><br>\n<code>ChatGPT</code>やGoogle DeepMindの<code>Gemini</code>などの生成AIを利用して、PDFの内容を要約・解析し、必要な情報を取り出す方法です。（PDF自体を直接アップロードできるサービスもあれば、テキストを貼り付けて要約させるアプローチもあります。）</p>\n<p data-sourcepos=\"213:1-213:16\"><strong>メリット</strong></p>\n<ul data-sourcepos=\"214:1-217:0\">\n<li data-sourcepos=\"214:1-214:119\">単なるテキスト抽出にとどまらず、要約や意図理解などの高度な自然言語解析も得意。</li>\n<li data-sourcepos=\"215:1-215:101\">プログラミング不要で、Webインターフェースからすぐ使える場合が多い。</li>\n<li data-sourcepos=\"216:1-217:0\">多言語対応: さまざまな言語のテキストを高精度で解析できる。</li>\n</ul>\n<p data-sourcepos=\"218:1-218:19\"><strong>デメリット</strong></p>\n<ul data-sourcepos=\"219:1-222:0\">\n<li data-sourcepos=\"219:1-219:99\">一定以上のボリュームを処理する場合、API利用料がかさむことがある。</li>\n<li data-sourcepos=\"220:1-220:114\">プライバシー/セキュリティの懸念: 機密データをクラウドへ送信する際は要注意。</li>\n<li data-sourcepos=\"221:1-222:0\">グラフや表などの正確な位置関係までは再現しにくい。</li>\n</ul>\n<p data-sourcepos=\"223:1-223:34\"><strong>こんなときにおすすめ</strong></p>\n<ul data-sourcepos=\"224:1-226:0\">\n<li data-sourcepos=\"224:1-224:119\">文章の意味を理解した要約や「このテキストで質問に回答できるようにしたい」とき。</li>\n<li data-sourcepos=\"225:1-226:0\">自社データを特定のトピックに沿って要約・分類したい場合。</li>\n</ul>\n<hr data-sourcepos=\"227:1-227:3\">\n<p data-sourcepos=\"228:1-229:232\"><strong>＜Ⅲ. OCRの活用＞</strong><br>\nスキャンした画像ベースのPDFや、文字情報が埋め込まれていないPDFからテキストを抽出する場合に OCR (Optical Character Recognition) を利用します。<code>Tesseract OCR</code>などが代表的です。</p>\n<p data-sourcepos=\"231:1-231:16\"><strong>メリット</strong></p>\n<ul data-sourcepos=\"232:1-234:0\">\n<li data-sourcepos=\"232:1-232:109\">画像化されたPDFにも対応: スキャンデータや画像形式のドキュメントを扱える。</li>\n<li data-sourcepos=\"233:1-234:0\">多言語対応: 設定次第で複数言語を認識可能。</li>\n</ul>\n<p data-sourcepos=\"235:1-235:19\"><strong>デメリット</strong></p>\n<ul data-sourcepos=\"236:1-238:0\">\n<li data-sourcepos=\"236:1-236:112\">精度が画像品質に左右される: 解像度や文字の読みやすさで結果が大きく変わる。</li>\n<li data-sourcepos=\"237:1-238:0\">処理時間: 大量のファイルを扱う場合は長時間のバッチ処理が必要になることも。</li>\n</ul>\n<p data-sourcepos=\"239:1-239:34\"><strong>こんなときにおすすめ</strong></p>\n<ul data-sourcepos=\"240:1-242:0\">\n<li data-sourcepos=\"240:1-240:113\">スキャンされた古い文書や紙資料をデジタル化したPDFから情報を抽出したいとき。</li>\n<li data-sourcepos=\"241:1-242:0\">文字列として取り出せない図表やスペックシートなどの画像内文字を読み取りたいとき。</li>\n</ul>\n<hr data-sourcepos=\"243:1-243:3\">\n<p data-sourcepos=\"244:2-245:120\"><strong>＜Ⅳ.  手作業（マニュアル抽出）＞</strong><br>\n人間がPDFを開き、必要な箇所をコピー＆ペーストや書き写しで抽出するアナログな方法。</p>\n<p data-sourcepos=\"247:1-247:16\"><strong>メリット</strong></p>\n<ul data-sourcepos=\"248:1-250:0\">\n<li data-sourcepos=\"248:1-248:77\">人が直接確認しながら作業するため、誤抽出が少ない。</li>\n<li data-sourcepos=\"249:1-250:0\">表や特殊な図版など、アルゴリズムでは難しい内容も対応可能。</li>\n</ul>\n<p data-sourcepos=\"251:1-251:19\"><strong>デメリット</strong></p>\n<ul data-sourcepos=\"252:1-254:0\">\n<li data-sourcepos=\"252:1-252:74\">大量のファイルを扱うときは時間と人件費がかかる。</li>\n<li data-sourcepos=\"253:1-254:0\">熟練度や集中力に左右される。</li>\n</ul>\n<p data-sourcepos=\"255:1-255:34\"><strong>こんなときにおすすめ</strong></p>\n<ul data-sourcepos=\"256:1-258:0\">\n<li data-sourcepos=\"256:1-256:74\">データ量がごく少量で、かつ正確性が最優先な場合。</li>\n<li data-sourcepos=\"257:1-258:0\">極端に複雑なレイアウトや、特殊文字・手書き要素が混在する場合。</li>\n</ul>\n<hr data-sourcepos=\"259:1-259:3\">\n<p data-sourcepos=\"260:1-261:139\"><strong>＜Ⅴ. 専用PDF解析ツールの利用＞</strong><br>\n<code>Adobe Acrobat</code>や<code>PDF Expert</code>など、有料・無料問わず商用/専用ソフトを活用してPDF情報を解析する方法です。</p>\n<p data-sourcepos=\"263:1-263:16\"><strong>メリット</strong></p>\n<ul data-sourcepos=\"264:1-267:0\">\n<li data-sourcepos=\"264:1-264:110\">市販ツールは独自のアルゴリズムを搭載していることが多く、抽出精度が高い。</li>\n<li data-sourcepos=\"265:1-265:74\">コードを書かずにGUIで簡単にファイル操作を行える。</li>\n<li data-sourcepos=\"266:1-267:0\">テキスト編集、注釈、比較、セキュリティ設定など、周辺機能が充実。</li>\n</ul>\n<p data-sourcepos=\"268:1-268:19\"><strong>デメリット</strong></p>\n<ul data-sourcepos=\"269:1-271:0\">\n<li data-sourcepos=\"269:1-269:59\">商用ソフトの場合はライセンス料が必要。</li>\n<li data-sourcepos=\"270:1-271:0\">特殊な処理を行いたい場合に独自のカスタマイズが難しいことも。</li>\n</ul>\n<p data-sourcepos=\"272:1-272:34\"><strong>こんなときにおすすめ</strong></p>\n<ul data-sourcepos=\"273:1-275:0\">\n<li data-sourcepos=\"273:1-273:77\">プログラミングは苦手だけどGUIでサクッと抽出したい。</li>\n<li data-sourcepos=\"274:1-275:0\">社内規定でAdobe製品のみ利用可能など、特定ツールが標準化されている環境。</li>\n</ul>\n<hr data-sourcepos=\"276:1-276:3\">\n<p data-sourcepos=\"277:1-278:171\"><strong>＜Ⅵ. クラウドサービスの活用＞</strong><br>\n<code>AWS Textract</code>、<code>Google Cloud Vision</code>、<code>Azure Form Recognizer</code>などのクラウドAIサービスを使い、PDFからテキストや構造を抽出する方法です。</p>\n<p data-sourcepos=\"280:1-280:16\"><strong>メリット</strong></p>\n<ul data-sourcepos=\"281:1-284:0\">\n<li data-sourcepos=\"281:1-281:47\">大量のファイルを同時処理可能。</li>\n<li data-sourcepos=\"282:1-282:98\">単なるテキスト抽出だけでなく、表やレイアウト構造を自動的に識別。</li>\n<li data-sourcepos=\"283:1-284:0\">サーバーの保守やアップデートはクラウドベンダー側が実施するため、管理が容易。</li>\n</ul>\n<p data-sourcepos=\"285:1-285:19\"><strong>デメリット</strong></p>\n<ul data-sourcepos=\"286:1-288:0\">\n<li data-sourcepos=\"286:1-286:74\">APIコール数やファイルサイズに応じて費用がかかる。</li>\n<li data-sourcepos=\"287:1-288:0\">機密情報の取り扱いに注意が必要。</li>\n</ul>\n<p data-sourcepos=\"289:1-289:34\"><strong>こんなときにおすすめ</strong></p>\n<ul data-sourcepos=\"290:1-292:0\">\n<li data-sourcepos=\"290:1-290:101\">大規模・高負荷のプロジェクトで、オンプレミスでの処理が困難な場合。</li>\n<li data-sourcepos=\"291:1-292:0\">表やフォームなど構造化データの抽出をクラウドで一括して行いたい場合。</li>\n</ul>\n<hr data-sourcepos=\"293:1-293:3\">\n<p data-sourcepos=\"294:1-294:184\">以上6つの抽出手法をテキスト抽出精度、表や図の読み取り精度、コスト、処理速度の観点で（個人的に）評価した表を以下に示します。</p>\n<table data-sourcepos=\"296:1-303:106\">\n<thead>\n<tr data-sourcepos=\"296:1-296:117\">\n<th data-sourcepos=\"296:2-296:29\">方法</th>\n<th style=\"text-align: center\" data-sourcepos=\"296:31-296:56\">テキスト抽出精度</th>\n<th style=\"text-align: center\" data-sourcepos=\"296:58-296:89\">表や図の読み取り精度</th>\n<th style=\"text-align: center\" data-sourcepos=\"296:91-296:101\">コスト</th>\n<th style=\"text-align: center\" data-sourcepos=\"296:103-296:116\">処理速度</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"298:1-298:102\">\n<td data-sourcepos=\"298:2-298:33\"><strong>1. Pythonライブラリ</strong></td>\n<td style=\"text-align: center\" data-sourcepos=\"298:35-298:53\">○</td>\n<td style=\"text-align: center\" data-sourcepos=\"298:55-298:77\">△</td>\n<td style=\"text-align: center\" data-sourcepos=\"298:79-298:88\">○</td>\n<td style=\"text-align: center\" data-sourcepos=\"298:90-298:101\">○</td>\n</tr>\n<tr data-sourcepos=\"299:1-299:108\">\n<td data-sourcepos=\"299:2-299:35\"><strong>2. 生成AIサービス</strong></td>\n<td style=\"text-align: center\" data-sourcepos=\"299:37-299:55\">◎</td>\n<td style=\"text-align: center\" data-sourcepos=\"299:57-299:83\">△ ～  ○</td>\n<td style=\"text-align: center\" data-sourcepos=\"299:85-299:94\">△</td>\n<td style=\"text-align: center\" data-sourcepos=\"299:96-299:107\">△</td>\n</tr>\n<tr data-sourcepos=\"300:1-300:109\">\n<td data-sourcepos=\"300:2-300:31\"><strong>3. OCRの活用</strong></td>\n<td style=\"text-align: center\" data-sourcepos=\"300:33-300:60\">△ ～  ○</td>\n<td style=\"text-align: center\" data-sourcepos=\"300:62-300:84\">○</td>\n<td style=\"text-align: center\" data-sourcepos=\"300:86-300:95\">○</td>\n<td style=\"text-align: center\" data-sourcepos=\"300:97-300:108\">△</td>\n</tr>\n<tr data-sourcepos=\"301:1-301:98\">\n<td data-sourcepos=\"301:2-301:31\"><strong>4. 手作業</strong></td>\n<td style=\"text-align: center\" data-sourcepos=\"301:33-301:51\">◎</td>\n<td style=\"text-align: center\" data-sourcepos=\"301:53-301:75\">◎</td>\n<td style=\"text-align: center\" data-sourcepos=\"301:77-301:85\">×</td>\n<td style=\"text-align: center\" data-sourcepos=\"301:87-301:97\">×</td>\n</tr>\n<tr data-sourcepos=\"302:1-302:105\">\n<td data-sourcepos=\"302:2-302:36\"><strong>5. 専用PDF解析ツール</strong></td>\n<td style=\"text-align: center\" data-sourcepos=\"302:38-302:56\">○</td>\n<td style=\"text-align: center\" data-sourcepos=\"302:58-302:80\">○</td>\n<td style=\"text-align: center\" data-sourcepos=\"302:82-302:91\">△</td>\n<td style=\"text-align: center\" data-sourcepos=\"302:93-302:104\">○</td>\n</tr>\n<tr data-sourcepos=\"303:1-303:106\">\n<td data-sourcepos=\"303:2-303:37\"><strong>6. クラウドサービス</strong></td>\n<td style=\"text-align: center\" data-sourcepos=\"303:39-303:57\">◎</td>\n<td style=\"text-align: center\" data-sourcepos=\"303:59-303:81\">○</td>\n<td style=\"text-align: center\" data-sourcepos=\"303:83-303:92\">△</td>\n<td style=\"text-align: center\" data-sourcepos=\"303:94-303:105\">△</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"306:1-306:219\">これらの特徴を踏まえて、使用するPDFファイルの特徴に応じて適切な方法を選択、もしくは組み合わせて使用することが重要です。以下にいくつかの例を示します</p>\n<ul data-sourcepos=\"308:1-315:110\">\n<li data-sourcepos=\"308:1-310:0\">\n<p data-sourcepos=\"308:3-309:123\">図や表が非常に多いPDFファイル<br>\n→ PDF解析ツールやクラウドサービスを利用して抽出し、必要に応じて手作業で修正する。</p>\n</li>\n<li data-sourcepos=\"311:1-313:0\">\n<p data-sourcepos=\"311:3-312:78\">図や表がほぼない文章中心のPDFファイル<br>\n→ Pythonライブラリを活用して効率よく抽出処理を行う。</p>\n</li>\n<li data-sourcepos=\"314:1-315:110\">\n<p data-sourcepos=\"314:3-315:110\">スキャンされた画像ベースのPDF<br>\n→ OCRを用いてテキストを抽出した後、生成AIサービスで要約や情報解析を行う。</p>\n</li>\n</ul>\n<hr data-sourcepos=\"316:1-316:3\">\n<h3 data-sourcepos=\"317:1-317:16\">\n<span id=\"実装\" class=\"fragment\"></span><a href=\"#%E5%AE%9F%E8%A3%85\"><i class=\"fa fa-link\"></i></a>＜実装＞</h3>\n<p data-sourcepos=\"318:1-318:240\">PDFデータを用いて情報抽出を行います。今回は、国税庁が公開した資料「年末調整のしかた」を使用します。この資料は表やグラフが少ないため、以下の手順で処理を進めました</p>\n<ol data-sourcepos=\"319:1-327:0\">\n<li data-sourcepos=\"319:1-321:0\">\n<p data-sourcepos=\"319:4-320:114\"><strong>Pythonライブラリを活用した抽出</strong><br>\n表やグラフの数が少ないため、Pythonライブラリを使用してテキストを抽出しました。</p>\n</li>\n<li data-sourcepos=\"322:1-324:0\">\n<p data-sourcepos=\"322:4-323:125\"><strong>Azure OpenAIを使用したテキスト処理</strong><br>\n抽出したデータを<code>Azure OpenAI</code>で処理し、文章の体裁の整理、簡易的な表を生成を行いました。</p>\n</li>\n<li data-sourcepos=\"325:1-327:0\">\n<p data-sourcepos=\"325:4-326:123\"><strong>手作業での修正</strong><br>\n数値やテキストの誤り、体裁の細かい崩れを確認し、必要な部分を手作業で修正しました。</p>\n</li>\n</ol>\n<p data-sourcepos=\"328:1-328:72\">PDFのデータ抽出を行うpythonコードを以下に示します。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"329:1-455:3\">\n<div class=\"code-lang\"><span class=\"bold\">pdf_text_extraction.py</span></div>\n<div class=\"highlight\"><pre><code><span class=\"kn\">import</span> <span class=\"n\">fitz</span>  <span class=\"c1\"># PyMuPDF\n</span><span class=\"kn\">from</span> <span class=\"n\">langchain_openai</span> <span class=\"kn\">import</span> <span class=\"n\">AzureChatOpenAI</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain.schema</span> <span class=\"kn\">import</span> <span class=\"n\">HumanMessage</span><span class=\"p\">,</span> <span class=\"n\">SystemMessage</span>\n<span class=\"kn\">from</span> <span class=\"n\">dotenv</span> <span class=\"kn\">import</span> <span class=\"n\">load_dotenv</span>\n<span class=\"kn\">import</span> <span class=\"n\">os</span>\n<span class=\"kn\">from</span> <span class=\"n\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">Optional</span>\n<span class=\"kn\">from</span> <span class=\"n\">tqdm</span> <span class=\"kn\">import</span> <span class=\"n\">tqdm</span>\n<span class=\"kn\">import</span> <span class=\"n\">time</span>\n\n<span class=\"c1\"># 環境変数の読み込み\n</span><span class=\"nf\">load_dotenv</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Azure OpenAI の設定\n</span><span class=\"n\">AZURE_OPENAI_API_KEY</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">AZURE_OPENAI_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">AZURE_OPENAI_ENDPOINT</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">AZURE_OPENAI_ENDPOINT</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">API_VERSION</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">API_VERSION</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># LLMのインスタンス作成\n</span><span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"nc\">AzureChatOpenAI</span><span class=\"p\">(</span>\n    <span class=\"n\">deployment_name</span><span class=\"o\">=</span><span class=\"n\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_key</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_API_KEY</span><span class=\"p\">,</span>\n    <span class=\"n\">azure_endpoint</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_ENDPOINT</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_version</span><span class=\"o\">=</span><span class=\"n\">API_VERSION</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">extract_and_restructure_pdf</span><span class=\"p\">(</span>\n    <span class=\"n\">pdf_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n    <span class=\"n\">output_md_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n    <span class=\"n\">output_raw_text_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n    <span class=\"n\">first_page</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"n\">last_page</span><span class=\"p\">:</span> <span class=\"n\">Optional</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"bp\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">max_retries</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">,</span>\n    <span class=\"n\">retry_delay</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n<span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n    \n    \n    <span class=\"sh\">\"\"\"</span><span class=\"s\">PDFの内容を抽出して整形し、Markdownと生テキストファイルに保存します。\n\n    Args:\n            pdf_path (str): 処理するPDFファイルのパス。\n            output_md_path (str): 整形結果を保存するMarkdownファイルのパス。\n            output_raw_text_path (str): 抽出された生テキストを保存するファイルのパス。\n            first_page (Optional[int]): 処理を開始するページ番号（1始まり）。デフォルトは1。\n            last_page (Optional[int]): 処理を終了するページ番号。デフォルトは最終ページ。\n            max_retries (int): LLM呼び出しの最大リトライ回数。デフォルトは3。\n            retry_delay (int): LLM呼び出し失敗時のリトライ間隔（秒）。デフォルトは2秒。\n    </span><span class=\"sh\">\"\"\"</span>    \n        \n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"c1\"># PDFを開く\n</span>        <span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"n\">fitz</span><span class=\"p\">.</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">pdf_path</span><span class=\"p\">)</span>\n        <span class=\"n\">total_pages</span> <span class=\"o\">=</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># ページ範囲の計算\n</span>        <span class=\"n\">first_page</span> <span class=\"o\">=</span> <span class=\"n\">first_page</span> <span class=\"ow\">or</span> <span class=\"mi\">1</span>\n        <span class=\"n\">last_page</span> <span class=\"o\">=</span> <span class=\"n\">last_page</span> <span class=\"ow\">or</span> <span class=\"n\">total_pages</span>\n\n        <span class=\"n\">raw_texts</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>  <span class=\"c1\"># 生テキストを保存するリスト\n</span>        <span class=\"n\">markdown_results</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>  <span class=\"c1\"># 整形後のテキストを保存するリスト\n</span>\n        <span class=\"c1\"># PDFページを順に処理\n</span>        <span class=\"k\">for</span> <span class=\"n\">page_num</span> <span class=\"ow\">in</span> <span class=\"nf\">tqdm</span><span class=\"p\">(</span><span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"n\">first_page</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">last_page</span><span class=\"p\">),</span> <span class=\"n\">desc</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">Processing PDF Pages</span><span class=\"sh\">\"</span><span class=\"p\">):</span>\n            <span class=\"n\">page</span> <span class=\"o\">=</span> <span class=\"n\">doc</span><span class=\"p\">[</span><span class=\"n\">page_num</span><span class=\"p\">]</span>\n            <span class=\"n\">extracted_text</span> <span class=\"o\">=</span> <span class=\"n\">page</span><span class=\"p\">.</span><span class=\"nf\">get_text</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">text</span><span class=\"sh\">\"</span><span class=\"p\">)</span>  <span class=\"c1\"># テキスト抽出\n</span>\n            <span class=\"c1\"># LLMへの指示を作成\n</span>            <span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n                <span class=\"nc\">SystemMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"p\">(</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">あなたは優れたアシスタントです。以下に与えられるテキストはPDFから抽出された内容であり、体裁が崩れている可能性があります。</span><span class=\"se\">\\n\\n</span><span class=\"sh\">\"</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">以下の指示に従って、テキストの整形を行ってください:</span><span class=\"se\">\\n\\n</span><span class=\"sh\">\"</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">1. 句読点や改行の位置を適切に整え、誤字脱字を修正してください（文脈に基づく範囲内で）。</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">2. 元のテキストに含まれる情報を削除しないでください。</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">3. 表形式のデータは可能な限り元のレイアウトを維持してください。</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">4. グラフの軸の数値関係を確認し、適切に説明してください。</span><span class=\"se\">\\n\\n</span><span class=\"sh\">\"</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">最終結果はMarkdown形式で出力してください。</span><span class=\"sh\">\"</span>\n                <span class=\"p\">)),</span>\n                <span class=\"nc\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">## ページ </span><span class=\"si\">{</span><span class=\"n\">page_num</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"se\">\\n\\n</span><span class=\"s\">### 抽出されたテキスト:</span><span class=\"se\">\\n\\n</span><span class=\"si\">{</span><span class=\"n\">extracted_text</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n            <span class=\"p\">]</span>\n\n            <span class=\"c1\"># LLMを使用して整形\n</span>            <span class=\"k\">for</span> <span class=\"n\">attempt</span> <span class=\"ow\">in</span> <span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"n\">max_retries</span><span class=\"p\">):</span>\n                <span class=\"k\">try</span><span class=\"p\">:</span>\n                    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"p\">.</span><span class=\"nf\">invoke</span><span class=\"p\">(</span><span class=\"n\">messages</span><span class=\"p\">)</span>\n                    <span class=\"n\">markdown_results</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">content</span><span class=\"p\">)</span>\n                    <span class=\"k\">break</span>\n                <span class=\"k\">except</span> <span class=\"nb\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n                    <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">Error during OpenAI API call for page </span><span class=\"si\">{</span><span class=\"n\">page_num</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\"> (attempt </span><span class=\"si\">{</span><span class=\"n\">attempt</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">): </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n                    <span class=\"k\">if</span> <span class=\"n\">attempt</span> <span class=\"o\">==</span> <span class=\"n\">max_retries</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n                        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">Failed after max retries for page </span><span class=\"si\">{</span><span class=\"n\">page_num</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">.</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n                    <span class=\"n\">time</span><span class=\"p\">.</span><span class=\"nf\">sleep</span><span class=\"p\">(</span><span class=\"n\">retry_delay</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># 抽出されたテキストを保存\n</span>            <span class=\"n\">raw_texts</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">## ページ </span><span class=\"si\">{</span><span class=\"n\">page_num</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"si\">{</span><span class=\"n\">extracted_text</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># 生テキストをファイルに保存\n</span>        <span class=\"k\">with</span> <span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">output_raw_text_path</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">w</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">utf-8</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">raw_file</span><span class=\"p\">:</span>\n            <span class=\"n\">raw_file</span><span class=\"p\">.</span><span class=\"nf\">write</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"se\">\\n\\n</span><span class=\"sh\">\"</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">(</span><span class=\"n\">raw_texts</span><span class=\"p\">))</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">抽出されたテキストが保存されました: </span><span class=\"si\">{</span><span class=\"n\">output_raw_text_path</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># 整形結果をMarkdownで保存\n</span>        <span class=\"k\">with</span> <span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">output_md_path</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">w</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">utf-8</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">md_file</span><span class=\"p\">:</span>\n            <span class=\"n\">md_file</span><span class=\"p\">.</span><span class=\"nf\">write</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"se\">\\n\\n</span><span class=\"sh\">\"</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">(</span><span class=\"n\">markdown_results</span><span class=\"p\">))</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">整形結果がMarkdownファイルに保存されました: </span><span class=\"si\">{</span><span class=\"n\">output_md_path</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n    <span class=\"k\">except</span> <span class=\"nb\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">Error processing PDF: </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">if</span> <span class=\"n\">__name__</span> <span class=\"o\">==</span> <span class=\"sh\">'</span><span class=\"s\">__main__</span><span class=\"sh\">'</span><span class=\"p\">:</span>\n    <span class=\"c1\"># 使用例\n</span>    <span class=\"n\">pdf_path</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">nencho_all.pdf</span><span class=\"sh\">\"</span>  <span class=\"c1\"># 処理するPDFファイルのパス\n</span>    <span class=\"n\">output_md_path</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">output/nencho_al_test.md</span><span class=\"sh\">\"</span>  <span class=\"c1\"># 整形結果の保存先\n</span>    <span class=\"n\">output_raw_text_path</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">output/nencho_all_test.txt</span><span class=\"sh\">\"</span>  <span class=\"c1\"># 抽出されたテキストの保存先\n</span>\n    <span class=\"c1\"># 出力ディレクトリの作成\n</span>    <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">makedirs</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">output</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">exist_ok</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># PDF処理の実行\n</span>    <span class=\"nf\">extract_and_restructure_pdf</span><span class=\"p\">(</span>\n        <span class=\"n\">pdf_path</span><span class=\"o\">=</span><span class=\"n\">pdf_path</span><span class=\"p\">,</span>\n        <span class=\"n\">output_md_path</span><span class=\"o\">=</span><span class=\"n\">output_md_path</span><span class=\"p\">,</span>\n        <span class=\"n\">output_raw_text_path</span><span class=\"o\">=</span><span class=\"n\">output_raw_text_path</span><span class=\"p\">,</span>\n        <span class=\"n\">first_page</span><span class=\"o\">=</span> <span class=\"bp\">None</span><span class=\"p\">,</span>  <span class=\"c1\"># 最初のページから処理\n</span>        <span class=\"n\">last_page</span><span class=\"o\">=</span> <span class=\"bp\">None</span>    <span class=\"c1\"># 最後のページまで処理\n</span>    <span class=\"p\">)</span>\n</code></pre></div>\n</div>\n<p data-sourcepos=\"457:1-458:51\">本記事では<code>PyMuPDF</code>と<code>GPT4o-mini</code>を使用していますが、PDFの特徴に応じて他のライブラリや生成AIを活用し、それらを比較してみるのも有効なアプローチです。また、PDFの内容や構造に応じた適切なプロンプト設計を行うことで、より正確で有用な出力を得ることが可能です。<br>\n出力結果は以下のようになりました。</p>\n<div class=\"code-frame\" data-lang=\"md\" data-sourcepos=\"460:1-494:3\">\n<div class=\"code-lang\"><span class=\"bold\">出力結果</span></div>\n<div class=\"highlight\"><pre><code><span class=\"gu\">## ページ 1</span>\n\n<span class=\"gu\">### 令和6年分 年末調整のしかた</span>\n\n法人番号: 700000120500002\n\n「年末調整がよくわかるページ」をご覧ください！\n\n国税庁ホームページには、「年末調整がよくわかるページ」を掲載しています。このページには、本年の定額減税を含めた年末調整の手順等を解説した動画やパンフレット、扶養控除等申告書など各種申告書、従業員向けの説明用リーフレットや各種申告書の記載例など、年末調整の際に役立つ情報を掲載していますので、ご活用ください。\n\nなお、動画による説明はYouTubeにも掲載していますので、ご活用ください。\n\n※ 令和6年分の各種情報については、令和6年10月頃に掲載いたします。\n\n年末調整に係る源泉徴収をした所得税及び復興特別所得税の納期限は、令和7年1月10日（金）（納期の特例の承認を受けている場合は、令和7年1月20日（月））です。\n\nその他、給料や報酬などについて源泉徴収をした所得税及び復興特別所得税の納期限については、2ページを確認してください。\n\n（よくわかるページ）\n\n（YouTube）\n\n年末調整に関する相談は、国税庁ホームページからチャットボットの「税務職員ふたば」をお気軽にご利用ください。年末調整の各種申告書の書き方や添付書類に関することなどについて、AIが自動で回答します。\n\n※ 公開期間は令和6年10月頃から令和7年1月下旬までの予定です。年末調整でお困りのときは、“ふたば” にご相談ください。\n\n（チャットボット）\n\n<span class=\"gu\">### 年末調整手続の電子化で業務の効率化！</span>\n\n年末調整手続の電子化を行うと、給与の支払者（勤務先）及び給与所得者（従業員）それぞれにおいて、書類の作成や確認、保管などの業務全般が大幅に効率化されるなど、双方に大きなメリットがあります。\n\nまた、国税庁では「年末調整控除申告書作成用ソフトウェア」（年調ソフト）を無償で提供しております。年末調整手続の電子化や年調ソフトについて、詳しくは国税庁ホームページをご覧ください。\n</code></pre></div>\n</div>\n<p data-sourcepos=\"496:1-496:358\">テキストはほぼ正確に抽出できていますが、順番が逆になっていたり、表の読み取りが誤っている部分があったので手作業で修正しました。この修正版のMarkdownファイル（mdファイル）は、Gitレポジトリ内の<code>data_sources</code>フォルダに保存されています。(<code>revised_nencho_all.md</code>)</p>\n<p data-sourcepos=\"498:1-498:321\">また、表やグラフに含まれる情報をRAGで活用する場合、LLMが内容を正しく理解できるよう、データの構造を明示的にする必要があります。特に、表やグラフ形式のデータは、数値や文字情報の縦・横の関係性をLLMに伝える工夫が重要です。</p>\n<p data-sourcepos=\"500:1-500:93\">以下に、PDFから抽出した表データをLLM向けに変換する例を示します。</p>\n<p data-sourcepos=\"502:1-502:40\"><strong>元の表（PDFのイメージ）：</strong></p>\n<table data-sourcepos=\"504:1-507:19\">\n<thead>\n<tr data-sourcepos=\"504:1-504:48\">\n<th data-sourcepos=\"504:2-504:8\">年度</th>\n<th data-sourcepos=\"504:10-504:28\">売上（億円）</th>\n<th data-sourcepos=\"504:30-504:47\">利益（億円）</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"506:1-506:19\">\n<td data-sourcepos=\"506:2-506:9\">2020年</td>\n<td data-sourcepos=\"506:11-506:14\">150</td>\n<td data-sourcepos=\"506:16-506:18\">20</td>\n</tr>\n<tr data-sourcepos=\"507:1-507:19\">\n<td data-sourcepos=\"507:2-507:9\">2021年</td>\n<td data-sourcepos=\"507:11-507:14\">200</td>\n<td data-sourcepos=\"507:16-507:18\">30</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"509:1-509:37\"><strong>LLM向けに変換した形式：</strong></p>\n<ul data-sourcepos=\"511:1-513:0\">\n<li data-sourcepos=\"511:1-511:62\">2020年度の売上は150億円、利益は20億円です。</li>\n<li data-sourcepos=\"512:1-513:0\">2021年年度の売上は200億円、利益は30億円です。</li>\n</ul>\n<p data-sourcepos=\"514:1-514:426\">上記のように、表の各行を文章形式で表現し、列の情報をキーワードとして明示的に記述することで、LLMは表データの構造と内容を効果的に理解できます。この形式に変換することで、LLMは「〇〇年度の売上はいくらですか？」といった質問に対して、表データに基づいた正確な回答を生成することが期待できます。</p>\n<hr data-sourcepos=\"516:1-516:3\">\n<p data-sourcepos=\"517:1-518:366\">長くなりましたが、以上がPDFからのデータ抽出方法となります。今回のRAG構築において、このPDFデータの前処理に最も時間を費やしました。RAGで期待通りの正確な回答を得るためには、元となるPDFデータから、いかに情報を正確に、そして適切に抽出できるかが非常に重要です。 どんなに高性能なLLMやRAGフレームワークを使ったとしても、知識源となるデータの精度が低ければ、精度の高い回答は望めません。<br>\n特に、PDFデータは、レイアウトの複雑さや文字認識の課題など、データ抽出の難易度が高い場合があります。だからこそ、RAG構築の成否を左右すると言っても過言ではない、データの前処理には丁寧に時間をかけることが重要だと、今回の経験を通して改めて感じました。</p>\n<h3 data-sourcepos=\"520:1-520:48\">\n<span id=\"2-ドキュメント分割-text-splitting\" class=\"fragment\"></span><a href=\"#2-%E3%83%89%E3%82%AD%E3%83%A5%E3%83%A1%E3%83%B3%E3%83%88%E5%88%86%E5%89%B2-text-splitting\"><i class=\"fa fa-link\"></i></a>2. ドキュメント分割 (Text Splitting)</h3>\n<p data-sourcepos=\"521:1-521:293\">PDFファイルからテキストデータを抽出したら、次にテキストを<strong>ドキュメント分割 (Text Splitting)</strong> する必要があります。ドキュメント分割とは、抽出したテキストを<strong>チャンク</strong>と呼ばれる小さな塊に分割する処理です。</p>\n<p data-sourcepos=\"523:1-523:70\">ドキュメント分割を行う主な目的は以下の2点です。</p>\n<ul data-sourcepos=\"524:1-529:0\">\n<li data-sourcepos=\"524:1-526:0\">\n<p data-sourcepos=\"524:3-525:210\"><strong>検索精度の向上:</strong><br>\n質問文とPDFドキュメント全体を比較するよりも、細かく分割されたチャンク単位で比較する方が、質問内容との関連性が高い箇所を特定しやすくなります。</p>\n</li>\n<li data-sourcepos=\"527:1-529:0\">\n<p data-sourcepos=\"527:3-528:431\"><strong>LLMのコンテキストウィンドウ制限への対応:</strong><br>\n大規模言語モデル (LLM) が一度に処理できるテキスト量には上限 (コンテキストウィンドウ) があります。長文のままLLMに入力すると、コンテキストウィンドウを超過し、処理が正常に行われない可能性があります。ドキュメントを分割することで、各チャンクがLLMのコンテキストウィンドウ内に収まるように調整します。</p>\n</li>\n</ul>\n<p data-sourcepos=\"530:1-530:531\">LangChainでは、様々なテキスト分割方法 (Text Splitter) が提供されています。記事では、<code>RecursiveCharacterTextSplitter</code> を使用します。これは、改行文字や句読点などを区切り文字として、意味的にまとまりのある単位でテキストを再帰的に分割するText Splitter です。分割の粒度を調整するために、chunk_size (チャンクの最大文字数) や chunk_overlap (チャンク間の重複文字数) などのパラメータを設定できます。</p>\n<p data-sourcepos=\"532:1-532:349\">他にも、指定された区切り文字で分割する<code>CharacterTextSplitter</code>や、Markdown形式のテキストを、Markdownの構造(見出し、リスト、コードブロックなど) を考慮して分割する<code>MarkdownTextSplitter</code>などもあるため、目的に応じて使い分けることで回答性能向上も期待できます。</p>\n<h3 data-sourcepos=\"534:1-534:36\">\n<span id=\"3-埋め込み生成embedding\" class=\"fragment\"></span><a href=\"#3-%E5%9F%8B%E3%82%81%E8%BE%BC%E3%81%BF%E7%94%9F%E6%88%90embedding\"><i class=\"fa fa-link\"></i></a>3. 埋め込み生成(Embedding)</h3>\n<p data-sourcepos=\"535:1-535:245\">ドキュメント分割で得られたテキストチャンクに対し、<strong>埋め込み生成 (Embedding)</strong> を行います。埋め込み生成とは、テキストの意味内容を捉えた数値ベクトルへと変換する処理です。</p>\n<p data-sourcepos=\"537:1-537:304\">埋め込み生成の目的は、テキスト同士の<strong>意味的な類似性</strong>を数値で比較可能にすることです。テキストをベクトル化することで、意味が近い文章はベクトル空間上で近い位置に配置され、類似度を計算できるようになります。</p>\n<p data-sourcepos=\"539:1-540:333\">記事では、高性能な<code>OpenAI Embeddings</code>を埋め込みモデルとして採用しています。<code>OpenAI Embeddings</code>は、OpenAI社が提供するAPIを通じて利用でき、テキストの意味を高精度に捉え、多言語 にも対応している点が特徴です。LangChain の <code>OpenAIEmbeddings</code>クラスで、OpenAI Embeddings APIを容易に扱えます。<br>\nOpenAI Embeddingsには、主に<code>text-embedding-ada-002</code>と <code>text-embedding-3(small/large)</code> の種類があり、それぞれ性能と価格が異なります。<code>text-embedding-ada-002</code>は、高性能かつ低価格 でバランスが良く、<code>text-embedding-3</code>は、さらに高性能を追求したい場合に適しています。</p>\n<p data-sourcepos=\"542:1-542:332\">また、埋め込みモデルは <code>OpenAI Embeddings</code>以外にも、Hugging Face Transformers の <code>Sentence-BERT</code>など、オープンソースで高性能なモデルが多数存在します。<code>Sentence-BERT</code>は、特に文章の類似度判定に優れており、RAGの検索精度向上に貢献する可能性があります。</p>\n<p data-sourcepos=\"544:1-544:339\">より専門的な内容に関しては、汎用モデルではなく、ドメイン固有のデータで事前学習やファインチューニングした埋め込みモデルを利用することで、専門用語や業界特有の言い回しを正確に理解できるようになり、さらなる精度向上が期待できます。</p>\n<h3 data-sourcepos=\"546:1-546:64\">\n<span id=\"4-ベクトルデータベースへの登録-vector-store\" class=\"fragment\"></span><a href=\"#4-%E3%83%99%E3%82%AF%E3%83%88%E3%83%AB%E3%83%87%E3%83%BC%E3%82%BF%E3%83%99%E3%83%BC%E3%82%B9%E3%81%B8%E3%81%AE%E7%99%BB%E9%8C%B2-vector-store\"><i class=\"fa fa-link\"></i></a>4. ベクトルデータベースへの登録 (Vector Store)</h3>\n<p data-sourcepos=\"547:1-547:317\">埋め込み生成ステップで得られた埋め込みベクトルを、<strong>ベクトルデータベース (Vector Store)</strong> に登録します。ベクトルデータベースは、大量のベクトルデータを効率的に管理し、類似検索を高速に行うことに特化したデータベースです。</p>\n<p data-sourcepos=\"549:1-549:93\">RAGシステムにおいて、ベクトルデータベースは以下の役割を担います</p>\n<ul data-sourcepos=\"550:1-554:0\">\n<li data-sourcepos=\"550:1-551:270\">\n<strong>大量ベクトルデータの効率的な管理:</strong><br>\nPDFドキュメント全体をベクトル化すると、大量の埋め込みベクトルが生成されます。ベクトルデータベースは、これらのベクトルデータを効率的に保存、管理、および検索するための基盤となります。</li>\n<li data-sourcepos=\"552:1-554:0\">\n<strong>高速な類似ベクトル検索:</strong><br>\n質問文をベクトル化し、ベクトルデータベースに対して類似検索を行うことで、質問文と意味的に類似するドキュメントやテキストチャンクを高速に見つけ出すことができます。</li>\n</ul>\n<p data-sourcepos=\"555:1-556:483\">記事では、学習用途に最適な<strong>ChromaDB</strong>をベクトルデータベースとして採用します。<br>\nChromaDBは、Python 環境で非常に扱いやすいオープンソースのベクトルデータベースです。インストールが簡単で、特別な設定なしにローカル環境ですぐに使い始められます。データはインメモリ(オプションで永続化も可能) で高速に処理され、LangChainとの連携もスムーズに行えるため、RAG構築の学習、プロトタイプ開発、そして個人利用に非常に適しています。</p>\n<p data-sourcepos=\"558:1-558:626\">また、ベクトルデータベースはほかにも<strong>FAISS</strong>や<strong>Pinecone</strong>など規模や目的に応じたものが存在します。<strong>FAISS</strong>はMeta社開発の高速類似検索ライブラリで、ローカル環境でも驚異的な速度でベクトル検索を実行でき、より高速な検索性能を求める場合や、大規模なデータを扱いたい場合に適しています。<strong>Pinecone</strong>は、高いスケーラビリティと可用性を備え、安定した運用と大規模データ処理を実現でき、商用利用や 大規模なRAGシステムを構築する場合に適しています。</p>\n<h3 data-sourcepos=\"560:1-560:17\">\n<span id=\"実装-1\" class=\"fragment\"></span><a href=\"#%E5%AE%9F%E8%A3%85-1\"><i class=\"fa fa-link\"></i></a>＜実装＞</h3>\n<p data-sourcepos=\"561:1-561:192\">以上の  2.ドキュメント分割、3.埋め込み生成(Embedding)、.4. ベクトルデータベースへの登録 (Vector Store)をまとめて行うコードを以下に示します。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"563:1-636:3\">\n<div class=\"code-lang\"><span class=\"bold\">create_db.py</span></div>\n<div class=\"highlight\"><pre><code><span class=\"kn\">from</span> <span class=\"n\">langchain_openai.embeddings</span> <span class=\"kn\">import</span> <span class=\"n\">AzureOpenAIEmbeddings</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain_chroma</span> <span class=\"kn\">import</span> <span class=\"n\">Chroma</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain_core.documents</span> <span class=\"kn\">import</span> <span class=\"n\">Document</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain.text_splitter</span> <span class=\"kn\">import</span> <span class=\"n\">RecursiveCharacterTextSplitter</span>\n<span class=\"kn\">import</span> <span class=\"n\">os</span>\n<span class=\"kn\">from</span> <span class=\"n\">dotenv</span> <span class=\"kn\">import</span> <span class=\"n\">load_dotenv</span>\n<span class=\"kn\">from</span> <span class=\"n\">tqdm</span> <span class=\"kn\">import</span> <span class=\"n\">tqdm</span>\n\n<span class=\"c1\"># 環境変数の読み込み\n</span><span class=\"nf\">load_dotenv</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Azure OpenAI の設定\n</span><span class=\"n\">AZURE_OPENAI_API_KEY</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">AZURE_OPENAI_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">AZURE_OPENAI_ENDPOINT</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">AZURE_OPENAI_ENDPOINT</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">API_VERSION</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">API_VERSION</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">DEPLOYMENT_ID_FOR_EMBEDDING</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">DEPLOYMENT_ID_FOR_EMBEDDING</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#　各チャンクの最大サイズ\n</span><span class=\"n\">chunk_size</span> <span class=\"o\">=</span> <span class=\"mi\">300</span>\n<span class=\"c1\"># チャンクサイズに対するオーバーラップの割合\n</span><span class=\"n\">overlap_ratio</span> <span class=\"o\">=</span> <span class=\"mf\">0.25</span>\n\n<span class=\"c1\"># LangChain の埋め込みクラスを初期化\n</span><span class=\"n\">embedding_model</span> <span class=\"o\">=</span> <span class=\"nc\">AzureOpenAIEmbeddings</span><span class=\"p\">(</span>\n    <span class=\"n\">deployment</span><span class=\"o\">=</span><span class=\"n\">DEPLOYMENT_ID_FOR_EMBEDDING</span><span class=\"p\">,</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">text-embedding-ada-002</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_key</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_API_KEY</span><span class=\"p\">,</span>\n    <span class=\"n\">azure_endpoint</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_ENDPOINT</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_version</span><span class=\"o\">=</span><span class=\"n\">API_VERSION</span><span class=\"p\">,</span>\n    <span class=\"n\">chunk_size</span><span class=\"o\">=</span><span class=\"n\">chunk_size</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Chroma データベースの初期化\n</span><span class=\"n\">output_db_folder</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">./chroma_db</span><span class=\"sh\">\"</span>\n<span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"nc\">Chroma</span><span class=\"p\">(</span><span class=\"n\">persist_directory</span><span class=\"o\">=</span><span class=\"n\">output_db_folder</span><span class=\"p\">,</span> <span class=\"n\">embedding_function</span><span class=\"o\">=</span><span class=\"n\">embedding_model</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Markdownファイルが保存されているフォルダ\n</span><span class=\"n\">input_folder</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">./data_sources/</span><span class=\"sh\">\"</span>\n<span class=\"c1\"># チャンク化用の設定\n</span><span class=\"n\">text_splitter</span> <span class=\"o\">=</span> <span class=\"nc\">RecursiveCharacterTextSplitter</span><span class=\"p\">(</span>\n    <span class=\"n\">chunk_size</span><span class=\"o\">=</span><span class=\"n\">chunk_size</span><span class=\"p\">,</span>  <span class=\"c1\"># 各チャンクの最大サイズ\n</span>    <span class=\"n\">chunk_overlap</span><span class=\"o\">=</span> <span class=\"nf\">int</span><span class=\"p\">(</span><span class=\"n\">chunk_size</span> <span class=\"o\">*</span> <span class=\"n\">overlap_ratio</span><span class=\"p\">)</span>  <span class=\"c1\"># チャンク間のオーバーラップ\n</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Markdownファイルを処理\n</span><span class=\"n\">md_files</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">f</span> <span class=\"k\">for</span> <span class=\"n\">f</span> <span class=\"ow\">in</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">listdir</span><span class=\"p\">(</span><span class=\"n\">input_folder</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"nf\">endswith</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">.md</span><span class=\"sh\">\"</span><span class=\"p\">)]</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">処理対象のMarkdownファイル: </span><span class=\"si\">{</span><span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">md_files</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s\"> 件</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">md_file</span> <span class=\"ow\">in</span> <span class=\"nf\">tqdm</span><span class=\"p\">(</span><span class=\"n\">md_files</span><span class=\"p\">,</span> <span class=\"n\">desc</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">Processing Markdown Files</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">unit</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">file</span><span class=\"sh\">\"</span><span class=\"p\">):</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">md_path</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">path</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">(</span><span class=\"n\">input_folder</span><span class=\"p\">,</span> <span class=\"n\">md_file</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Markdownファイルを読み込み\n</span>        <span class=\"k\">with</span> <span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">md_path</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">r</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">utf-8</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n            <span class=\"n\">md_text</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"nf\">read</span><span class=\"p\">()</span>\n\n        <span class=\"c1\"># チャンク化\n</span>        <span class=\"n\">chunks</span> <span class=\"o\">=</span> <span class=\"n\">text_splitter</span><span class=\"p\">.</span><span class=\"nf\">split_text</span><span class=\"p\">(</span><span class=\"n\">md_text</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># 各チャンクをDocumentオブジェクトとして作成\n</span>        <span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n            <span class=\"nc\">Document</span><span class=\"p\">(</span><span class=\"n\">page_content</span><span class=\"o\">=</span><span class=\"n\">chunk</span><span class=\"p\">,</span> <span class=\"n\">metadata</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"sh\">\"</span><span class=\"s\">source</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">md_file</span><span class=\"p\">})</span>\n            <span class=\"k\">for</span> <span class=\"n\">chunk</span> <span class=\"ow\">in</span> <span class=\"n\">chunks</span>\n        <span class=\"p\">]</span>\n\n        <span class=\"c1\"># テキストをベクトル化してChromaに保存\n</span>        <span class=\"n\">db</span><span class=\"p\">.</span><span class=\"nf\">add_documents</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">)</span>\n\n    <span class=\"k\">except</span> <span class=\"nb\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">エラーが発生しました: </span><span class=\"si\">{</span><span class=\"n\">md_file</span><span class=\"si\">}</span><span class=\"s\"> - </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">すべてのMarkdownファイルの処理が完了し、データベースが作成されました。出力先: </span><span class=\"si\">{</span><span class=\"n\">output_db_folder</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n<p data-sourcepos=\"637:1-639:120\">チャンクサイズはタスク、ドキュメントの種類、使用するモデル、そして目的によって大きく異なります。 例えば、技術書やFAQなど、特定の情報や手順をピンポイントで検索したケースでは比較的小さなチャンクサイズを設定しますが、小説や物語などの文脈の流れやキャラクターの関係性を理解することが重要な場合は大きなチャンクサイズを設定する必要があります。ただ、「これが正解」というサイズはなく、実験的に最適なサイズを見つけることが重要らしいです。<br>\nまた、チャンクオーバーラップも同様に明確な正解はありませんが、チャンクサイズの25％程度の値に設定することが多いらしいです。<br>\nこれらの値はRAGの精度に直結してくるものですので、慎重に調整を行う必要があります。</p>\n<hr data-sourcepos=\"641:1-641:3\">\n<p data-sourcepos=\"642:1-642:204\">以上で外部データの登録処理は完了です。次に、登録した外部データを活用し、RAGを用いた質問応答を実現するための検索および生成処理に進みます。</p>\n<h2 data-sourcepos=\"644:1-644:53\">\n<span id=\"43-検索と応答生成-retrieval--generation\" class=\"fragment\"></span><a href=\"#43-%E6%A4%9C%E7%B4%A2%E3%81%A8%E5%BF%9C%E7%AD%94%E7%94%9F%E6%88%90-retrieval--generation\"><i class=\"fa fa-link\"></i></a>4.3 検索と応答生成 (Retrieval &amp; Generation)</h2>\n<p data-sourcepos=\"645:1-645:391\">外部データの登録が完了したら、登録したデータを使って質問応答を行うフェーズです。このフェーズは大きく分けて 検索 (Retrieval) と 応答生成 (Generation) の2つのステップから構成されます。 ユーザーからの質問に対して、RAGシステムがどのように答えを導き出すのか、解説していきます。</p>\n<hr data-sourcepos=\"647:1-647:3\">\n<h3 data-sourcepos=\"648:1-648:45\">\n<span id=\"1-質問クエリの入力前処理\" class=\"fragment\"></span><a href=\"#1-%E8%B3%AA%E5%95%8F%E3%82%AF%E3%82%A8%E3%83%AA%E3%81%AE%E5%85%A5%E5%8A%9B%E5%89%8D%E5%87%A6%E7%90%86\"><i class=\"fa fa-link\"></i></a>1. 質問(クエリ)の入力・前処理</h3>\n<p data-sourcepos=\"649:1-649:162\">最初のステップでは、ユーザーからの質問（クエリ）を受け取り、RAGシステムが処理しやすいように前処理を行います。</p>\n<p data-sourcepos=\"651:1-651:222\">まず、ユーザーはRAGシステムに対して、自然言語で質問を入力します。例えば、「〇〇について教えてください」「〇〇のやり方は？」といった具体的な質問です。</p>\n<p data-sourcepos=\"653:1-653:159\">入力された質問は、そのままでは検索処理に利用できない場合があります。そこで、以下のような前処理を行います。</p>\n<ul data-sourcepos=\"655:1-661:0\">\n<li data-sourcepos=\"655:1-656:81\">トークン化：<br>\n質問文を単語や記号などの単位（トークン）に分割します。</li>\n<li data-sourcepos=\"657:1-658:129\">正規化：<br>\n表記ゆれの修正、小文字化、不要な記号の除去などを行い、質問文を統一的な形式に整えます。</li>\n<li data-sourcepos=\"659:1-661:0\">埋め込み (ベクトル化)：<br>\n前処理された質問文を、外部データ登録の際と同様に埋め込みモデルを用いてベクトルに変換します。ベクトル化することで、質問の意味内容を数値データとして表現し、後続の類似度検索を効率的に行う準備をします。</li>\n</ul>\n<p data-sourcepos=\"662:1-662:313\">また、より精度を向上させるために、ユーザーの質問をLLMに再定義させることで、曖昧な表現を明確化し、より具体的なクエリに変換する場合もあります。詳しい内容については、 第5章 精度向上のためのテクニック で解説します。</p>\n<hr data-sourcepos=\"664:1-664:3\">\n<h3 data-sourcepos=\"665:1-665:37\">\n<span id=\"2-類似度検索retrieval\" class=\"fragment\"></span><a href=\"#2-%E9%A1%9E%E4%BC%BC%E5%BA%A6%E6%A4%9C%E7%B4%A2retrieval\"><i class=\"fa fa-link\"></i></a>2. 類似度検索（Retrieval）</h3>\n<p data-sourcepos=\"666:1-666:234\">前処理でベクトル化された質問（クエリベクトル）を使い、登録された外部データの中から質問内容と関連性の高いチャンクを検索します。以下に主要な手順をまとめます。</p>\n<p data-sourcepos=\"668:1-668:49\"><strong>1. ベクトルデータベースとの照合</strong></p>\n<ul data-sourcepos=\"669:1-670:0\">\n<li data-sourcepos=\"669:1-670:0\">事前にベクトル化して登録しておいた外部データのチャンク（ドキュメントベクトル）と質問のベクトルをベクトルデータベース上で比較します。</li>\n</ul>\n<p data-sourcepos=\"671:1-671:22\"><strong>2. 類似度計算</strong></p>\n<ul data-sourcepos=\"672:1-674:0\">\n<li data-sourcepos=\"672:1-672:143\">ベクトル同士の類似度を計算します。類似度が高いほど、質問とチャンクの内容が近いと判断されます。</li>\n<li data-sourcepos=\"673:1-674:0\">\n<strong>コサイン類似度</strong>などがよく使用されます。</li>\n</ul>\n<p data-sourcepos=\"675:1-675:37\"><strong>3. チャンクのランキング</strong></p>\n<ul data-sourcepos=\"676:1-678:0\">\n<li data-sourcepos=\"676:1-676:68\">類似度が高い順にチャンクをランキングします。</li>\n<li data-sourcepos=\"677:1-678:0\">上位のチャンクほど、質問への回答に役立つ可能性が高いと考えられます。</li>\n</ul>\n<p data-sourcepos=\"679:1-679:26\"><strong>4. 上位K件の取得</strong></p>\n<ul data-sourcepos=\"680:1-682:0\">\n<li data-sourcepos=\"680:1-680:104\">ランキング上位から <strong>K件</strong>（事前に設定した数）のチャンクを取得します。</li>\n<li data-sourcepos=\"681:1-682:0\">取得されたチャンクは、次のステップの応答生成で利用されます。</li>\n</ul>\n<p data-sourcepos=\"683:1-683:228\">なお、取得するチャンクの数や類似度計算のアルゴリズムは、データの特性やタスクに応じて調整が必要です。試行錯誤を重ねて最適な設定を見つけることが重要です。</p>\n<hr data-sourcepos=\"685:1-685:3\">\n<h3 data-sourcepos=\"686:1-686:37\">\n<span id=\"3-llmへのプロンプト構築\" class=\"fragment\"></span><a href=\"#3-llm%E3%81%B8%E3%81%AE%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88%E6%A7%8B%E7%AF%89\"><i class=\"fa fa-link\"></i></a>3. LLMへのプロンプト構築</h3>\n<p data-sourcepos=\"687:1-687:162\">検索ステップで取得した関連性の高いチャンクを、LLM（大規模言語モデル）に入力するためのプロンプトを構築します。</p>\n<p data-sourcepos=\"689:1-689:239\">質問文に加えて、検索されたチャンクを文脈情報として <strong>指示文(プロンプト)</strong> に組み込みます。これにより、LLMは外部データの内容を踏まえて回答を生成することができます。</p>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"691:1-707:3\">\n<div class=\"code-lang\"><span class=\"bold\">プロンプト例</span></div>\n<div class=\"highlight\"><pre><code>＜質問＞ \n{ユーザーからの質問}\n\n＜参考情報＞\n{検索されたチャンク1}\n{検索されたチャンク2}\n...\n{検索されたチャンクK}\n\n＜指示＞\n1. 参考情報のみから判断してください。\n2. 参考情報から判断できない場合、「分かりません」と答えてください。\n3. 根拠となった参考情報を提示してください\n\n上記の参考情報と指示に基づいて、質問に答えてください。\n</code></pre></div>\n</div>\n<p data-sourcepos=\"709:1-710:81\">より良い回答を得るためには、プロンプトの表現や構成を工夫する<strong>プロンプトエンジニアリング</strong>が重要です。<br>\nプロンプトでは、以下のポイントを意識すると効果的です：</p>\n<ul data-sourcepos=\"711:3-714:0\">\n<li data-sourcepos=\"711:3-711:46\">必要な情報を簡潔に記載する。</li>\n<li data-sourcepos=\"712:3-712:70\">指示文を明確にし、期待する回答形式を伝える。</li>\n<li data-sourcepos=\"713:3-714:0\">文脈に応じて具体的な例やフォーマットを提示する。</li>\n</ul>\n<p data-sourcepos=\"715:1-715:419\">上の例では、指示1によりLLMは学習された知識ではなく、参考情報のみを用いて判断します。また、指示2により、参考情報に該当がない場合は「分かりません」と明確に回答するため、誤回答(ハルシネーション)が軽減できます。さらに、指示3によって、回答に根拠を含めることで、応答の透明性が向上します。</p>\n<p data-sourcepos=\"717:1-717:153\">プロンプト設計はLLMの解答精度に直結するため、適切な文脈付与と試行錯誤による調整が重要な工程となります。</p>\n<hr data-sourcepos=\"719:1-719:3\">\n<h3 data-sourcepos=\"720:1-720:35\">\n<span id=\"4-応答生成generation\" class=\"fragment\"></span><a href=\"#4-%E5%BF%9C%E7%AD%94%E7%94%9F%E6%88%90generation\"><i class=\"fa fa-link\"></i></a>4. 応答生成（Generation）</h3>\n<p data-sourcepos=\"721:1-721:452\">構築されたプロンプトをLLMに入力し、質問に対する回答を生成させます。LLMは、プロンプトに含まれる質問と文脈情報（検索されたチャンク）に基づいて、自然言語で回答を生成します。本記事では、同様にGPT 4o-miniのチャットモデルを使用していますが、使用するモデルはタスクの性質や応答の要件に応じて選択することが重要です。</p>\n<p data-sourcepos=\"723:1-723:299\">例えば、より高度な推論や詳細な回答が必要な場合には、GPT-4やLlama70Bのような大規模モデルを活用し、逆に簡潔な回答やリアルタイム性が重視されるタスクでは、Llama 2 7Bなどの軽量モデルを利用することが推奨されます。</p>\n<hr data-sourcepos=\"725:1-725:3\">\n<h3 data-sourcepos=\"726:1-726:34\">\n<span id=\"5-応答の後処理出力\" class=\"fragment\"></span><a href=\"#5-%E5%BF%9C%E7%AD%94%E3%81%AE%E5%BE%8C%E5%87%A6%E7%90%86%E5%87%BA%E5%8A%9B\"><i class=\"fa fa-link\"></i></a>5. 応答の後処理・出力</h3>\n<p data-sourcepos=\"727:1-727:135\">最後に、LLMによって生成された回答を、必要に応じて後処理し、ユーザーに出力するステップです。</p>\n<p data-sourcepos=\"729:1-729:81\">生成された回答に対して、以下のような後処理を行います：</p>\n<ul data-sourcepos=\"731:1-740:0\">\n<li data-sourcepos=\"731:1-733:0\">\n<p data-sourcepos=\"731:3-732:137\"><strong>言い換え</strong><br>\n回答文をより自然で分かりやすい表現に言い換えることで、ユーザーにとって理解しやすくします。</p>\n</li>\n<li data-sourcepos=\"734:1-736:0\">\n<p data-sourcepos=\"734:3-735:107\"><strong>不要な情報の削除</strong><br>\n回答に含まれるノイズや冗長な部分を取り除き、必要な情報のみを残します。</p>\n</li>\n<li data-sourcepos=\"737:1-740:0\">\n<p data-sourcepos=\"737:3-739:67\"><strong>フォーマット調整</strong><br>\n回答をユーザーインターフェースに適した形式に整形します。<br>\n例: マークダウン形式、箇条書き、表形式など。</p>\n</li>\n</ul>\n<p data-sourcepos=\"741:1-741:186\">その後、後処理された回答をユーザーに提示します。適切な形式で出力することで、ユーザーは質問に対する回答を得ることができます。</p>\n<hr data-sourcepos=\"743:1-743:3\">\n<h3 data-sourcepos=\"744:1-744:16\">\n<span id=\"実装-2\" class=\"fragment\"></span><a href=\"#%E5%AE%9F%E8%A3%85-2\"><i class=\"fa fa-link\"></i></a>＜実装＞</h3>\n<p data-sourcepos=\"745:1-745:173\">4.3 検索と応答生成 (Retrieval &amp; Generation)を実行するためのコードを以下に示します。LangChainを用いることで、簡単に実装できます。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"747:1-821:3\">\n<div class=\"code-lang\"><span class=\"bold\">query.py</span></div>\n<div class=\"highlight\"><pre><code><span class=\"kn\">from</span> <span class=\"n\">langchain_openai.embeddings</span> <span class=\"kn\">import</span> <span class=\"n\">AzureOpenAIEmbeddings</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain_chroma</span> <span class=\"kn\">import</span> <span class=\"n\">Chroma</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain.prompts</span> <span class=\"kn\">import</span> <span class=\"n\">PromptTemplate</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain_openai.chat_models</span> <span class=\"kn\">import</span> <span class=\"n\">AzureChatOpenAI</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain.schema</span> <span class=\"kn\">import</span> <span class=\"n\">HumanMessage</span>\n<span class=\"kn\">import</span> <span class=\"n\">os</span>\n<span class=\"kn\">from</span> <span class=\"n\">dotenv</span> <span class=\"kn\">import</span> <span class=\"n\">load_dotenv</span>\n\n<span class=\"nf\">load_dotenv</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Azure OpenAI の設定\n</span><span class=\"n\">AZURE_OPENAI_API_KEY</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">AZURE_OPENAI_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">AZURE_OPENAI_ENDPOINT</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">AZURE_OPENAI_ENDPOINT</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">API_VERSION</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">API_VERSION</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">DEPLOYMENT_ID_FOR_EMBEDDING</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">DEPLOYMENT_ID_FOR_EMBEDDING</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span><span class=\"sh\">\"</span><span class=\"p\">)</span>  <span class=\"c1\"># Chat用のデプロイ名\n</span>\n<span class=\"c1\"># LangChain の埋め込みクラスを初期化\n</span><span class=\"n\">embedding_model</span> <span class=\"o\">=</span> <span class=\"nc\">AzureOpenAIEmbeddings</span><span class=\"p\">(</span>\n    <span class=\"n\">deployment</span><span class=\"o\">=</span><span class=\"n\">DEPLOYMENT_ID_FOR_EMBEDDING</span><span class=\"p\">,</span>  <span class=\"c1\"># デプロイ名\n</span>    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">text-embedding-ada-002</span><span class=\"sh\">\"</span><span class=\"p\">,</span>  <span class=\"c1\"># 埋め込みモデル名\n</span>    <span class=\"n\">openai_api_key</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_API_KEY</span><span class=\"p\">,</span>\n    <span class=\"n\">azure_endpoint</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_ENDPOINT</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_version</span><span class=\"o\">=</span><span class=\"n\">API_VERSION</span><span class=\"p\">,</span>\n    <span class=\"n\">chunk_size</span><span class=\"o\">=</span><span class=\"mi\">300</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Azure ChatOpenAI を初期化\n</span><span class=\"n\">chat</span> <span class=\"o\">=</span> <span class=\"nc\">AzureChatOpenAI</span><span class=\"p\">(</span>\n    <span class=\"n\">deployment_name</span><span class=\"o\">=</span><span class=\"n\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span><span class=\"p\">,</span>  <span class=\"c1\"># Chat用のデプロイ名\n</span>    <span class=\"n\">openai_api_key</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_API_KEY</span><span class=\"p\">,</span>\n    <span class=\"n\">azure_endpoint</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_ENDPOINT</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_version</span><span class=\"o\">=</span><span class=\"n\">API_VERSION</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Chroma データベースの初期化\n</span><span class=\"n\">output_db_folder</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">./chroma_db</span><span class=\"sh\">\"</span>\n<span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"nc\">Chroma</span><span class=\"p\">(</span><span class=\"n\">persist_directory</span><span class=\"o\">=</span><span class=\"n\">output_db_folder</span><span class=\"p\">,</span> <span class=\"n\">embedding_function</span><span class=\"o\">=</span><span class=\"n\">embedding_model</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 質問の定義\n</span><span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">令和6年分の年末調整は前年と異なる部分がありますか？</span><span class=\"sh\">\"</span>\n\n<span class=\"c1\"># データベースから類似度の高いドキュメントを取得\n</span><span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"n\">db</span><span class=\"p\">.</span><span class=\"nf\">similarity_search</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ドキュメントの内容を結合\n</span><span class=\"n\">documents_string</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">([</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">---------------------------</span><span class=\"se\">\\n</span><span class=\"si\">{</span><span class=\"n\">doc</span><span class=\"p\">.</span><span class=\"n\">page_content</span><span class=\"si\">}</span><span class=\"sh\">\"</span> <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">documents</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># プロンプトテンプレートを初期化（1段階に統合）\n</span><span class=\"n\">combined_prompt</span> <span class=\"o\">=</span> <span class=\"nc\">PromptTemplate</span><span class=\"p\">(</span>\n    <span class=\"n\">template</span><span class=\"o\">=</span><span class=\"sh\">\"\"\"</span><span class=\"s\">以下の文章と質問を基にして、質問に対する答えを日本語で出力してください。\n\n1. 文章のみから判断してください。\n2. 文章から全く判断できない場合、「分かりません」と答えてください。\n3. ソースとなった文章を提示してください\n\n文章:\n{document}\n\n質問: {query}</span><span class=\"sh\">\"\"\"</span><span class=\"p\">,</span>\n    <span class=\"n\">input_variables</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">document</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">query</span><span class=\"sh\">\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># チャットモデルに問い合わせ\n</span><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">chat</span><span class=\"p\">.</span><span class=\"nf\">invoke</span><span class=\"p\">([</span>\n    <span class=\"nc\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"n\">combined_prompt</span><span class=\"p\">.</span><span class=\"nf\">format</span><span class=\"p\">(</span><span class=\"n\">document</span><span class=\"o\">=</span><span class=\"n\">documents_string</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"o\">=</span><span class=\"n\">query</span><span class=\"p\">))</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">answer</span> <span class=\"o\">=</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">content</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># 結果を出力\n</span><span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">質問: </span><span class=\"si\">{</span><span class=\"n\">query</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">回答: </span><span class=\"si\">{</span><span class=\"n\">answer</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n</code></pre></div>\n</div>\n<p data-sourcepos=\"822:1-822:90\">上のコードを実行すると、以下のような出力結果が得られました。</p>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"823:1-829:3\">\n<div class=\"code-lang\"><span class=\"bold\">質問回答結果</span></div>\n<div class=\"highlight\"><pre><code>質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n回答: はい、令和6年分の年末調整には前年と比べて変わった点があります。具体的には、令和6年分所得税について定額による特別控除（定額減税）が実施されていることです。\n\nソース:\n「令和6年分所得税について、定額による所得税の特別控除（以下「**定額減税**」といいます。）が実施されています。」\n</code></pre></div>\n</div>\n<p data-sourcepos=\"832:1-832:364\">上のコードでは類似度の高いドキュメント上位5件を取得していますが、タスクによって取得件数を調整することが重要です。例えば、広範な情報が必要な場合は件数を増やし、逆に精度重視で特定の情報を求める場合は件数を減らすことで、応答の質を最適化できます。</p>\n<hr data-sourcepos=\"834:1-834:3\">\n<p data-sourcepos=\"835:1-835:333\">以上で基本的なRAGシステム構築は完了となります。しかし、実際の運用では、タスクやデータに合わせてシステムをカスタマイズすることが求められます。次の章では、さらに応答精度を向上させるための具体的なテクニックについて解説します。</p>\n<h1 data-sourcepos=\"837:1-837:49\">\n<span id=\"第5章-精度向上のためのテクニック\" class=\"fragment\"></span><a href=\"#%E7%AC%AC5%E7%AB%A0-%E7%B2%BE%E5%BA%A6%E5%90%91%E4%B8%8A%E3%81%AE%E3%81%9F%E3%82%81%E3%81%AE%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF\"><i class=\"fa fa-link\"></i></a>第5章 精度向上のためのテクニック</h1>\n<p data-sourcepos=\"838:1-838:808\">精度向上のためには、まず<strong>PDFからのデータ抽出が正確に行われていることが大前提</strong>です。そのうえで、より高性能な埋め込みモデルを使用したり、ベクトルデータベースを最新のものに置き換えたり、あるいはチャットモデルを高性能なものに切り替えたりするなど、システムの基盤的なアップグレードが挙げられます。しかし実運用においては、システムのアップグレードだけでは十分に精度が上がらない状況に直面することもあるかと思います。そのため、本章ではこれらの基本的な手法に加え、より細かな工夫や補助的な方法によって、さらなる精度の向上を図るテクニックを紹介します。</p>\n<h2 data-sourcepos=\"840:1-840:36\">\n<span id=\"1-チャンクサイズの調整\" class=\"fragment\"></span><a href=\"#1-%E3%83%81%E3%83%A3%E3%83%B3%E3%82%AF%E3%82%B5%E3%82%A4%E3%82%BA%E3%81%AE%E8%AA%BF%E6%95%B4\"><i class=\"fa fa-link\"></i></a>1. チャンクサイズの調整</h2>\n<p data-sourcepos=\"841:1-841:324\">チャンクサイズとは、文章を分割する際の一塊あたりの長さを指します。小さすぎると文脈が分断されて本来は関係の深い情報が分散され、関連性の薄い情報まで同時に取り込んでしまい、類似度算出が不正確になるデメリットがあります。</p>\n<p data-sourcepos=\"843:1-843:291\">以下に、年末調整に関するPDFデータを使用し、チャンクサイズを 50、300、600 に変更した際の出力結果を示します。なお、チャンクの重なり（オーバーラップ）は、それぞれのチャンクサイズの 25% に設定しています。</p>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"845:1-848:3\">\n<div class=\"code-lang\"><span class=\"bold\">チャンクサイズ=50</span></div>\n<div class=\"highlight\"><pre><code>質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n回答: 分かりません。\n</code></pre></div>\n</div>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"849:1-852:3\">\n<div class=\"code-lang\"><span class=\"bold\">チャンクサイズ=300</span></div>\n<div class=\"highlight\"><pre><code>質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n回答: はい、令和6年分の年末調整には前年と異なる部分があり、特に定額減税の実施が新たに行われています。\n</code></pre></div>\n</div>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"854:1-857:3\">\n<div class=\"code-lang\"><span class=\"bold\">チャンクサイズ=600</span></div>\n<div class=\"highlight\"><pre><code>質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n回答: 令和6年分の年末調整には前年と異なる部分があります。特に、令和6年分所得税について定額減税が実施されており、その年末調整時点の定額減税の額を算出し、年間の所得税額の計算を行う必要があることが明記されています。また、合計所得金額が1,805万円を超える場合、年調減税額を控除しないなどの規定も変更されています。\n</code></pre></div>\n</div>\n<p data-sourcepos=\"858:1-860:568\">この結果から、チャンクサイズの調整は、質問応答の精度に大きく影響を与えることが分かります。<br>\nチャンクサイズが小さい場合（50）、質問に対する回答を得られませんでした。これは、チャンクが短すぎるために、質問に必要な情報がチャンク内に含まれていなかった、もしくは文脈が途切れてしまい、AIが質問とチャンクの内容を適切に紐付けられなかったと考えられます。<br>\n一方、チャンクサイズを大きくするにつれて（300、600）、回答の質が向上しています。チャンクサイズ300では、令和6年分の年末調整に前年と異なる点があること、そしてその一つが定額減税であることを捉えられています。さらにチャンクサイズ600では、定額減税に関するより詳細な情報、例えば年末調整時点での減税額算出や、合計所得金額による規定の変更など、より深い情報まで回答に含めることができています。</p>\n<p data-sourcepos=\"862:1-862:180\">このように、チャンクサイズの調整は、簡単でありながら質問応答システムの性能を大きく左右する、非常に重要な要素と言えます。</p>\n<h2 data-sourcepos=\"864:1-864:42\">\n<span id=\"2-取得するチャンク数の設定\" class=\"fragment\"></span><a href=\"#2-%E5%8F%96%E5%BE%97%E3%81%99%E3%82%8B%E3%83%81%E3%83%A3%E3%83%B3%E3%82%AF%E6%95%B0%E3%81%AE%E8%A8%AD%E5%AE%9A\"><i class=\"fa fa-link\"></i></a>2. 取得するチャンク数の設定</h2>\n<p data-sourcepos=\"865:1-865:489\">チャンクベースで検索や処理を行う場合、検索時に取得するチャンク数が少なすぎると必要な情報を十分に得られず、逆に多すぎると不要な情報が混在してノイズとなり、有用な情報が埋もれてしまう場合があります。したがって、アプリケーションの目的や文書の内容、ユーザーのクエリの特性に応じて、取得するチャンク数を適切に設定することが重要です。</p>\n<p data-sourcepos=\"867:1-867:675\">具体的には、チャンク数が少ないと文脈や細部が欠落してしまい、回答の根拠が不十分になるリスクがあります。一方で、チャンク数が多すぎると、処理コストが増大するだけでなく、ノイズが大きくなり回答が曖昧になりやすくなります。ただし一部では、ノイズの種類によっては背景知識の追加として機能し、回答の精度向上につながったケースが報告されています。こういった例外的な状況も考慮しつつ、実際のテストやチューニングを繰り返して最適なチャンク数を見極めることが重要です。</p>\n<h2 data-sourcepos=\"868:1-868:30\">\n<span id=\"3-メタデータの活用\" class=\"fragment\"></span><a href=\"#3-%E3%83%A1%E3%82%BF%E3%83%87%E3%83%BC%E3%82%BF%E3%81%AE%E6%B4%BB%E7%94%A8\"><i class=\"fa fa-link\"></i></a>3. メタデータの活用</h2>\n<p data-sourcepos=\"869:1-869:306\">文章の内容だけでなく、メタデータと呼ばれる文書に付随する情報を効果的に利用することも重要です。メタデータとは、作成日時や著者名、文書の種類、部門やカテゴリーなど、テキストそのもの以外の属性情報を指します。</p>\n<p data-sourcepos=\"871:1-871:43\"><strong>＜メタデータが果たす役割＞</strong></p>\n<ul data-sourcepos=\"872:1-880:0\">\n<li data-sourcepos=\"872:1-874:0\">\n<p data-sourcepos=\"872:3-873:351\"><strong>検索の効率化</strong><br>\n検索やリランキングの際にメタデータを活用すると、関連性の高いドキュメントを素早く絞り込めます。たとえば、“契約書”だけを検索対象とする、特定の年月に作成された文書に限定するなど、カテゴリや時系列に基づくフィルタリングが簡単に行えます。</p>\n</li>\n<li data-sourcepos=\"875:1-877:0\">\n<p data-sourcepos=\"875:3-876:309\"><strong>ノイズの除去</strong><br>\nメタデータを使って、目的に合わないドキュメントをあらかじめ排除できます。たとえば、対象が「社内規定」に関する文書のみの場合、それ以外の業務マニュアルや顧客資料を省くことができ、回答の精度を向上させられます。</p>\n</li>\n<li data-sourcepos=\"878:1-880:0\">\n<p data-sourcepos=\"878:3-879:393\"><strong>リランキングへの貢献</strong><br>\nRAGにおけるリランキング工程では、クエリと文書内容の類似度だけでなく、メタデータに基づくスコアリングを加味することで、より的確な文書を上位に表示できます。たとえば、最近更新された文書を優先する、特定の著者や部署が作成した文書を高評価するなどの仕組みが考えられます。</p>\n</li>\n</ul>\n<p data-sourcepos=\"881:1-881:43\"><strong>＜メタデータ活用の実装例＞</strong></p>\n<ul data-sourcepos=\"882:1-890:0\">\n<li data-sourcepos=\"882:1-884:0\">\n<p data-sourcepos=\"882:3-883:297\"><strong>部門タグを使ったフィルタリング</strong><br>\n文書に「経理部」「人事部」などのタグが付与されている場合、クエリが「給与計算」「雇用契約」などのテーマを含んでいるときは「経理部」「人事部」のタグがある文書を優先的に検索・リランキングの対象とする。</p>\n</li>\n<li data-sourcepos=\"885:1-887:0\">\n<p data-sourcepos=\"885:3-886:231\"><strong>時系列情報の活用</strong><br>\n「更新日」が含まれるメタデータを用いて、最新の文書を上位に表示する。税制や法律の情報は古い文書よりも最新の文書が正確な場合が多いため、回答の精度が上がる。</p>\n</li>\n<li data-sourcepos=\"888:1-890:0\">\n<p data-sourcepos=\"888:3-889:244\"><strong>セクション情報の活用</strong><br>\n文書を章や節ごとに分割し、その構造情報をメタデータとして扱う。ユーザーが「第3章の細則について詳しく教えて」と尋ねた場合に、該当セクションに絞った情報を提示できる。</p>\n</li>\n</ul>\n<p data-sourcepos=\"891:1-891:46\"><strong>＜メタデータ活用時の注意点＞</strong></p>\n<ul data-sourcepos=\"892:1-900:0\">\n<li data-sourcepos=\"892:1-894:0\">\n<p data-sourcepos=\"892:3-893:201\"><strong>メタデータの整備</strong><br>\nメタデータが正確に整備されていないと、誤ったフィルタリングやリランキングの原因になります。データの標準化や定期的な更新管理が必要です。</p>\n</li>\n<li data-sourcepos=\"895:1-897:0\">\n<p data-sourcepos=\"895:3-896:249\"><strong>過剰な制限に注意</strong><br>\nメタデータで厳しくフィルタリングしすぎると、有用な文書を見落とす可能性があります。必要に応じて柔軟にスコアリングを調整し、情報を狭めすぎないようにすることが重要です。</p>\n</li>\n<li data-sourcepos=\"898:1-900:0\">\n<p data-sourcepos=\"898:3-899:195\"><strong>セキュリティとの兼ね合い</strong><br>\n部門やアクセス権限などのメタデータを扱う場合、誤って機密情報が検索・リランキングの対象にならないように、適切な権限管理が必要です。</p>\n</li>\n</ul>\n<p data-sourcepos=\"901:1-901:409\">メタデータの活用は、<strong>単にテキストベースの検索では拾いきれないドキュメントの属性情報を検索・リランキングに反映するための有力な手段</strong>です。正しく管理されたメタデータを活用すれば、RAGワークフロー全体の精度を高めるだけでなく、回答をより効率的かつ適切に取得できるようになります。</p>\n<h2 data-sourcepos=\"902:1-902:39\">\n<span id=\"4-検索アルゴリズムの選択\" class=\"fragment\"></span><a href=\"#4-%E6%A4%9C%E7%B4%A2%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0%E3%81%AE%E9%81%B8%E6%8A%9E\"><i class=\"fa fa-link\"></i></a>4. 検索アルゴリズムの選択</h2>\n<p data-sourcepos=\"904:1-904:266\">検索アルゴリズムを選定する際には、データの特性や検索の目的に応じて適切な手法を選ぶことが重要です。検索手法は、大きく分けて「<strong>類似度検索</strong>」と「<strong>キーワード検索</strong>」に分類できます。</p>\n<p data-sourcepos=\"906:1-906:25\"><strong>＜類似度検索＞</strong></p>\n<p data-sourcepos=\"908:1-909:93\">類似度検索は、文章やデータ同士の「<strong>意味的な近さ</strong>」を計算し、関連性の高い情報を抽出する手法です。<br>\n代表的な類似度計算の方法としては、以下のようなものがあります。</p>\n<ul data-sourcepos=\"911:1-919:0\">\n<li data-sourcepos=\"911:1-912:124\">\n<strong>コサイン類似度</strong><br>\nベクトルの角度を基準に類似性を測る手法で、テキスト検索や文書分類に適しています。</li>\n<li data-sourcepos=\"913:1-914:139\">\n<strong>ユークリッド距離</strong><br>\nデータ間の直線距離を計算する方法で、数値データや位置情報の類似度を求める際に用いられます。</li>\n<li data-sourcepos=\"915:1-916:142\">\n<strong>マンハッタン距離</strong><br>\n縦横の移動のみを考慮した距離計算方法で、高次元データの類似度評価に適している場合があります。</li>\n<li data-sourcepos=\"917:1-919:0\">\n<strong>ジャカード係数</strong><br>\n集合の共通要素の割合を比較する手法で、タグベース（キーワードやカテゴリ情報）の類似度計算に有効です。</li>\n</ul>\n<p data-sourcepos=\"920:1-920:211\">類似度検索では、<strong>文章全体の文脈</strong>を考慮した検索が可能ですが、専門用語や固有名詞がベクトル化される過程で本来の意味が失われるリスクもあります。</p>\n<p data-sourcepos=\"922:1-922:31\"><strong>＜キーワード検索＞</strong></p>\n<p data-sourcepos=\"924:1-924:114\">キーワード検索は、特定の単語やフレーズを文字列として厳密に検索する方法です。</p>\n<ul data-sourcepos=\"926:1-928:0\">\n<li data-sourcepos=\"926:1-926:113\">法律文書や商品名、社名など、<strong>正確な一致</strong>が求められる場面に適しています。</li>\n<li data-sourcepos=\"927:1-928:0\">言葉の表現が異なる場合にヒットしないなど、<strong>言い換えへの対応が困難</strong>という課題があります。</li>\n</ul>\n<p data-sourcepos=\"929:1-929:46\"><strong>＜組み合わせによる精度向上＞</strong></p>\n<p data-sourcepos=\"931:1-931:190\">実際の検索システムでは、以下のように<strong>キーワード検索と類似度検索を組み合わせる</strong>ことで、より精度の高い情報取得が可能になります。</p>\n<ol data-sourcepos=\"933:1-937:0\">\n<li data-sourcepos=\"933:1-934:74\">\n<strong>キーワード検索で候補を絞り込む</strong><br>\nまずは厳密な文字列一致により、範囲を狭めます。</li>\n<li data-sourcepos=\"935:1-937:0\">\n<strong>類似度検索を使って、関連性の高い情報を取得する</strong><br>\n候補の中から、文脈や意味に基づいて最適な結果を得ます。</li>\n</ol>\n<p data-sourcepos=\"938:1-938:294\">このように、<strong>キーワード検索の正確性</strong>と<strong>類似度検索の柔軟性</strong>を組み合わせることで、<strong>精度と効率</strong>を両立できます。検索の目的やデータの特性に応じて、どの手法をどのように組み合わせるかを検討しましょう。</p>\n<h2 data-sourcepos=\"940:1-940:24\">\n<span id=\"5-リランキング\" class=\"fragment\"></span><a href=\"#5-%E3%83%AA%E3%83%A9%E3%83%B3%E3%82%AD%E3%83%B3%E3%82%B0\"><i class=\"fa fa-link\"></i></a>5. リランキング</h2>\n<p data-sourcepos=\"941:1-941:448\">RAGのプロセスでは、まず検索段階で関連性の高い文書を取得し、それらを元に最終的な文章生成を行います。しかし、一次検索（ファーストパス）で取り出した文書が必ずしも最適とは限りません。そこで、<strong>リランキング</strong>という手法を用いて、検索結果を再評価し、最も関連性の高い文書を上位に並べ直すことが重要になります。</p>\n<p data-sourcepos=\"943:1-943:52\"><strong>＜RAGにおけるリランキングの流れ＞</strong></p>\n<ol data-sourcepos=\"945:1-955:0\">\n<li data-sourcepos=\"945:1-947:0\">\n<p data-sourcepos=\"945:4-945:36\"><strong>一次検索（Retrieval）</strong></p>\n<ul data-sourcepos=\"946:4-947:0\">\n<li data-sourcepos=\"946:4-947:0\">大まかな手法（たとえばベクトル類似度検索）で候補文書を抽出し、その中からある程度数を絞り込みます。</li>\n</ul>\n</li>\n<li data-sourcepos=\"948:1-951:0\">\n<p data-sourcepos=\"948:4-948:43\"><strong>リランキング（Re-ranking）</strong></p>\n<ul data-sourcepos=\"949:4-951:0\">\n<li data-sourcepos=\"949:4-949:161\">抽出された候補文書を、より高度な手法や追加の特徴量を用いて<strong>再スコアリング</strong>し、順位付けをやり直します。</li>\n<li data-sourcepos=\"950:4-951:0\">例：大規模言語モデル（LLM）の埋め込みを使ったより詳細な類似度計算など</li>\n</ul>\n</li>\n<li data-sourcepos=\"952:1-955:0\">\n<p data-sourcepos=\"952:4-952:59\"><strong>最終的な文章生成（Augmented Generation）</strong></p>\n<ul data-sourcepos=\"953:4-955:0\">\n<li data-sourcepos=\"953:4-955:0\">リランキングによって厳選された文書をコンテキストとして、LLMに文章生成を行わせます。</li>\n</ul>\n</li>\n</ol>\n<p data-sourcepos=\"956:1-956:55\"><strong>＜リランキングの必要性とメリット＞</strong></p>\n<ul data-sourcepos=\"958:1-966:0\">\n<li data-sourcepos=\"958:1-960:0\">\n<p data-sourcepos=\"958:3-959:194\"><strong>検索精度の向上</strong><br>\nLLMは入力の先頭部分に注目しやすい特性があります。そのを活かし、関連性の高い文書を上位に配置することで、最適な回答生成を促します。</p>\n</li>\n<li data-sourcepos=\"961:1-963:0\">\n<p data-sourcepos=\"961:3-962:251\"><strong>追加の特徴量やアルゴリズムを反映</strong><br>\nファーストパスで単純なベクトル検索を行い、リランキング段階で高度なテキスト埋め込みやモデル推論結果を取り入れることで、計算コストを抑えながらも精度を向上させられます。</p>\n</li>\n<li data-sourcepos=\"964:1-966:0\">\n<p data-sourcepos=\"964:3-965:185\"><strong>柔軟なフィルタリング</strong><br>\n不要な情報やドメインが違う文書を後から除外するなど、リランキングで再度スコアリングすることでノイズ除去がしやすくなります。</p>\n</li>\n</ul>\n<p data-sourcepos=\"967:1-967:40\"><strong>＜リランキングの実装例＞</strong></p>\n<ol data-sourcepos=\"969:1-978:0\">\n<li data-sourcepos=\"969:1-973:0\">\n<p data-sourcepos=\"969:4-969:40\"><strong>BM25 + LLM埋め込み再評価</strong></p>\n<ul data-sourcepos=\"970:4-973:0\">\n<li data-sourcepos=\"970:4-970:92\">ファーストパス：BM25などの伝統的な検索手法で候補を絞り込み</li>\n<li data-sourcepos=\"971:4-971:133\">リランキング：LLMを使って埋め込みベクトルを生成し、コサイン類似度などで再スコアリング</li>\n<li data-sourcepos=\"972:4-973:0\">生成：厳選した上位文書をプロンプトに組み込み、回答を生成</li>\n</ul>\n</li>\n<li data-sourcepos=\"974:1-978:0\">\n<p data-sourcepos=\"974:4-974:75\"><strong>キーワード検索 + 類似度計算モデルの組み合わせ</strong></p>\n<ul data-sourcepos=\"975:4-978:0\">\n<li data-sourcepos=\"975:4-975:76\">ファーストパス：キーワード検索で高速に候補抽出</li>\n<li data-sourcepos=\"976:4-976:146\">リランキング：BERT系モデルなどの文章埋め込みを使い、文脈に応じた類似度を計算して順位を付け直す</li>\n<li data-sourcepos=\"977:4-978:0\">生成：リランキング後の文書を追加コンテキストにして生成モデルを動作させる</li>\n</ul>\n</li>\n</ol>\n<p data-sourcepos=\"979:1-979:19\"><strong>＜注意点＞</strong></p>\n<ul data-sourcepos=\"981:1-987:0\">\n<li data-sourcepos=\"981:1-983:0\">\n<p data-sourcepos=\"981:3-982:185\"><strong>計算コスト</strong><br>\nリランキングに高負荷なモデルを使うと、その分計算コストが上がります。適度に絞り込んだ上でリランキングを行う戦略が重要です。</p>\n</li>\n<li data-sourcepos=\"984:1-987:0\">\n<p data-sourcepos=\"984:3-985:197\"><strong>過剰なフィルタリング</strong><br>\n絞り込みすぎると、本来意味がある文書を逃してしまう可能性があります。設定するスコアの閾値や候補数をチューニングすることが大切です。</p>\n</li>\n</ul>\n<p data-sourcepos=\"988:1-988:332\">RAGワークフローで高品質な回答を得るには、<strong>ファーストパス検索</strong>で幅広く候補文書を取得しつつ、<strong>リランキング</strong>でより高度に関連度を評価して最終的に厳選した文書をLLMに渡すことで、ノイズを抑えた、より正確な生成結果が期待できます。</p>\n<h2 data-sourcepos=\"990:1-990:45\">\n<span id=\"6-プロンプトエンジニアリング\" class=\"fragment\"></span><a href=\"#6-%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2%E3%83%AA%E3%83%B3%E3%82%B0\"><i class=\"fa fa-link\"></i></a>6. プロンプトエンジニアリング</h2>\n<p data-sourcepos=\"991:1-992:180\">RAGの精度を向上させるためには、<strong>検索結果の適切な利用と、LLMへの指示の最適化</strong>が不可欠です。<br>\n適切なプロンプトエンジニアリングを行うことで、データに基づいた正確な回答を得ることが可能になります。以下に例を示します。</p>\n<p data-sourcepos=\"994:1-994:55\"><strong>＜プロンプトエンジニアリングの例＞</strong></p>\n<p data-sourcepos=\"996:1-997:230\"><strong>1. 質問と参考情報の構造を明確にする</strong><br>\n検索結果をそのまま並べるのではなく、<strong>質問部分と関連文書（参考情報）を区別</strong>して提示することで、モデルがどの情報をもとに回答すべきかを理解しやすくします。</p>\n<p data-sourcepos=\"999:2-999:35\"><strong>例：構造化プロンプト</strong></p>\n<div class=\"code-frame\" data-lang=\"plaintext\" data-sourcepos=\"1000:1-1007:3\"><div class=\"highlight\"><pre><code>【質問】  \n2024年の所得税控除の変更点は？  \n\n【参考情報】  \n- 所得税控除は2024年に5万円増額されました。  \n- 医療費控除には小規模な変更がありました。\n</code></pre></div></div>\n<p data-sourcepos=\"1008:1-1008:22\"><strong>なぜ有効か？</strong></p>\n<ul data-sourcepos=\"1009:1-1011:0\">\n<li data-sourcepos=\"1009:1-1009:80\">モデルが情報の役割（質問・情報源）を明確に把握できる</li>\n<li data-sourcepos=\"1010:1-1011:0\">検索結果に基づく回答を促し、余計な推論を抑制できる</li>\n</ul>\n<hr data-sourcepos=\"1012:1-1012:3\">\n<p data-sourcepos=\"1013:1-1014:271\"><strong>2. 「分からない」と答えることを許可する</strong><br>\nLLMは情報が不足していても、推測を交えて回答を生成しようとする傾向があります。そこで、<strong>明確な情報がない場合は「分かりません」と答えるように促す</strong>と、誤った回答を減らすことができます。</p>\n<p data-sourcepos=\"1016:1-1016:55\"><strong>例：不確実な回答を避けるプロンプト</strong></p>\n<div class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"1017:1-1025:3\"><div class=\"highlight\"><pre><code>以下の情報のみを基に質問に回答してください。\nもし、明確な情報が見つからない場合は、「分かりません」と回答してください。\n\n【質問】  \n ~~~~~\n【参考情報】\n ~~~~~\n</code></pre></div></div>\n<p data-sourcepos=\"1026:1-1026:22\"><strong>なぜ有効か？</strong></p>\n<ul data-sourcepos=\"1027:1-1029:0\">\n<li data-sourcepos=\"1027:1-1027:62\">不確実な情報を無理に生成するリスクを低減</li>\n<li data-sourcepos=\"1028:1-1029:0\">データが不足している際に誤答が生じる可能性を抑制</li>\n</ul>\n<hr data-sourcepos=\"1030:1-1030:3\">\n<p data-sourcepos=\"1031:1-1032:241\"><strong>3. データベースの情報のみを基に判断させる</strong><br>\nLLMは、事前学習された知識をもとに推論しがちです。そこで、<strong>検索結果のみに基づいて回答するよう、あらかじめ指示する</strong>ことで、RAGの目的に沿った回答を得やすくなります。</p>\n<p data-sourcepos=\"1034:1-1034:46\"><strong>例：推測を抑制するプロンプト</strong></p>\n<div class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"1035:1-1043:3\"><div class=\"highlight\"><pre><code>あなたはデータベースから取得した情報のみを基に回答するアシスタントです。\n事前知識や推測を加えず、以下の参考情報のみをもとに質問に回答してください。\n\n【質問】  \n ~~~~~\n【参考情報】 \n ~~~~~\n</code></pre></div></div>\n<p data-sourcepos=\"1045:1-1045:22\"><strong>なぜ有効か？</strong></p>\n<ul data-sourcepos=\"1046:1-1047:0\">\n<li data-sourcepos=\"1046:1-1047:0\">事前学習された知識による推測を抑え、検索結果ベースの回答を生成できる。</li>\n</ul>\n<hr data-sourcepos=\"1048:2-1048:4\">\n<p data-sourcepos=\"1049:1-1050:219\"><strong>4. 回答のフォーマットを指定する</strong><br>\nモデルが不要な情報を追加しないよう、回答形式（箇条書き・表形式・最大文字数など）を指定すると、情報が整理され、誤答や冗長な回答が減る傾向があります</p>\n<p data-sourcepos=\"1052:1-1052:43\"><strong>例：箇条書きでの回答を指示</strong></p>\n<div class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"1053:1-1060:3\"><div class=\"highlight\"><pre><code>次の情報を参考に、質問に対して 「簡潔な箇条書き」 で回答してください。\n\n【質問】  \n ~~~~~\n【参考情報】 \n ~~~~~\n</code></pre></div></div>\n<p data-sourcepos=\"1061:1-1061:22\"><strong>なぜ有効か？</strong></p>\n<ul data-sourcepos=\"1062:1-1062:95\">\n<li data-sourcepos=\"1062:1-1062:95\">冗長な回答や背景知識の羅列を防ぎ、要点を簡潔にまとめさせやすい</li>\n</ul>\n<hr data-sourcepos=\"1063:1-1063:3\">\n<p data-sourcepos=\"1064:1-1066:37\"><strong>5. 生成のステップを分割し、段階的に回答させる</strong><br>\n一度に結論を出させるのではなく、「<strong>関連情報の要約</strong>」→「<strong>最終回答</strong>」 のように複数ステップに分けると、モデルが文脈を整理しやすくなり、回答の精度が向上します。<br>\n<strong>例：ステップを分割する</strong></p>\n<div class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"1067:1-1074:3\"><div class=\"highlight\"><pre><code>① まず、以下の参考情報を要約してください。\n② その要約をもとに、質問に回答してください。\n【質問】  \n ~~~~~\n【参考情報】 \n ~~~~~\n</code></pre></div></div>\n<p data-sourcepos=\"1075:1-1075:22\"><strong>なぜ有効か？</strong></p>\n<ul data-sourcepos=\"1076:1-1076:101\">\n<li data-sourcepos=\"1076:1-1076:101\">長い文章や複雑な情報を段階的に処理し、より正確な回答を導きやすい。</li>\n</ul>\n<hr data-sourcepos=\"1077:1-1077:3\">\n<p data-sourcepos=\"1078:1-1079:216\">上記のようなプロンプトエンジニアリングを行うことで、RAGの検索結果を最大限に活用し、モデルの不必要な推測を抑えながら正確な回答を引き出せます。<br>\nこれらはあくまで一例であり、実際のユースケースに応じてさらに細かい指示や別の工夫を取り入れることで、RAGの精度と信頼性を一層高めることが可能です。</p>\n<h2 data-sourcepos=\"1081:1-1081:21\">\n<span id=\"7-クエリ拡張\" class=\"fragment\"></span><a href=\"#7-%E3%82%AF%E3%82%A8%E3%83%AA%E6%8B%A1%E5%BC%B5\"><i class=\"fa fa-link\"></i></a>7. クエリ拡張</h2>\n<p data-sourcepos=\"1083:1-1083:368\">RAGの精度を向上させるためには、検索クエリを適切に拡張し、より関連性の高い情報を取得することが重要です。<strong>クエリ拡張（Query Expansion）</strong> は、元のクエリをより詳細にリフレーズしたり、関連する表現を追加したりすることで、検索結果の精度を向上させる手法です。</p>\n<p data-sourcepos=\"1085:1-1085:60\">例えば、以下のような状況が考えられます。</p>\n<ul data-sourcepos=\"1086:1-1089:0\">\n<li data-sourcepos=\"1086:1-1086:184\">ユーザーの入力が曖昧な場合（例：「控除の申請はどうするの？」 → 「年末調整における扶養控除の申請方法を教えてください。」）</li>\n<li data-sourcepos=\"1087:1-1087:106\">同義語や類義語が考えられる場合（例：「マイナンバー」 → 「個人番号」）</li>\n<li data-sourcepos=\"1088:1-1089:0\">クエリが短すぎて検索対象を十分にカバーできない場合（例：「控除の方法」 → 「2024年の所得税における控除の申請方法」）</li>\n</ul>\n<p data-sourcepos=\"1090:1-1090:256\">クエリ拡張についての詳細な情報は、<a href=\"https://speakerdeck.com/smiyawaki0820/retrieval-based-lm-rag-system-zatukurili-jie-suru\" rel=\"nofollow noopener\" target=\"_blank\">こちら</a>のサイトの資料で非常にわかりやすくまとめられており、参考になりました。</p>\n<hr data-sourcepos=\"1092:1-1092:3\">\n<h3 data-sourcepos=\"1093:1-1093:23\">\n<span id=\"実装例\" class=\"fragment\"></span><a href=\"#%E5%AE%9F%E8%A3%85%E4%BE%8B\"><i class=\"fa fa-link\"></i></a><strong>＜実装例＞</strong>\n</h3>\n<p data-sourcepos=\"1095:1-1095:201\">以下のコードでは、ユーザーが入力したクエリを、LLM（大規模言語モデル）を用いてリフレーズし、検索に適した形に変換する方法を示しています。</p>\n<p data-sourcepos=\"1097:1-1097:46\"><strong>コード例：クエリのリフレーズ</strong></p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"1098:1-1182:3\">\n<div class=\"code-lang\"><span class=\"bold\">query_expansion_1.py</span></div>\n<div class=\"highlight\"><pre><code><span class=\"kn\">from</span> <span class=\"n\">langchain_openai.embeddings</span> <span class=\"kn\">import</span> <span class=\"n\">AzureOpenAIEmbeddings</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain_chroma</span> <span class=\"kn\">import</span> <span class=\"n\">Chroma</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain.prompts</span> <span class=\"kn\">import</span> <span class=\"n\">PromptTemplate</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain_openai.chat_models</span> <span class=\"kn\">import</span> <span class=\"n\">AzureChatOpenAI</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain.schema</span> <span class=\"kn\">import</span> <span class=\"n\">HumanMessage</span>\n<span class=\"kn\">import</span> <span class=\"n\">os</span>\n<span class=\"kn\">from</span> <span class=\"n\">dotenv</span> <span class=\"kn\">import</span> <span class=\"n\">load_dotenv</span>\n\n<span class=\"nf\">load_dotenv</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Azure OpenAI の設定\n</span><span class=\"n\">AZURE_OPENAI_API_KEY</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">AZURE_OPENAI_API_KEY</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">AZURE_OPENAI_ENDPOINT</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">AZURE_OPENAI_ENDPOINT</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">API_VERSION</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">API_VERSION</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">DEPLOYMENT_ID_FOR_EMBEDDING</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">DEPLOYMENT_ID_FOR_EMBEDDING</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"n\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"p\">.</span><span class=\"nf\">getenv</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># LangChain の埋め込みクラスを初期化\n</span><span class=\"n\">embedding_model</span> <span class=\"o\">=</span> <span class=\"nc\">AzureOpenAIEmbeddings</span><span class=\"p\">(</span>\n    <span class=\"n\">deployment</span><span class=\"o\">=</span><span class=\"n\">DEPLOYMENT_ID_FOR_EMBEDDING</span><span class=\"p\">,</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">text-embedding-ada-002</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_key</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_API_KEY</span><span class=\"p\">,</span>\n    <span class=\"n\">azure_endpoint</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_ENDPOINT</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_version</span><span class=\"o\">=</span><span class=\"n\">API_VERSION</span><span class=\"p\">,</span>\n    <span class=\"n\">chunk_size</span><span class=\"o\">=</span><span class=\"mi\">300</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Azure ChatOpenAI を初期化\n</span><span class=\"n\">chat</span> <span class=\"o\">=</span> <span class=\"nc\">AzureChatOpenAI</span><span class=\"p\">(</span>\n    <span class=\"n\">deployment_name</span><span class=\"o\">=</span><span class=\"n\">DEPLOYMENT_ID_FOR_CHAT_COMPLETION</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_key</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_API_KEY</span><span class=\"p\">,</span>\n    <span class=\"n\">azure_endpoint</span><span class=\"o\">=</span><span class=\"n\">AZURE_OPENAI_ENDPOINT</span><span class=\"p\">,</span>\n    <span class=\"n\">openai_api_version</span><span class=\"o\">=</span><span class=\"n\">API_VERSION</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Chroma データベースの初期化\n</span><span class=\"n\">output_db_folder</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">./chroma_db</span><span class=\"sh\">\"</span>\n<span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"nc\">Chroma</span><span class=\"p\">(</span><span class=\"n\">persist_directory</span><span class=\"o\">=</span><span class=\"n\">output_db_folder</span><span class=\"p\">,</span> <span class=\"n\">embedding_function</span><span class=\"o\">=</span><span class=\"n\">embedding_model</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 質問の定義\n</span><span class=\"n\">original_query</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">令和6年分の年末調整は前年と異なる部分がありますか？</span><span class=\"sh\">\"</span>\n\n<span class=\"c1\"># 質問をリフレーズするプロンプト\n</span><span class=\"n\">rephrase_prompt</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"sh\">\"\"\"</span><span class=\"s\">\n以下の質問をリフレーズしてください。できるだけ詳細で検索に適した形にしてください。\n質問: </span><span class=\"si\">{</span><span class=\"n\">original_query</span><span class=\"si\">}</span><span class=\"s\">\n</span><span class=\"sh\">\"\"\"</span>\n<span class=\"c1\"># 質問をリフレーズ\n</span><span class=\"n\">rephrased_query</span> <span class=\"o\">=</span> <span class=\"n\">chat</span><span class=\"p\">.</span><span class=\"nf\">invoke</span><span class=\"p\">([</span><span class=\"nc\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"n\">rephrase_prompt</span><span class=\"p\">)]).</span><span class=\"n\">content</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># リフレーズされた質問を拡張クエリとして使用し、データベースから類似度の高いドキュメントを取得\n</span><span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"n\">db</span><span class=\"p\">.</span><span class=\"nf\">similarity_search</span><span class=\"p\">(</span><span class=\"n\">rephrased_query</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># ドキュメントの内容を結合\n</span><span class=\"n\">documents_string</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">([</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">---------------------------</span><span class=\"se\">\\n</span><span class=\"si\">{</span><span class=\"n\">doc</span><span class=\"p\">.</span><span class=\"n\">page_content</span><span class=\"si\">}</span><span class=\"sh\">\"</span> <span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">documents</span><span class=\"p\">])</span>\n\n<span class=\"c1\"># プロンプトテンプレートを初期化\n</span><span class=\"n\">combined_prompt</span> <span class=\"o\">=</span> <span class=\"nc\">PromptTemplate</span><span class=\"p\">(</span>\n    <span class=\"n\">template</span><span class=\"o\">=</span><span class=\"sh\">\"\"\"</span><span class=\"s\">以下の文章と質問を基にして、質問に対する答えを日本語で出力してください。\n\n1. 文章のみから判断してください。\n2. 文章から全く判断できない場合、「分かりません」と答えてください。\n3. ソースとなった文章を提示してください。\n\n文章:\n{document}\n\n質問: {query}</span><span class=\"sh\">\"\"\"</span><span class=\"p\">,</span>\n    <span class=\"n\">input_variables</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">document</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">query</span><span class=\"sh\">\"</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># チャットモデルに問い合わせ\n</span><span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">chat</span><span class=\"p\">.</span><span class=\"nf\">invoke</span><span class=\"p\">([</span>\n    <span class=\"nc\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"n\">combined_prompt</span><span class=\"p\">.</span><span class=\"nf\">format</span><span class=\"p\">(</span><span class=\"n\">document</span><span class=\"o\">=</span><span class=\"n\">documents_string</span><span class=\"p\">,</span> <span class=\"n\">query</span><span class=\"o\">=</span><span class=\"n\">original_query</span><span class=\"p\">))</span>\n<span class=\"p\">])</span>\n\n<span class=\"n\">answer</span> <span class=\"o\">=</span> <span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">content</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># 結果を出力\n</span><span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">元の質問: </span><span class=\"si\">{</span><span class=\"n\">original_query</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">リフレーズされた質問: </span><span class=\"si\">{</span><span class=\"n\">rephrased_query</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">回答: </span><span class=\"si\">{</span><span class=\"n\">answer</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n</code></pre></div>\n</div>\n<p data-sourcepos=\"1184:1-1184:40\"><strong>リフレーズによる出力結果</strong></p>\n<div class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"1185:1-1189:3\"><div class=\"highlight\"><pre><code>元の質問: 令和6年分の年末調整は前年と異なる部分がありますか？\nリフレーズされた質問: 令和6年分の年末調整において、前年とは異なる点があるかどうか、具体的な変更内容や新たな制度、手続きの違いについて詳しく教えてください。\n回答: はい、令和6年分の年末調整には前年と異なる部分があります。具体的には、定額減税の実施があり、年末調整時点での定額減税の額を算出する必要があります。\n</code></pre></div></div>\n<hr data-sourcepos=\"1191:1-1191:3\">\n<p data-sourcepos=\"1192:1-1194:207\"><strong>＜クエリ拡張のメリット＞</strong><br>\n<strong>1. 検索精度の向上</strong><br>\nLLMを活用してクエリ拡張することで、より具体的で詳細なクエリが生成され、検索エンジンやベクトルデータベースが適切な結果を返しやすくなります。</p>\n<p data-sourcepos=\"1196:1-1197:240\"><strong>2. ユーザーの意図を明確にできる</strong><br>\nユーザーが入力するクエリは、主観的で曖昧な表現を含むことが多く、検索対象に適さない場合があります。クエリ拡張を行うことで、意図を明確にしたクエリを生成できます。</p>\n<p data-sourcepos=\"1199:1-1200:351\"><strong>3. 検索対象の幅を広げる</strong><br>\nクエリを拡張することで、単一の表現だけでは得られない多面的な情報を取得しやすくなります。単純なキーワードに依存している場合、特定のソースからの情報に偏りがちですが、複数の言い回しや関連用語を加えることで、検索結果の網羅性が向上します。</p>\n<p data-sourcepos=\"1202:1-1202:19\"><strong>＜応用例＞</strong></p>\n<p data-sourcepos=\"1204:1-1204:303\">以下では、リフレーズ手法をさらに発展させる形で、さまざまなクエリ拡張の方法を紹介します。各手法を実行するための Python ファイルは、Git リポジトリに公開されていますので、興味のある方はぜひ試してみてください。</p>\n<p data-sourcepos=\"1206:2-1206:87\">1．<strong>複数のクエリを作成し検索する方法（<code>query_expansion_2.py</code>）</strong></p>\n<ul data-sourcepos=\"1207:4-1208:193\">\n<li data-sourcepos=\"1207:4-1207:288\">\n<strong>概要</strong>: 元のクエリを複数のバリエーションにリフレーズし、それぞれのクエリで検索を行います。その後、複数の検索結果を統合（たとえば RRF：Reciprocal Rank Fusion）することで、検索精度を高める手法です。</li>\n<li data-sourcepos=\"1208:4-1208:193\">\n<strong>メリット</strong>: 一つのクエリだけでは見落としてしまう情報を、複数のクエリで補い合う形で検索できるため、情報の網羅性が向上します。</li>\n</ul>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"1209:1-1212:5\">\n<div class=\"code-lang\"><span class=\"bold\">クエリ改善例</span></div>\n<div class=\"highlight\"><pre><code>元の質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n リフレーズされた質問: ['1. 令和6年の年末調整には、前年と比べて変更点がありますか？', '2. 令和6年の年末調整で、昨年とは異なる点がございますか？', '3. 令和6年分の年末調整には、前年とは異なる要素が含まれていますか？', '4. 令和6年度の年末調整は、前年度と違う部分がありますか？', '5. 令和6年の年末調整に関して、前年と異なる部分はありますか？']\n</code></pre></div>\n</div>\n<hr data-sourcepos=\"1213:1-1213:3\">\n<p data-sourcepos=\"1214:1-1214:105\">2． <strong>検索結果に基づいてクエリを動的に改善する手法（<code>query_expansion_3.py</code>）</strong></p>\n<ul data-sourcepos=\"1215:4-1216:277\">\n<li data-sourcepos=\"1215:4-1215:253\">\n<strong>概要</strong>: まずは元のクエリで検索を行い、結果が不十分と判断された場合に、LLM（大規模言語モデル）を用いてクエリそのものを再構築（リフレーズや補足）し、再検索を行います。</li>\n<li data-sourcepos=\"1216:4-1216:277\">\n<strong>メリット</strong>: 検索結果をフィードバックとして活用することで、段階的にクエリを最適化し、適切な情報にたどり着くまでクエリを改善できます。特に、不明点や不足が明確になった場合に有効です。</li>\n</ul>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"1217:1-1220:3\">\n<div class=\"code-lang\"><span class=\"bold\">クエリ改善例</span></div>\n<div class=\"highlight\"><pre><code>元の質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n改良された質問: 令和6年分の年末調整の方法と、前年との違いについて詳しく解説している情報を探しているのですが、特に定額減税に関する変更点や手続きの特徴、注意すべきポイントについて具体的な情報が知りたいです。\n</code></pre></div>\n</div>\n<p data-sourcepos=\"1222:1-1222:135\">3． <strong>元の質問を抽象化して検索を行い、その後具体的な回答を導出する手法（<code>query_expansion_4.py</code>）</strong></p>\n<ul data-sourcepos=\"1223:4-1224:374\">\n<li data-sourcepos=\"1223:4-1223:268\">\n<strong>概要</strong>: 質問が具体的すぎる場合、最初に質問を抽象化して検索することで、より汎用的な文書を取得します。その後、取得した情報をもとに、再度質問に合わせた具体的な回答を生成します。</li>\n<li data-sourcepos=\"1224:4-1224:374\">\n<strong>メリット</strong>: あまりにも細分化されたクエリでは十分な検索結果が得られない場合がありますが、抽象化することで必要な背景知識や関連する一般的な情報を得やすくなります。最終的には元の質問に合った具体的な回答へ落とし込むため、柔軟性と網羅性を両立できます。</li>\n</ul>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"1225:1-1230:3\">\n<div class=\"code-lang\"><span class=\"bold\">query_rephrase_4.py実行結果</span></div>\n<div class=\"highlight\"><pre><code>元の質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n抽象化された質問: 質問: 年末調整における変更点や新たな要素は何ですか？\n抽象的な回答: 年末調整における変更点や新たな要素として、令和6年分の所得税において**定額減税**が実施されることが挙げられます。これに伴い、年末調整の際には定額減税の額を算出し、それを基に年間の所得税額を計算する必要があります。この新しい要素により、年末調整に関連する事務が従来とは異なり 、特別控除の考慮が求められるようになります。詳細や手続きについては、国税庁のホームページでも案内されているため、参照することが推奨されます。   \n具体的な回答: 令和6年分の年末調整には、前年と異なる部分として定額減税が実施されます。これにより、年末調整の際に定額減税の額を算出し、それに基づいて年間の所得税額を計算する必要があります。この新しい要素により、年末調整に関連する事務が従来とは異なり、特別控除の考慮が求められるようになり ます。詳細や手続きについては、国税庁のホームページを参照することが推奨されます。\n</code></pre></div>\n</div>\n<p data-sourcepos=\"1231:1-1231:424\">これらの手法を組み合わせることで、<strong>クエリ拡張の効果を最大化し、より正確かつ豊富な検索結果を得ることが可能</strong>になります。特に、検索結果の活用や抽象化・具体化といったアプローチは、RAG（Retrieval-Augmented Generation）の精度向上にも大いに役立ちます。ぜひ、各スクリプトを実行して試してみてください。</p>\n<hr data-sourcepos=\"1233:1-1233:3\">\n<p data-sourcepos=\"1234:1-1235:333\">クエリ拡張は、RAGの検索プロセスにおいて精度を向上させる重要な手法の一つです。<br>\nLLMを用いたリフレーズを活用することで、より具体的で意味の明確なクエリを生成し、適切な検索結果を取得できるようになります。また、類義語の追加や補足情報の挿入など、他の手法と組み合わせることで、さらなる精度向上が期待できます。</p>\n<h1 data-sourcepos=\"1237:1-1237:19\">\n<span id=\"第6章-まとめ\" class=\"fragment\"></span><a href=\"#%E7%AC%AC6%E7%AB%A0-%E3%81%BE%E3%81%A8%E3%82%81\"><i class=\"fa fa-link\"></i></a>第6章 まとめ</h1>\n<p data-sourcepos=\"1238:1-1238:564\">本記事では、RAG（Retrieval-Augmented Generation）の基本的な仕組みから、LangChainを活用した実装方法、検索精度を向上させる工夫までを紹介しました。検索の精度を高めるには、チャンクサイズの調整やメタデータの活用、リランキング、クエリ拡張など、さまざまなアプローチがあります。特に、クエリのリフレーズや動的な改善、抽象化を取り入れることで、より的確な検索結果を得ることができることが分かりました。</p>\n<p data-sourcepos=\"1240:1-1241:91\">RAGは、大規模言語モデルをより効果的に活用し、最新の情報や専門知識を柔軟に組み込める点で非常に有用な技術です。本記事の内容が、RAGを活用したシステムの構築や検索精度向上の参考になれば幸いです。<br>\n今回利用した環境やコードは<a href=\"git%E3%81%AEURL\">こちら</a>にまとめております。</p>\n<h1 data-sourcepos=\"1243:1-1243:22\">\n<span id=\"第7章-参考資料\" class=\"fragment\"></span><a href=\"#%E7%AC%AC7%E7%AB%A0-%E5%8F%82%E8%80%83%E8%B3%87%E6%96%99\"><i class=\"fa fa-link\"></i></a>第7章 参考資料</h1>\n<p data-sourcepos=\"1244:1-1244:16\"><strong>＜書籍＞</strong></p>\n<ul data-sourcepos=\"1245:1-1248:0\">\n<li data-sourcepos=\"1245:1-1245:981\"><a href=\"https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297136333/ref=sr_1_2_sspa?adgrpid=156801342469&amp;dib=eyJ2IjoiMSJ9.iUa4odO5qgcS966iULz3HtcTc2uqQu0lVWd7hosPI6MA14Y-B61aYYPUS92JRkE7ne1PM9ASU07HNCHOLA7b1AsP5nf8Gh2K4u3cUCYWpGMiXa29adm6yymKO0P-1WR6VnXtocbGKS0d8INl1SPdfa5u1aOdbFvoXbp9E2Sut-FgGxy0C62OoSxk-c-gySknbxhccbn5FJI_YueYoZpgzhHgaRPkZz1fPt5my_oHa6Kl79IQQ_j4gJ8CdY1RLlSIBn-j8TDYLwQsYIgQzSgEe6fwgmFjZlM5VrBO_srMSL4.vLuK9aFlV34yjO3H2Gtu1sQMjDi3V6hnVZJjQABhgfY&amp;dib_tag=se&amp;gad_source=1&amp;hvadid=685864338618&amp;hvdev=c&amp;hvlocphy=9166121&amp;hvnetw=g&amp;hvqmt=e&amp;hvrand=18040040681330066797&amp;hvtargid=kwd-2270348275400&amp;hydadcr=1792_13657028&amp;jp-ad-ap=0&amp;keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1+%E8%A8%80%E8%AA%9E+%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&amp;mcid=753301ec7f8030b2bd433cf43109b8ff&amp;qid=1738131408&amp;sr=8-2-spons&amp;sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&amp;psc=1\" rel=\"nofollow noopener\" target=\"_blank\">大規模言語モデル入門</a></li>\n<li data-sourcepos=\"1246:1-1246:1119\"><a href=\"https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80%E2%85%A1%E3%80%9C%E7%94%9F%E6%88%90%E5%9E%8BLLM%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%A8%E8%A9%95%E4%BE%A1-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297143933/ref=sr_1_3_sspa?adgrpid=156801342469&amp;dib=eyJ2IjoiMSJ9.iUa4odO5qgcS966iULz3HtcTc2uqQu0lVWd7hosPI6MA14Y-B61aYYPUS92JRkE7ne1PM9ASU07HNCHOLA7b1AsP5nf8Gh2K4u3cUCYWpGMiXa29adm6yymKO0P-1WR6VnXtocbGKS0d8INl1SPdfa5u1aOdbFvoXbp9E2Sut-FgGxy0C62OoSxk-c-gySknbxhccbn5FJI_YueYoZpgzhHgaRPkZz1fPt5my_oHa6Kl79IQQ_j4gJ8CdY1RLlSIBn-j8TDYLwQsYIgQzSgEe6fwgmFjZlM5VrBO_srMSL4.vLuK9aFlV34yjO3H2Gtu1sQMjDi3V6hnVZJjQABhgfY&amp;dib_tag=se&amp;gad_source=1&amp;hvadid=685864338618&amp;hvdev=c&amp;hvlocphy=9166121&amp;hvnetw=g&amp;hvqmt=e&amp;hvrand=18040040681330066797&amp;hvtargid=kwd-2270348275400&amp;hydadcr=1792_13657028&amp;jp-ad-ap=0&amp;keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1+%E8%A8%80%E8%AA%9E+%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&amp;mcid=753301ec7f8030b2bd433cf43109b8ff&amp;qid=1738131408&amp;sr=8-3-spons&amp;sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&amp;psc=1\" rel=\"nofollow noopener\" target=\"_blank\">大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価</a></li>\n<li data-sourcepos=\"1247:1-1248:0\"><a href=\"https://www.amazon.co.jp/LangChain%E5%AE%8C%E5%85%A8%E5%85%A5%E9%96%80-%E7%94%9F%E6%88%90AI%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E9%96%8B%E7%99%BA%E3%81%8C%E3%81%AF%E3%81%8B%E3%81%A9%E3%82%8B%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%93%8D%E3%82%8A%E6%96%B9-%E7%94%B0%E6%9D%91-%E6%82%A0-ebook/dp/B0CL49K74N/ref=sr_1_3?adgrpid=155650325217&amp;dib=eyJ2IjoiMSJ9.cb2DSolyrUa1Vyx5yimjomJ1vXv1THhF84w7hqgn1EWm79jYA-dlRkZFRriUBiDcCN0wWd_8FlJQUoA6PABkQc8jDeAvpRCy2hEvYb2D6jBtphdV55ES3cLn7HRE7rsHByWpT0fGiXMms5XqNDkAr-dYdd51-7TIYV5vpNiYMbvDPzFYWjjsuDWOWashvkcVtj8hymSMKIBIuwNySVfURa7akWg3i8r4CMMLMMrF0Rk.4rI-motw7xKRBs6batXzXS7BUIJgQW0EZ3LCP-EeBmo&amp;dib_tag=se&amp;gad_source=1&amp;hvadid=687716088906&amp;hvdev=c&amp;hvlocphy=9166121&amp;hvnetw=g&amp;hvqmt=e&amp;hvrand=3850225481485846046&amp;hvtargid=kwd-2278701652466&amp;hydadcr=27486_14701136&amp;jp-ad-ap=0&amp;keywords=lang+chain+%E6%9C%AC&amp;mcid=df081cd160633ae2851a77addccc61f5&amp;qid=1738130620&amp;sr=8-3\" rel=\"nofollow noopener\" target=\"_blank\">LangChain完全入門　生成AIアプリケーション開発がはかどる大規模言語モデルの操り方</a></li>\n</ul>\n<p data-sourcepos=\"1249:1-1249:22\"><strong>＜Webサイト＞</strong></p>\n<ul data-sourcepos=\"1250:1-1255:0\">\n<li data-sourcepos=\"1250:1-1250:204\"><a href=\"https://www.dir.co.jp/world/entry/solution/rag\" rel=\"nofollow noopener\" target=\"_blank\">生成AIのビジネス活用で注目されるRAG（検索拡張生成）とは？ - 仕組みや活用例、精度向上のノウハウなどを紹介</a></li>\n<li data-sourcepos=\"1251:1-1255:0\"><a href=\"https://speakerdeck.com/smiyawaki0820/retrieval-based-lm-rag-system-zatukurili-jie-suru\" rel=\"nofollow noopener\" target=\"_blank\">Retrieval-based LM (RAG system) ざっくり理解する</a></li>\n</ul>\n",
      "body": "# 第1章 はじめに\n\n## 1.1 本記事の概要と目的\n本記事では、大規模言語モデル（LLM）をより効果的に活用する手法として注目されている「RAG（Retrieval-Augmented Generation）」の概要と、Python向けフレームワークであるLangChainを使った実装方法について解説します。特に、PDFデータを外部情報源として扱う具体的な方法を取り上げ、「データ検索と回答生成の流れ」 を順を追って説明します。\n\n本記事の目的は、次の3点です。\n- RAGの基本概念・メリットを理解する\n- LangChainを使ったPDFデータの登録・検索・回答生成を実装する\n- 実装の注意点や精度向上のコツをつかむ\n\nこの記事を参考にしていただくことで、PDFドキュメントを活用したRAG構築のアイデアを形にするためのヒントを得られることを目指しています。\n\nなお、使用しているコード及びデータは[こちらのGitリポジトリ](https://github.com/syu-tam/rag)に公開しています。必要に応じてダウンロードやクローンしてご利用ください。\n\n\n## 1.2 想定読者\n本記事は、次のような方を対象にしています。\n\n- 大規模言語モデル（LLM）の活用に関心がある方\n- LangChainを使ったシステム開発に興味がある方\n- PDFデータを活用して情報検索や回答生成を試してみたい方\n\nPythonでの開発経験がある程度あり、機械学習や自然言語処理に触れたことがある方であれば、スムーズに進められる内容となっています。\n\n## 1.3 前提知識\n\n- Pythonの基礎\n    - Pythonスクリプトの実行、仮想環境の作成（venvやcondaなど）、pipによるライブラリのインストールを理解している。\n    - 変数、関数、リスト、辞書などの基本的な文法を理解している\n\n- LLM（大規模言語モデル)の概要\n    - GPTやBERTなどのモデルに関して、テキスト生成の仕組みをおおまかに把握している。\n\n- Embeddings(埋め込み)の概念\n    - 単語や文をベクトル空間に埋め込む手法（Word2VecやBERTのような考え方）に馴染みがある。\n\nもし不明点があれば、随時公式ドキュメントや他の入門記事を参照いただくとスムーズです。\n\nまた、以下の2冊の書籍はLLMについて網羅的にまとめられており、非常に参考になりました。\n- [大規模言語モデル入門](https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297136333/ref=sr_1_2_sspa?adgrpid=156801342469&dib=eyJ2IjoiMSJ9.iUa4odO5qgcS966iULz3HtcTc2uqQu0lVWd7hosPI6MA14Y-B61aYYPUS92JRkE7ne1PM9ASU07HNCHOLA7b1AsP5nf8Gh2K4u3cUCYWpGMiXa29adm6yymKO0P-1WR6VnXtocbGKS0d8INl1SPdfa5u1aOdbFvoXbp9E2Sut-FgGxy0C62OoSxk-c-gySknbxhccbn5FJI_YueYoZpgzhHgaRPkZz1fPt5my_oHa6Kl79IQQ_j4gJ8CdY1RLlSIBn-j8TDYLwQsYIgQzSgEe6fwgmFjZlM5VrBO_srMSL4.vLuK9aFlV34yjO3H2Gtu1sQMjDi3V6hnVZJjQABhgfY&dib_tag=se&gad_source=1&hvadid=685864338618&hvdev=c&hvlocphy=9166121&hvnetw=g&hvqmt=e&hvrand=18040040681330066797&hvtargid=kwd-2270348275400&hydadcr=1792_13657028&jp-ad-ap=0&keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1+%E8%A8%80%E8%AA%9E+%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&mcid=753301ec7f8030b2bd433cf43109b8ff&qid=1738131408&sr=8-2-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1)\n- [大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価](https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80%E2%85%A1%E3%80%9C%E7%94%9F%E6%88%90%E5%9E%8BLLM%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%A8%E8%A9%95%E4%BE%A1-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297143933/ref=sr_1_3_sspa?adgrpid=156801342469&dib=eyJ2IjoiMSJ9.iUa4odO5qgcS966iULz3HtcTc2uqQu0lVWd7hosPI6MA14Y-B61aYYPUS92JRkE7ne1PM9ASU07HNCHOLA7b1AsP5nf8Gh2K4u3cUCYWpGMiXa29adm6yymKO0P-1WR6VnXtocbGKS0d8INl1SPdfa5u1aOdbFvoXbp9E2Sut-FgGxy0C62OoSxk-c-gySknbxhccbn5FJI_YueYoZpgzhHgaRPkZz1fPt5my_oHa6Kl79IQQ_j4gJ8CdY1RLlSIBn-j8TDYLwQsYIgQzSgEe6fwgmFjZlM5VrBO_srMSL4.vLuK9aFlV34yjO3H2Gtu1sQMjDi3V6hnVZJjQABhgfY&dib_tag=se&gad_source=1&hvadid=685864338618&hvdev=c&hvlocphy=9166121&hvnetw=g&hvqmt=e&hvrand=18040040681330066797&hvtargid=kwd-2270348275400&hydadcr=1792_13657028&jp-ad-ap=0&keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1+%E8%A8%80%E8%AA%9E+%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&mcid=753301ec7f8030b2bd433cf43109b8ff&qid=1738131408&sr=8-3-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1)\n\n# 第2章 RAG(Retrieval-Augmented Generation)とは\n## 2.1 RAGの概要\nRAG (Retrieval-Augmented Generation) は、検索拡張生成と訳される技術で、近年注目を集めている生成AIの応用手法の一つです。\n生成AI、特に大規模言語モデル (LLM) は、膨大な知識を学習していますが、その知識は学習データに含まれる情報に基づいています。そのため、\n- 最新情報に対応できない\n- 専門的な知識や特定のドメイン知識が不足している\n- 事実に基づかない内容 (ハルシネーション) を生成してしまう\n\nといった課題がありました。\n\nRAGは、これらの課題を克服するために、LLMが外部の知識源を参照しながら回答を生成する仕組みを取り入れています。具体的には、質問内容に関連する情報を検索し、その情報をLLMに与えることで、より正確で信頼性の高い回答生成を可能にします。\nRAGを活用することで、\n\n- 常に最新の情報に基づいた回答\n- 専門知識や社内データなど、特定の知識に基づいた回答\n- 根拠のある、ハルシネーションを抑制した回答\n\nを生成することが期待できます。\n\n## 2.2 RAGの仕組み\nRAGの仕組みは、大きく分けて 「検索 (Retrieval)」（下図①から③） と 「生成 (Generation)」（下図④から⑥） の2つのステップから構成されます。\n\n参考：[生成AIのビジネス活用で注目されるRAG（検索拡張生成）とは？ - 仕組みや活用例、精度向上のノウハウなどを紹介](https://www.dir.co.jp/world/entry/solution/rag)\n\n![20240723092206.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3985295/a0ad0a94-cb31-021a-c8a3-c7aed48f1676.png)\n\n**＜検索 (Retrieval) ステップ＞**\n\nRAGシステムは、まず ユーザーからの質問（上図①) を受け付けます。すると、システムは質問内容に関連する情報を 外部の知識源 から探し出す検索を行います。\n\n本記事では、知識源として PDFデータ を活用します。PDFデータは、あらかじめ ベクトルデータベース という、意味に基づいた検索 が得意な特殊なデータベースに登録しておきます。\n\n検索ステップでは、以下の3つの処理を行います。\n\n**1. 質問文をベクトル化(上図①)：** 質問文を 埋め込みモデル (例：OpenAI Embeddings、HuggingFace Transformers) というAIモデルで、意味 を捉えた数値データ (ベクトル) に変換します。\n**2. ベクトル検索（上図②)：** ベクトル化された質問文と、ベクトルデータベース(例：FAISS、Chroma、Pinecone)に登録されたPDFドキュメントのベクトルを比較し、意味的に近い ドキュメントを検索します。\n**3. 関連情報を取得（上図③)：** 検索されたドキュメントの中から、質問に関連性の高い箇所を抽出します。\n\n---\n**＜生成 (Generation) ステップ＞**\n\n検索ステップで取得した関連情報と、質問文を組み合わせて、LLMに入力します。\n\nLLMは、与えられた情報 (関連情報と質問文) をもとに、質問に対する回答文を生成します。この際、LLMは検索された関連情報を参照しながら回答を生成するため、外部知識に基づいた、より正確で根拠のある回答を生成することが可能になります。\n\n生成ステップでは、以下の2つの処理を行います。\n\n**1. プロンプト作成(上図④)：** 質問文と検索された関連情報を組み合わせ、LLMへの指示文 (プロンプト) を作成します。\n**2. 回答生成と出力(上図⑤＆⑥)：** 作成されたプロンプトをLLMに入力し、回答文を生成させた後。ユーザーに解答を表示します。\n\nこのように、RAGは 「検索」 と 「生成」 のステップを組み合わせることで、LLMが持つ生成能力と、外部知識源の情報を効果的に統合し、より高度な質問応答システムを構築するための技術です。\n\n# 第3章 LangChainとは　\n## 3.1 LangChainの概要\nLangChainは、大規模言語モデル（LLM）の活用をより効果的にするためのPythonライブラリおよびフレームワークです。このフレームワークは、テキスト生成、データ検索、エージェントの管理といったタスクを柔軟に組み合わせて、さまざまなアプリケーションを構築できるよう設計されています。\n\nLangchainについての詳細な実装は、[LangChain完全入門　生成AIアプリケーション開発がはかどる大規模言語モデルの操り方](https://www.amazon.co.jp/LangChain%E5%AE%8C%E5%85%A8%E5%85%A5%E9%96%80-%E7%94%9F%E6%88%90AI%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E9%96%8B%E7%99%BA%E3%81%8C%E3%81%AF%E3%81%8B%E3%81%A9%E3%82%8B%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%93%8D%E3%82%8A%E6%96%B9-%E7%94%B0%E6%9D%91-%E6%82%A0-ebook/dp/B0CL49K74N/ref=sr_1_3?adgrpid=155650325217&dib=eyJ2IjoiMSJ9.cb2DSolyrUa1Vyx5yimjomJ1vXv1THhF84w7hqgn1EWm79jYA-dlRkZFRriUBiDcCN0wWd_8FlJQUoA6PABkQc8jDeAvpRCy2hEvYb2D6jBtphdV55ES3cLn7HRE7rsHByWpT0fGiXMms5XqNDkAr-dYdd51-7TIYV5vpNiYMbvDPzFYWjjsuDWOWashvkcVtj8hymSMKIBIuwNySVfURa7akWg3i8r4CMMLMMrF0Rk.4rI-motw7xKRBs6batXzXS7BUIJgQW0EZ3LCP-EeBmo&dib_tag=se&gad_source=1&hvadid=687716088906&hvdev=c&hvlocphy=9166121&hvnetw=g&hvqmt=e&hvrand=3850225481485846046&hvtargid=kwd-2278701652466&hydadcr=27486_14701136&jp-ad-ap=0&keywords=lang+chain+%E6%9C%AC&mcid=df081cd160633ae2851a77addccc61f5&qid=1738130620&sr=8-3)の本で非常にわかりやすくまとめられており、参考になりました。\n\n## 3.2 LangChainの特徴\nLangChainは、次のような特徴を持っています。\n\n**1. LLMを基盤にしたタスク管理**\n- テキスト生成や質問応答だけでなく、検索やデータ操作を含む高度なワークフローを構築できます。\n\n\n**2. モジュール性**\n- 各コンポーネント（例：データ読み込み、埋め込み生成、検索処理）を独立して管理できるため、目的に応じた柔軟な構成が可能です。\n\n**3. 豊富なインテグレーション**\n- OpenAIやHugging Faceのモデルだけでなく、FAISSやChromaなどのベクトルデータベース、PDFリーダーなど、さまざまな外部ツールと連携可能です。\n\n**4. エージェントのサポート**\n- 「エージェント」と呼ばれる機能を活用することで、ユーザーの指示に基づいて動的にタスクを切り替えたり、複数の機能を組み合わせて実行できます。\n\n## 3.3 ユースケース\nLangChainは以下のようなシナリオで活用されています。\n\n- **質問応答システム**\n外部データを元に、LLMを活用した精度の高いQAシステムを構築。\n\n- **ドキュメント要約**\n長い文書をチャンク化して要約を生成するワークフローを実装。\n\n- **カスタムチャットボット**\n特定のドメインデータを利用した対話型アプリケーションの開発。\n\n- **データ統合ツール**\n様々なデータソースを取り込み、ベクトル検索とLLMを組み合わせた分析。\n\n\n# 第4章 構築手順 - LangChain × PDFデータ RAG実装ステップ\n## 4.1 環境構築\nLangChainを使ったRAG実装を始めるために、Python環境を整えます。本記事では、Python 3.11.9を使用して動作確認を行っています。\n\n---\n**＜仮想環境の作成＞**\n```bash\n# 仮想環境の作成\n$ python -m venv langchain_env\n\n# 仮想環境の有効化（Windows）\n$ langchain_env/Scripts/activate\n```\n---\n**＜ライブラリのインストール＞**\n\n```bash\n$ pip install PyMuPDF langchain langchain-openai langchain-chroma python-dotenv\n```\n以下の表は各ライブラリの簡単な説明です。\n\n| ライブラリ名              | 説明                                                                                     |\n|---------------------------|------------------------------------------------------------------------------------------|\n| PyMuPDF               | PDFやその他のドキュメント形式を操作するためのライブラリ。`fitz`モジュールを通じて利用可能。 |\n| langchain             | 大規模言語モデル（LLM）を活用したアプリケーションを構築するためのフレームワーク。          |\n| langchain-openai      | LangChainでOpenAIのAPIを利用するための拡張ライブラリ。                                     |\n| langchain-chroma      | LangChainとChroma（ベクトルデータベース）を連携させるためのライブラリ。                   |\n| python-dotenv         | `.env`ファイルから環境変数を読み込むためのライブラリ。                                      |\n\n---\n**＜APIキーの設定＞**\nプロジェクトのルートディレクトリに `.env` ファイルを作成し、使用するLLMのAPIキーやエンドポイントを設定しておきます。本記事では、Azure OpenAIのGPT-4o miniを使用する例を示します。他のサービスを利用する場合は、適宜設定内容を変更してください。\n```python:.env\nOPENAI_API_KEY=your_openai_api_key_here\nAPI_ENDPOINT=https://your-resource-name.openai.azure.com/\nAPI_VERSION=2023-05-15  # Azure OpenAIの場合の例\nDEPLOYMENT_ID_FOR_CHAT_COMPLETION=your_chat_completion_deployment_id\nDEPLOYMENT_ID_FOR_EMBEDDING=your_embedding_deployment_id\n```\n**`.env` ファイルの設定項目(Azure OpenAIの場合）**\n\n| 項目名                           | 説明                                                                                          |\n|-----------------------------------|---------------------------------------------------------------------------------------------|\n| OPENAI_API_KEY                | OpenAIまたはAzure OpenAIで発行されたAPIキー。                                                 |\n| API_ENDPOINT                  | Azure OpenAIサービスのエンドポイントURL。                                                    |\n| API_VERSION                  | 使用するAPIのバージョン。Azureでは一般的に `2023-05-15`など を指定します。                         |\n|DEPLOYMENT_ID_FOR_CHAT_COMPLETION | Chat Completion用にデプロイされたモデルのID。                                                  |\n| DEPLOYMENT_ID_FOR_EMBEDDING   | Embedding用にデプロイされたモデルのID。                                                       |\n\n **⚠️ 注意事項: `.env` ファイルの管理**\n>`.env` ファイルにはAPIキーやエンドポイントURLなどの**機密情報**が含まれます。そのため、**必ず `.gitignore` に追加し、Gitでバージョン管理しないようにしてください。**\n\n## 4.2 外部データの登録\n### 1. PDFデータの準備\nまず、RAGシステムの知識源とするPDFファイルを用意します。本記事では、動作確認用のPDFファイルとして、国税庁が公開した「令和6年分 年末調整のしかた」を活用しました。[こちら](https://www.nta.go.jp/publication/pamph/gensen/nencho2024/01.htm)からダウンロードできます。\n\n活用したいPDFファイルを選んだら、テキストやグラフ、表の情報の抽出を行います。RAG構築において、この情報をいかに正確に抽出できるかが非常に重要となるため、詳しく解説していきます。本記事では、PDFデータの抽出方法を6つ紹介し、それぞれのメリットとデメリット、そして用途に応じた選択のポイントをまとめます。\n\n---\n**＜Ⅰ. Pythonライブラリの活用＞**\n\n `PyMuPDF（fitz）`や`pdfminer`、`PyPDF2`などの Pythonライブラリを利用して、プログラムを通じてテキストや構造情報を抽出する方法です。\n **メリット**\n- 大量のPDFを自動で一括処理できる。\n- Pythonスクリプトで独自の前処理・後処理を組み込み可能。\n- オープンソースのライブラリなら無料で利用可能。\n\n**デメリット**\n- スクリプトを書くための知識が求められる。\n- PDFのレイアウトや文字コードの扱いによって抽出精度に差がでる。\n- グラフや表の読み取り精度が低い。\n\n**こんなときにおすすめ**\n- RAG用に大量のPDFデータを定期的に収集・更新する必要があるとき。\n- 自由度の高い前処理や後処理（特定のセクションだけ抽出、マーカー付きテキストだけ取り出すなど）が必要なとき。\n\n---\n**＜Ⅱ. 生成AIサービスの活用＞**\n`ChatGPT`やGoogle DeepMindの`Gemini`などの生成AIを利用して、PDFの内容を要約・解析し、必要な情報を取り出す方法です。（PDF自体を直接アップロードできるサービスもあれば、テキストを貼り付けて要約させるアプローチもあります。）\n\n**メリット**\n- 単なるテキスト抽出にとどまらず、要約や意図理解などの高度な自然言語解析も得意。\n- プログラミング不要で、Webインターフェースからすぐ使える場合が多い。\n- 多言語対応: さまざまな言語のテキストを高精度で解析できる。\n\n**デメリット**\n-  一定以上のボリュームを処理する場合、API利用料がかさむことがある。\n-  プライバシー/セキュリティの懸念: 機密データをクラウドへ送信する際は要注意。\n-  グラフや表などの正確な位置関係までは再現しにくい。\n\n**こんなときにおすすめ**\n- 文章の意味を理解した要約や「このテキストで質問に回答できるようにしたい」とき。\n- 自社データを特定のトピックに沿って要約・分類したい場合。\n\n---\n**＜Ⅲ. OCRの活用＞**\nスキャンした画像ベースのPDFや、文字情報が埋め込まれていないPDFからテキストを抽出する場合に OCR (Optical Character Recognition) を利用します。`Tesseract OCR`などが代表的です。\n\n**メリット**\n- 画像化されたPDFにも対応: スキャンデータや画像形式のドキュメントを扱える。\n- 多言語対応: 設定次第で複数言語を認識可能。\n\n**デメリット**\n- 精度が画像品質に左右される: 解像度や文字の読みやすさで結果が大きく変わる。\n- 処理時間: 大量のファイルを扱う場合は長時間のバッチ処理が必要になることも。\n\n**こんなときにおすすめ**\n- スキャンされた古い文書や紙資料をデジタル化したPDFから情報を抽出したいとき。\n- 文字列として取り出せない図表やスペックシートなどの画像内文字を読み取りたいとき。\n\n---\n **＜Ⅳ.  手作業（マニュアル抽出）＞**\n人間がPDFを開き、必要な箇所をコピー＆ペーストや書き写しで抽出するアナログな方法。\n\n**メリット**\n- 人が直接確認しながら作業するため、誤抽出が少ない。\n- 表や特殊な図版など、アルゴリズムでは難しい内容も対応可能。\n\n**デメリット**\n- 大量のファイルを扱うときは時間と人件費がかかる。\n- 熟練度や集中力に左右される。\n\n**こんなときにおすすめ**\n- データ量がごく少量で、かつ正確性が最優先な場合。\n- 極端に複雑なレイアウトや、特殊文字・手書き要素が混在する場合。\n\n---\n**＜Ⅴ. 専用PDF解析ツールの利用＞**\n`Adobe Acrobat`や`PDF Expert`など、有料・無料問わず商用/専用ソフトを活用してPDF情報を解析する方法です。\n\n**メリット**\n- 市販ツールは独自のアルゴリズムを搭載していることが多く、抽出精度が高い。\n- コードを書かずにGUIで簡単にファイル操作を行える。\n-  テキスト編集、注釈、比較、セキュリティ設定など、周辺機能が充実。\n\n**デメリット**\n- 商用ソフトの場合はライセンス料が必要。\n- 特殊な処理を行いたい場合に独自のカスタマイズが難しいことも。\n\n**こんなときにおすすめ**\n- プログラミングは苦手だけどGUIでサクッと抽出したい。\n- 社内規定でAdobe製品のみ利用可能など、特定ツールが標準化されている環境。\n\n---\n**＜Ⅵ. クラウドサービスの活用＞**\n`AWS Textract`、`Google Cloud Vision`、`Azure Form Recognizer`などのクラウドAIサービスを使い、PDFからテキストや構造を抽出する方法です。\n\n**メリット**\n- 大量のファイルを同時処理可能。\n- 単なるテキスト抽出だけでなく、表やレイアウト構造を自動的に識別。\n- サーバーの保守やアップデートはクラウドベンダー側が実施するため、管理が容易。\n\n**デメリット**\n- APIコール数やファイルサイズに応じて費用がかかる。\n- 機密情報の取り扱いに注意が必要。\n\n**こんなときにおすすめ**\n- 大規模・高負荷のプロジェクトで、オンプレミスでの処理が困難な場合。\n- 表やフォームなど構造化データの抽出をクラウドで一括して行いたい場合。\n\n---\n以上6つの抽出手法をテキスト抽出精度、表や図の読み取り精度、コスト、処理速度の観点で（個人的に）評価した表を以下に示します。\n\n| 方法                     | テキスト抽出精度 | 表や図の読み取り精度 | コスト | 処理速度 |\n|------------------------|:------------------:|:--------------------:|:------:|:--------:|\n| **1. Pythonライブラリ**   | ○               | △                   | ○      | ○        |\n| **2. 生成AIサービス**      | ◎               | △ ～  ○              | △      | △        |\n| **3. OCRの活用**          | △ ～  ○               | ○                   | ○      | △        |\n| **4. 手作業**             | ◎               | ◎                   | ×      | ×        |\n| **5. 専用PDF解析ツール**   | ○               | ○                   | △      | ○        |\n| **6. クラウドサービス**    | ◎               | ○                   | △      | △        |\n\n\nこれらの特徴を踏まえて、使用するPDFファイルの特徴に応じて適切な方法を選択、もしくは組み合わせて使用することが重要です。以下にいくつかの例を示します\n\n- 図や表が非常に多いPDFファイル \n  → PDF解析ツールやクラウドサービスを利用して抽出し、必要に応じて手作業で修正する。\n\n- 図や表がほぼない文章中心のPDFファイル  \n  → Pythonライブラリを活用して効率よく抽出処理を行う。\n\n- スキャンされた画像ベースのPDF \n  → OCRを用いてテキストを抽出した後、生成AIサービスで要約や情報解析を行う。\n---\n### ＜実装＞\nPDFデータを用いて情報抽出を行います。今回は、国税庁が公開した資料「年末調整のしかた」を使用します。この資料は表やグラフが少ないため、以下の手順で処理を進めました\n1. **Pythonライブラリを活用した抽出**\n表やグラフの数が少ないため、Pythonライブラリを使用してテキストを抽出しました。\n\n2. **Azure OpenAIを使用したテキスト処理**\n抽出したデータを`Azure OpenAI`で処理し、文章の体裁の整理、簡易的な表を生成を行いました。\n\n3. **手作業での修正**\n数値やテキストの誤り、体裁の細かい崩れを確認し、必要な部分を手作業で修正しました。\n\nPDFのデータ抽出を行うpythonコードを以下に示します。\n```python:pdf_text_extraction.py\nimport fitz  # PyMuPDF\nfrom langchain_openai import AzureChatOpenAI\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom dotenv import load_dotenv\nimport os\nfrom typing import Optional\nfrom tqdm import tqdm\nimport time\n\n# 環境変数の読み込み\nload_dotenv()\n\n# Azure OpenAI の設定\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\nAPI_VERSION = os.getenv(\"API_VERSION\")\nDEPLOYMENT_ID_FOR_CHAT_COMPLETION = os.getenv(\"DEPLOYMENT_ID_FOR_CHAT_COMPLETION\")\n\n# LLMのインスタンス作成\nllm = AzureChatOpenAI(\n    deployment_name=DEPLOYMENT_ID_FOR_CHAT_COMPLETION,\n    openai_api_key=AZURE_OPENAI_API_KEY,\n    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    openai_api_version=API_VERSION,\n)\n\ndef extract_and_restructure_pdf(\n    pdf_path: str,\n    output_md_path: str,\n    output_raw_text_path: str,\n    first_page: Optional[int] = 1,\n    last_page: Optional[int] = None,\n    max_retries: int = 3,\n    retry_delay: int = 2\n) -> None:\n    \n    \n    \"\"\"PDFの内容を抽出して整形し、Markdownと生テキストファイルに保存します。\n\n    Args:\n            pdf_path (str): 処理するPDFファイルのパス。\n            output_md_path (str): 整形結果を保存するMarkdownファイルのパス。\n            output_raw_text_path (str): 抽出された生テキストを保存するファイルのパス。\n            first_page (Optional[int]): 処理を開始するページ番号（1始まり）。デフォルトは1。\n            last_page (Optional[int]): 処理を終了するページ番号。デフォルトは最終ページ。\n            max_retries (int): LLM呼び出しの最大リトライ回数。デフォルトは3。\n            retry_delay (int): LLM呼び出し失敗時のリトライ間隔（秒）。デフォルトは2秒。\n    \"\"\"    \n        \n    try:\n        # PDFを開く\n        doc = fitz.open(pdf_path)\n        total_pages = len(doc)\n\n        # ページ範囲の計算\n        first_page = first_page or 1\n        last_page = last_page or total_pages\n\n        raw_texts = []  # 生テキストを保存するリスト\n        markdown_results = []  # 整形後のテキストを保存するリスト\n\n        # PDFページを順に処理\n        for page_num in tqdm(range(first_page - 1, last_page), desc=\"Processing PDF Pages\"):\n            page = doc[page_num]\n            extracted_text = page.get_text(\"text\")  # テキスト抽出\n\n            # LLMへの指示を作成\n            messages = [\n                SystemMessage(content=(\n                    \"あなたは優れたアシスタントです。以下に与えられるテキストはPDFから抽出された内容であり、体裁が崩れている可能性があります。\\n\\n\"\n                    \"以下の指示に従って、テキストの整形を行ってください:\\n\\n\"\n                    \"1. 句読点や改行の位置を適切に整え、誤字脱字を修正してください（文脈に基づく範囲内で）。\\n\"\n                    \"2. 元のテキストに含まれる情報を削除しないでください。\\n\"\n                    \"3. 表形式のデータは可能な限り元のレイアウトを維持してください。\\n\"\n                    \"4. グラフの軸の数値関係を確認し、適切に説明してください。\\n\\n\"\n                    \"最終結果はMarkdown形式で出力してください。\"\n                )),\n                HumanMessage(content=f\"## ページ {page_num + 1}\\n\\n### 抽出されたテキスト:\\n\\n{extracted_text}\")\n            ]\n\n            # LLMを使用して整形\n            for attempt in range(max_retries):\n                try:\n                    response = llm.invoke(messages)\n                    markdown_results.append(response.content)\n                    break\n                except Exception as e:\n                    print(f\"Error during OpenAI API call for page {page_num + 1} (attempt {attempt + 1}): {e}\")\n                    if attempt == max_retries - 1:\n                        print(f\"Failed after max retries for page {page_num + 1}.\")\n                    time.sleep(retry_delay)\n\n            # 抽出されたテキストを保存\n            raw_texts.append(f\"## ページ {page_num + 1}\\n{extracted_text}\")\n\n        # 生テキストをファイルに保存\n        with open(output_raw_text_path, \"w\", encoding=\"utf-8\") as raw_file:\n            raw_file.write(\"\\n\\n\".join(raw_texts))\n        print(f\"抽出されたテキストが保存されました: {output_raw_text_path}\")\n\n        # 整形結果をMarkdownで保存\n        with open(output_md_path, \"w\", encoding=\"utf-8\") as md_file:\n            md_file.write(\"\\n\\n\".join(markdown_results))\n        print(f\"整形結果がMarkdownファイルに保存されました: {output_md_path}\")\n\n    except Exception as e:\n        print(f\"Error processing PDF: {e}\")\n\nif __name__ == '__main__':\n    # 使用例\n    pdf_path = \"nencho_all.pdf\"  # 処理するPDFファイルのパス\n    output_md_path = \"output/nencho_al_test.md\"  # 整形結果の保存先\n    output_raw_text_path = \"output/nencho_all_test.txt\"  # 抽出されたテキストの保存先\n\n    # 出力ディレクトリの作成\n    os.makedirs('output', exist_ok=True)\n\n    # PDF処理の実行\n    extract_and_restructure_pdf(\n        pdf_path=pdf_path,\n        output_md_path=output_md_path,\n        output_raw_text_path=output_raw_text_path,\n        first_page= None,  # 最初のページから処理\n        last_page= None    # 最後のページまで処理\n    )\n```\n\n本記事では`PyMuPDF`と`GPT4o-mini`を使用していますが、PDFの特徴に応じて他のライブラリや生成AIを活用し、それらを比較してみるのも有効なアプローチです。また、PDFの内容や構造に応じた適切なプロンプト設計を行うことで、より正確で有用な出力を得ることが可能です。\n出力結果は以下のようになりました。\n\n```md:出力結果\n## ページ 1\n\n### 令和6年分 年末調整のしかた\n\n法人番号: 700000120500002\n\n「年末調整がよくわかるページ」をご覧ください！\n\n国税庁ホームページには、「年末調整がよくわかるページ」を掲載しています。このページには、本年の定額減税を含めた年末調整の手順等を解説した動画やパンフレット、扶養控除等申告書など各種申告書、従業員向けの説明用リーフレットや各種申告書の記載例など、年末調整の際に役立つ情報を掲載していますので、ご活用ください。\n\nなお、動画による説明はYouTubeにも掲載していますので、ご活用ください。\n\n※ 令和6年分の各種情報については、令和6年10月頃に掲載いたします。\n\n年末調整に係る源泉徴収をした所得税及び復興特別所得税の納期限は、令和7年1月10日（金）（納期の特例の承認を受けている場合は、令和7年1月20日（月））です。\n\nその他、給料や報酬などについて源泉徴収をした所得税及び復興特別所得税の納期限については、2ページを確認してください。\n\n（よくわかるページ）\n\n（YouTube）\n\n年末調整に関する相談は、国税庁ホームページからチャットボットの「税務職員ふたば」をお気軽にご利用ください。年末調整の各種申告書の書き方や添付書類に関することなどについて、AIが自動で回答します。\n\n※ 公開期間は令和6年10月頃から令和7年1月下旬までの予定です。年末調整でお困りのときは、“ふたば” にご相談ください。\n\n（チャットボット）\n\n### 年末調整手続の電子化で業務の効率化！\n\n年末調整手続の電子化を行うと、給与の支払者（勤務先）及び給与所得者（従業員）それぞれにおいて、書類の作成や確認、保管などの業務全般が大幅に効率化されるなど、双方に大きなメリットがあります。\n\nまた、国税庁では「年末調整控除申告書作成用ソフトウェア」（年調ソフト）を無償で提供しております。年末調整手続の電子化や年調ソフトについて、詳しくは国税庁ホームページをご覧ください。\n```\n\nテキストはほぼ正確に抽出できていますが、順番が逆になっていたり、表の読み取りが誤っている部分があったので手作業で修正しました。この修正版のMarkdownファイル（mdファイル）は、Gitレポジトリ内の`data_sources`フォルダに保存されています。(`revised_nencho_all.md`)\n\nまた、表やグラフに含まれる情報をRAGで活用する場合、LLMが内容を正しく理解できるよう、データの構造を明示的にする必要があります。特に、表やグラフ形式のデータは、数値や文字情報の縦・横の関係性をLLMに伝える工夫が重要です。\n\n以下に、PDFから抽出した表データをLLM向けに変換する例を示します。\n\n**元の表（PDFのイメージ）：**\n\n|年度\t|売上（億円）\t|利益（億円）|\n|-------|-------|-------|\n|2020年\t|150\t|20 |\n|2021年\t|200\t|30 |\n\n**LLM向けに変換した形式：**\n\n- 2020年度の売上は150億円、利益は20億円です。\n- 2021年年度の売上は200億円、利益は30億円です。\n\n上記のように、表の各行を文章形式で表現し、列の情報をキーワードとして明示的に記述することで、LLMは表データの構造と内容を効果的に理解できます。この形式に変換することで、LLMは「〇〇年度の売上はいくらですか？」といった質問に対して、表データに基づいた正確な回答を生成することが期待できます。\n\n---\n長くなりましたが、以上がPDFからのデータ抽出方法となります。今回のRAG構築において、このPDFデータの前処理に最も時間を費やしました。RAGで期待通りの正確な回答を得るためには、元となるPDFデータから、いかに情報を正確に、そして適切に抽出できるかが非常に重要です。 どんなに高性能なLLMやRAGフレームワークを使ったとしても、知識源となるデータの精度が低ければ、精度の高い回答は望めません。\n特に、PDFデータは、レイアウトの複雑さや文字認識の課題など、データ抽出の難易度が高い場合があります。だからこそ、RAG構築の成否を左右すると言っても過言ではない、データの前処理には丁寧に時間をかけることが重要だと、今回の経験を通して改めて感じました。\n\n### 2. ドキュメント分割 (Text Splitting)\nPDFファイルからテキストデータを抽出したら、次にテキストを**ドキュメント分割 (Text Splitting)** する必要があります。ドキュメント分割とは、抽出したテキストを**チャンク**と呼ばれる小さな塊に分割する処理です。\n\nドキュメント分割を行う主な目的は以下の2点です。\n- **検索精度の向上:**\n質問文とPDFドキュメント全体を比較するよりも、細かく分割されたチャンク単位で比較する方が、質問内容との関連性が高い箇所を特定しやすくなります。\n\n- **LLMのコンテキストウィンドウ制限への対応:** \n大規模言語モデル (LLM) が一度に処理できるテキスト量には上限 (コンテキストウィンドウ) があります。長文のままLLMに入力すると、コンテキストウィンドウを超過し、処理が正常に行われない可能性があります。ドキュメントを分割することで、各チャンクがLLMのコンテキストウィンドウ内に収まるように調整します。\n\nLangChainでは、様々なテキスト分割方法 (Text Splitter) が提供されています。記事では、`RecursiveCharacterTextSplitter` を使用します。これは、改行文字や句読点などを区切り文字として、意味的にまとまりのある単位でテキストを再帰的に分割するText Splitter です。分割の粒度を調整するために、chunk_size (チャンクの最大文字数) や chunk_overlap (チャンク間の重複文字数) などのパラメータを設定できます。\n\n他にも、指定された区切り文字で分割する`CharacterTextSplitter`や、Markdown形式のテキストを、Markdownの構造(見出し、リスト、コードブロックなど) を考慮して分割する`MarkdownTextSplitter`などもあるため、目的に応じて使い分けることで回答性能向上も期待できます。\n\n### 3. 埋め込み生成(Embedding)\nドキュメント分割で得られたテキストチャンクに対し、**埋め込み生成 (Embedding)** を行います。埋め込み生成とは、テキストの意味内容を捉えた数値ベクトルへと変換する処理です。\n\n埋め込み生成の目的は、テキスト同士の**意味的な類似性**を数値で比較可能にすることです。テキストをベクトル化することで、意味が近い文章はベクトル空間上で近い位置に配置され、類似度を計算できるようになります。\n\n記事では、高性能な`OpenAI Embeddings`を埋め込みモデルとして採用しています。`OpenAI Embeddings`は、OpenAI社が提供するAPIを通じて利用でき、テキストの意味を高精度に捉え、多言語 にも対応している点が特徴です。LangChain の `OpenAIEmbeddings`クラスで、OpenAI Embeddings APIを容易に扱えます。\nOpenAI Embeddingsには、主に`text-embedding-ada-002`と `text-embedding-3(small/large)` の種類があり、それぞれ性能と価格が異なります。`text-embedding-ada-002`は、高性能かつ低価格 でバランスが良く、`text-embedding-3`は、さらに高性能を追求したい場合に適しています。\n\nまた、埋め込みモデルは `OpenAI Embeddings`以外にも、Hugging Face Transformers の `Sentence-BERT`など、オープンソースで高性能なモデルが多数存在します。`Sentence-BERT`は、特に文章の類似度判定に優れており、RAGの検索精度向上に貢献する可能性があります。\n\nより専門的な内容に関しては、汎用モデルではなく、ドメイン固有のデータで事前学習やファインチューニングした埋め込みモデルを利用することで、専門用語や業界特有の言い回しを正確に理解できるようになり、さらなる精度向上が期待できます。\n\n### 4. ベクトルデータベースへの登録 (Vector Store)\n埋め込み生成ステップで得られた埋め込みベクトルを、**ベクトルデータベース (Vector Store)** に登録します。ベクトルデータベースは、大量のベクトルデータを効率的に管理し、類似検索を高速に行うことに特化したデータベースです。\n\nRAGシステムにおいて、ベクトルデータベースは以下の役割を担います\n- **大量ベクトルデータの効率的な管理:**\nPDFドキュメント全体をベクトル化すると、大量の埋め込みベクトルが生成されます。ベクトルデータベースは、これらのベクトルデータを効率的に保存、管理、および検索するための基盤となります。\n- **高速な類似ベクトル検索:**\n質問文をベクトル化し、ベクトルデータベースに対して類似検索を行うことで、質問文と意味的に類似するドキュメントやテキストチャンクを高速に見つけ出すことができます。\n\n記事では、学習用途に最適な**ChromaDB**をベクトルデータベースとして採用します。\nChromaDBは、Python 環境で非常に扱いやすいオープンソースのベクトルデータベースです。インストールが簡単で、特別な設定なしにローカル環境ですぐに使い始められます。データはインメモリ(オプションで永続化も可能) で高速に処理され、LangChainとの連携もスムーズに行えるため、RAG構築の学習、プロトタイプ開発、そして個人利用に非常に適しています。\n\nまた、ベクトルデータベースはほかにも**FAISS**や**Pinecone**など規模や目的に応じたものが存在します。**FAISS**はMeta社開発の高速類似検索ライブラリで、ローカル環境でも驚異的な速度でベクトル検索を実行でき、より高速な検索性能を求める場合や、大規模なデータを扱いたい場合に適しています。**Pinecone**は、高いスケーラビリティと可用性を備え、安定した運用と大規模データ処理を実現でき、商用利用や 大規模なRAGシステムを構築する場合に適しています。\n\n###  ＜実装＞\n以上の  2.ドキュメント分割、3.埋め込み生成(Embedding)、.4. ベクトルデータベースへの登録 (Vector Store)をまとめて行うコードを以下に示します。\n\n```python:create_db.py\nfrom langchain_openai.embeddings import AzureOpenAIEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nimport os\nfrom dotenv import load_dotenv\nfrom tqdm import tqdm\n\n# 環境変数の読み込み\nload_dotenv()\n\n# Azure OpenAI の設定\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\nAPI_VERSION = os.getenv(\"API_VERSION\")\nDEPLOYMENT_ID_FOR_EMBEDDING = os.getenv(\"DEPLOYMENT_ID_FOR_EMBEDDING\")\n\n#　各チャンクの最大サイズ\nchunk_size = 300\n# チャンクサイズに対するオーバーラップの割合\noverlap_ratio = 0.25\n\n# LangChain の埋め込みクラスを初期化\nembedding_model = AzureOpenAIEmbeddings(\n    deployment=DEPLOYMENT_ID_FOR_EMBEDDING,\n    model=\"text-embedding-ada-002\",\n    openai_api_key=AZURE_OPENAI_API_KEY,\n    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    openai_api_version=API_VERSION,\n    chunk_size=chunk_size\n)\n\n# Chroma データベースの初期化\noutput_db_folder = \"./chroma_db\"\ndb = Chroma(persist_directory=output_db_folder, embedding_function=embedding_model)\n\n# Markdownファイルが保存されているフォルダ\ninput_folder = \"./data_sources/\"\n# チャンク化用の設定\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=chunk_size,  # 各チャンクの最大サイズ\n    chunk_overlap= int(chunk_size * overlap_ratio)  # チャンク間のオーバーラップ\n)\n\n# Markdownファイルを処理\nmd_files = [f for f in os.listdir(input_folder) if f.endswith(\".md\")]\nprint(f\"処理対象のMarkdownファイル: {len(md_files)} 件\")\n\nfor md_file in tqdm(md_files, desc=\"Processing Markdown Files\", unit=\"file\"):\n    try:\n        md_path = os.path.join(input_folder, md_file)\n\n        # Markdownファイルを読み込み\n        with open(md_path, \"r\", encoding=\"utf-8\") as f:\n            md_text = f.read()\n\n        # チャンク化\n        chunks = text_splitter.split_text(md_text)\n\n        # 各チャンクをDocumentオブジェクトとして作成\n        documents = [\n            Document(page_content=chunk, metadata={\"source\": md_file})\n            for chunk in chunks\n        ]\n\n        # テキストをベクトル化してChromaに保存\n        db.add_documents(documents)\n\n    except Exception as e:\n        print(f\"エラーが発生しました: {md_file} - {e}\")\n\nprint(f\"すべてのMarkdownファイルの処理が完了し、データベースが作成されました。出力先: {output_db_folder}\")\n```\nチャンクサイズはタスク、ドキュメントの種類、使用するモデル、そして目的によって大きく異なります。 例えば、技術書やFAQなど、特定の情報や手順をピンポイントで検索したケースでは比較的小さなチャンクサイズを設定しますが、小説や物語などの文脈の流れやキャラクターの関係性を理解することが重要な場合は大きなチャンクサイズを設定する必要があります。ただ、「これが正解」というサイズはなく、実験的に最適なサイズを見つけることが重要らしいです。\nまた、チャンクオーバーラップも同様に明確な正解はありませんが、チャンクサイズの25％程度の値に設定することが多いらしいです。\nこれらの値はRAGの精度に直結してくるものですので、慎重に調整を行う必要があります。\n\n---\n以上で外部データの登録処理は完了です。次に、登録した外部データを活用し、RAGを用いた質問応答を実現するための検索および生成処理に進みます。\n\n## 4.3 検索と応答生成 (Retrieval & Generation)\n外部データの登録が完了したら、登録したデータを使って質問応答を行うフェーズです。このフェーズは大きく分けて 検索 (Retrieval) と 応答生成 (Generation) の2つのステップから構成されます。 ユーザーからの質問に対して、RAGシステムがどのように答えを導き出すのか、解説していきます。\n\n---\n### 1. 質問(クエリ)の入力・前処理\n最初のステップでは、ユーザーからの質問（クエリ）を受け取り、RAGシステムが処理しやすいように前処理を行います。\n\nまず、ユーザーはRAGシステムに対して、自然言語で質問を入力します。例えば、「〇〇について教えてください」「〇〇のやり方は？」といった具体的な質問です。\n\n入力された質問は、そのままでは検索処理に利用できない場合があります。そこで、以下のような前処理を行います。\n\n- トークン化：\n質問文を単語や記号などの単位（トークン）に分割します。\n- 正規化：\n表記ゆれの修正、小文字化、不要な記号の除去などを行い、質問文を統一的な形式に整えます。\n- 埋め込み (ベクトル化)：\n前処理された質問文を、外部データ登録の際と同様に埋め込みモデルを用いてベクトルに変換します。ベクトル化することで、質問の意味内容を数値データとして表現し、後続の類似度検索を効率的に行う準備をします。\n\nまた、より精度を向上させるために、ユーザーの質問をLLMに再定義させることで、曖昧な表現を明確化し、より具体的なクエリに変換する場合もあります。詳しい内容については、 第5章 精度向上のためのテクニック で解説します。\n\n---\n### 2. 類似度検索（Retrieval）\n前処理でベクトル化された質問（クエリベクトル）を使い、登録された外部データの中から質問内容と関連性の高いチャンクを検索します。以下に主要な手順をまとめます。\n\n**1. ベクトルデータベースとの照合**\n- 事前にベクトル化して登録しておいた外部データのチャンク（ドキュメントベクトル）と質問のベクトルをベクトルデータベース上で比較します。\n\n**2. 類似度計算**\n- ベクトル同士の類似度を計算します。類似度が高いほど、質問とチャンクの内容が近いと判断されます。\n- **コサイン類似度**などがよく使用されます。\n\n**3. チャンクのランキング**\n- 類似度が高い順にチャンクをランキングします。\n- 上位のチャンクほど、質問への回答に役立つ可能性が高いと考えられます。\n\n**4. 上位K件の取得**\n- ランキング上位から **K件**（事前に設定した数）のチャンクを取得します。\n- 取得されたチャンクは、次のステップの応答生成で利用されます。\n\nなお、取得するチャンクの数や類似度計算のアルゴリズムは、データの特性やタスクに応じて調整が必要です。試行錯誤を重ねて最適な設定を見つけることが重要です。\n\n---\n### 3. LLMへのプロンプト構築\n検索ステップで取得した関連性の高いチャンクを、LLM（大規模言語モデル）に入力するためのプロンプトを構築します。\n\n質問文に加えて、検索されたチャンクを文脈情報として **指示文(プロンプト)** に組み込みます。これにより、LLMは外部データの内容を踏まえて回答を生成することができます。\n\n```:プロンプト例\n＜質問＞ \n{ユーザーからの質問}\n\n＜参考情報＞\n{検索されたチャンク1}\n{検索されたチャンク2}\n...\n{検索されたチャンクK}\n\n＜指示＞\n1. 参考情報のみから判断してください。\n2. 参考情報から判断できない場合、「分かりません」と答えてください。\n3. 根拠となった参考情報を提示してください\n\n上記の参考情報と指示に基づいて、質問に答えてください。\n```\n\nより良い回答を得るためには、プロンプトの表現や構成を工夫する**プロンプトエンジニアリング**が重要です。\nプロンプトでは、以下のポイントを意識すると効果的です：\n  - 必要な情報を簡潔に記載する。\n  - 指示文を明確にし、期待する回答形式を伝える。\n  - 文脈に応じて具体的な例やフォーマットを提示する。\n\n上の例では、指示1によりLLMは学習された知識ではなく、参考情報のみを用いて判断します。また、指示2により、参考情報に該当がない場合は「分かりません」と明確に回答するため、誤回答(ハルシネーション)が軽減できます。さらに、指示3によって、回答に根拠を含めることで、応答の透明性が向上します。\n\nプロンプト設計はLLMの解答精度に直結するため、適切な文脈付与と試行錯誤による調整が重要な工程となります。\n\n---\n### 4. 応答生成（Generation）\n構築されたプロンプトをLLMに入力し、質問に対する回答を生成させます。LLMは、プロンプトに含まれる質問と文脈情報（検索されたチャンク）に基づいて、自然言語で回答を生成します。本記事では、同様にGPT 4o-miniのチャットモデルを使用していますが、使用するモデルはタスクの性質や応答の要件に応じて選択することが重要です。\n\n例えば、より高度な推論や詳細な回答が必要な場合には、GPT-4やLlama70Bのような大規模モデルを活用し、逆に簡潔な回答やリアルタイム性が重視されるタスクでは、Llama 2 7Bなどの軽量モデルを利用することが推奨されます。\n\n---\n### 5. 応答の後処理・出力\n最後に、LLMによって生成された回答を、必要に応じて後処理し、ユーザーに出力するステップです。\n\n生成された回答に対して、以下のような後処理を行います：\n\n- **言い換え**  \n  回答文をより自然で分かりやすい表現に言い換えることで、ユーザーにとって理解しやすくします。\n\n- **不要な情報の削除**  \n  回答に含まれるノイズや冗長な部分を取り除き、必要な情報のみを残します。\n\n- **フォーマット調整**  \n  回答をユーザーインターフェースに適した形式に整形します。  \n  例: マークダウン形式、箇条書き、表形式など。\n\nその後、後処理された回答をユーザーに提示します。適切な形式で出力することで、ユーザーは質問に対する回答を得ることができます。\n\n---\n### ＜実装＞\n4.3 検索と応答生成 (Retrieval & Generation)を実行するためのコードを以下に示します。LangChainを用いることで、簡単に実装できます。\n\n```python:query.py\nfrom langchain_openai.embeddings import AzureOpenAIEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai.chat_models import AzureChatOpenAI\nfrom langchain.schema import HumanMessage\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Azure OpenAI の設定\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\nAPI_VERSION = os.getenv(\"API_VERSION\")\nDEPLOYMENT_ID_FOR_EMBEDDING = os.getenv(\"DEPLOYMENT_ID_FOR_EMBEDDING\")\nDEPLOYMENT_ID_FOR_CHAT_COMPLETION = os.getenv(\"DEPLOYMENT_ID_FOR_CHAT_COMPLETION\")  # Chat用のデプロイ名\n\n# LangChain の埋め込みクラスを初期化\nembedding_model = AzureOpenAIEmbeddings(\n    deployment=DEPLOYMENT_ID_FOR_EMBEDDING,  # デプロイ名\n    model=\"text-embedding-ada-002\",  # 埋め込みモデル名\n    openai_api_key=AZURE_OPENAI_API_KEY,\n    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    openai_api_version=API_VERSION,\n    chunk_size=300\n)\n\n# Azure ChatOpenAI を初期化\nchat = AzureChatOpenAI(\n    deployment_name=DEPLOYMENT_ID_FOR_CHAT_COMPLETION,  # Chat用のデプロイ名\n    openai_api_key=AZURE_OPENAI_API_KEY,\n    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    openai_api_version=API_VERSION,\n)\n\n# Chroma データベースの初期化\noutput_db_folder = \"./chroma_db\"\ndb = Chroma(persist_directory=output_db_folder, embedding_function=embedding_model)\n\n# 質問の定義\nquery = \"令和6年分の年末調整は前年と異なる部分がありますか？\"\n\n# データベースから類似度の高いドキュメントを取得\ndocuments = db.similarity_search(query, k=5)\n\n# ドキュメントの内容を結合\ndocuments_string = \"\\n\".join([f\"---------------------------\\n{doc.page_content}\" for doc in documents])\n\n# プロンプトテンプレートを初期化（1段階に統合）\ncombined_prompt = PromptTemplate(\n    template=\"\"\"以下の文章と質問を基にして、質問に対する答えを日本語で出力してください。\n\n1. 文章のみから判断してください。\n2. 文章から全く判断できない場合、「分かりません」と答えてください。\n3. ソースとなった文章を提示してください\n\n文章:\n{document}\n\n質問: {query}\"\"\",\n    input_variables=[\"document\", \"query\"]\n)\n\n# チャットモデルに問い合わせ\nresponse = chat.invoke([\n    HumanMessage(content=combined_prompt.format(document=documents_string, query=query))\n])\n\nanswer = response.content.strip()\n\n# 結果を出力\nprint(f\"質問: {query}\")\nprint(f\"回答: {answer}\")\n```\n上のコードを実行すると、以下のような出力結果が得られました。\n``` :質問回答結果      \n質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n回答: はい、令和6年分の年末調整には前年と比べて変わった点があります。具体的には、令和6年分所得税について定額による特別控除（定額減税）が実施されていることです。\n\nソース:\n「令和6年分所得税について、定額による所得税の特別控除（以下「**定額減税**」といいます。）が実施されています。」\n```\n\n\n上のコードでは類似度の高いドキュメント上位5件を取得していますが、タスクによって取得件数を調整することが重要です。例えば、広範な情報が必要な場合は件数を増やし、逆に精度重視で特定の情報を求める場合は件数を減らすことで、応答の質を最適化できます。\n\n---\n以上で基本的なRAGシステム構築は完了となります。しかし、実際の運用では、タスクやデータに合わせてシステムをカスタマイズすることが求められます。次の章では、さらに応答精度を向上させるための具体的なテクニックについて解説します。\n\n# 第5章 精度向上のためのテクニック\n精度向上のためには、まず**PDFからのデータ抽出が正確に行われていることが大前提**です。そのうえで、より高性能な埋め込みモデルを使用したり、ベクトルデータベースを最新のものに置き換えたり、あるいはチャットモデルを高性能なものに切り替えたりするなど、システムの基盤的なアップグレードが挙げられます。しかし実運用においては、システムのアップグレードだけでは十分に精度が上がらない状況に直面することもあるかと思います。そのため、本章ではこれらの基本的な手法に加え、より細かな工夫や補助的な方法によって、さらなる精度の向上を図るテクニックを紹介します。\n\n## 1. チャンクサイズの調整\nチャンクサイズとは、文章を分割する際の一塊あたりの長さを指します。小さすぎると文脈が分断されて本来は関係の深い情報が分散され、関連性の薄い情報まで同時に取り込んでしまい、類似度算出が不正確になるデメリットがあります。\n\n以下に、年末調整に関するPDFデータを使用し、チャンクサイズを 50、300、600 に変更した際の出力結果を示します。なお、チャンクの重なり（オーバーラップ）は、それぞれのチャンクサイズの 25% に設定しています。\n\n```:チャンクサイズ=50\n質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n回答: 分かりません。\n```\n```:チャンクサイズ=300\n質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n回答: はい、令和6年分の年末調整には前年と異なる部分があり、特に定額減税の実施が新たに行われています。\n```\n\n```:チャンクサイズ=600\n質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n回答: 令和6年分の年末調整には前年と異なる部分があります。特に、令和6年分所得税について定額減税が実施されており、その年末調整時点の定額減税の額を算出し、年間の所得税額の計算を行う必要があることが明記されています。また、合計所得金額が1,805万円を超える場合、年調減税額を控除しないなどの規定も変更されています。\n```\nこの結果から、チャンクサイズの調整は、質問応答の精度に大きく影響を与えることが分かります。\nチャンクサイズが小さい場合（50）、質問に対する回答を得られませんでした。これは、チャンクが短すぎるために、質問に必要な情報がチャンク内に含まれていなかった、もしくは文脈が途切れてしまい、AIが質問とチャンクの内容を適切に紐付けられなかったと考えられます。\n一方、チャンクサイズを大きくするにつれて（300、600）、回答の質が向上しています。チャンクサイズ300では、令和6年分の年末調整に前年と異なる点があること、そしてその一つが定額減税であることを捉えられています。さらにチャンクサイズ600では、定額減税に関するより詳細な情報、例えば年末調整時点での減税額算出や、合計所得金額による規定の変更など、より深い情報まで回答に含めることができています。\n\nこのように、チャンクサイズの調整は、簡単でありながら質問応答システムの性能を大きく左右する、非常に重要な要素と言えます。\n\n## 2. 取得するチャンク数の設定\nチャンクベースで検索や処理を行う場合、検索時に取得するチャンク数が少なすぎると必要な情報を十分に得られず、逆に多すぎると不要な情報が混在してノイズとなり、有用な情報が埋もれてしまう場合があります。したがって、アプリケーションの目的や文書の内容、ユーザーのクエリの特性に応じて、取得するチャンク数を適切に設定することが重要です。\n\n具体的には、チャンク数が少ないと文脈や細部が欠落してしまい、回答の根拠が不十分になるリスクがあります。一方で、チャンク数が多すぎると、処理コストが増大するだけでなく、ノイズが大きくなり回答が曖昧になりやすくなります。ただし一部では、ノイズの種類によっては背景知識の追加として機能し、回答の精度向上につながったケースが報告されています。こういった例外的な状況も考慮しつつ、実際のテストやチューニングを繰り返して最適なチャンク数を見極めることが重要です。\n## 3. メタデータの活用\n文章の内容だけでなく、メタデータと呼ばれる文書に付随する情報を効果的に利用することも重要です。メタデータとは、作成日時や著者名、文書の種類、部門やカテゴリーなど、テキストそのもの以外の属性情報を指します。\n\n**＜メタデータが果たす役割＞**\n- **検索の効率化**\n検索やリランキングの際にメタデータを活用すると、関連性の高いドキュメントを素早く絞り込めます。たとえば、“契約書”だけを検索対象とする、特定の年月に作成された文書に限定するなど、カテゴリや時系列に基づくフィルタリングが簡単に行えます。\n\n- **ノイズの除去**\nメタデータを使って、目的に合わないドキュメントをあらかじめ排除できます。たとえば、対象が「社内規定」に関する文書のみの場合、それ以外の業務マニュアルや顧客資料を省くことができ、回答の精度を向上させられます。\n\n- **リランキングへの貢献**\nRAGにおけるリランキング工程では、クエリと文書内容の類似度だけでなく、メタデータに基づくスコアリングを加味することで、より的確な文書を上位に表示できます。たとえば、最近更新された文書を優先する、特定の著者や部署が作成した文書を高評価するなどの仕組みが考えられます。\n\n**＜メタデータ活用の実装例＞**\n- **部門タグを使ったフィルタリング**\n文書に「経理部」「人事部」などのタグが付与されている場合、クエリが「給与計算」「雇用契約」などのテーマを含んでいるときは「経理部」「人事部」のタグがある文書を優先的に検索・リランキングの対象とする。\n\n- **時系列情報の活用**\n「更新日」が含まれるメタデータを用いて、最新の文書を上位に表示する。税制や法律の情報は古い文書よりも最新の文書が正確な場合が多いため、回答の精度が上がる。\n\n- **セクション情報の活用**\n文書を章や節ごとに分割し、その構造情報をメタデータとして扱う。ユーザーが「第3章の細則について詳しく教えて」と尋ねた場合に、該当セクションに絞った情報を提示できる。\n\n**＜メタデータ活用時の注意点＞**\n- **メタデータの整備**\nメタデータが正確に整備されていないと、誤ったフィルタリングやリランキングの原因になります。データの標準化や定期的な更新管理が必要です。\n\n- **過剰な制限に注意**\nメタデータで厳しくフィルタリングしすぎると、有用な文書を見落とす可能性があります。必要に応じて柔軟にスコアリングを調整し、情報を狭めすぎないようにすることが重要です。\n\n- **セキュリティとの兼ね合い**\n部門やアクセス権限などのメタデータを扱う場合、誤って機密情報が検索・リランキングの対象にならないように、適切な権限管理が必要です。\n\nメタデータの活用は、**単にテキストベースの検索では拾いきれないドキュメントの属性情報を検索・リランキングに反映するための有力な手段**です。正しく管理されたメタデータを活用すれば、RAGワークフロー全体の精度を高めるだけでなく、回答をより効率的かつ適切に取得できるようになります。\n## 4. 検索アルゴリズムの選択\n\n検索アルゴリズムを選定する際には、データの特性や検索の目的に応じて適切な手法を選ぶことが重要です。検索手法は、大きく分けて「**類似度検索**」と「**キーワード検索**」に分類できます。\n\n**＜類似度検索＞**\n\n類似度検索は、文章やデータ同士の「**意味的な近さ**」を計算し、関連性の高い情報を抽出する手法です。  \n代表的な類似度計算の方法としては、以下のようなものがあります。\n\n- **コサイン類似度**  \n  ベクトルの角度を基準に類似性を測る手法で、テキスト検索や文書分類に適しています。  \n- **ユークリッド距離**  \n  データ間の直線距離を計算する方法で、数値データや位置情報の類似度を求める際に用いられます。  \n- **マンハッタン距離**  \n  縦横の移動のみを考慮した距離計算方法で、高次元データの類似度評価に適している場合があります。  \n- **ジャカード係数**  \n  集合の共通要素の割合を比較する手法で、タグベース（キーワードやカテゴリ情報）の類似度計算に有効です。\n\n類似度検索では、**文章全体の文脈**を考慮した検索が可能ですが、専門用語や固有名詞がベクトル化される過程で本来の意味が失われるリスクもあります。\n\n**＜キーワード検索＞**\n\nキーワード検索は、特定の単語やフレーズを文字列として厳密に検索する方法です。\n\n- 法律文書や商品名、社名など、**正確な一致**が求められる場面に適しています。  \n- 言葉の表現が異なる場合にヒットしないなど、**言い換えへの対応が困難**という課題があります。\n\n**＜組み合わせによる精度向上＞**\n\n実際の検索システムでは、以下のように**キーワード検索と類似度検索を組み合わせる**ことで、より精度の高い情報取得が可能になります。\n\n1. **キーワード検索で候補を絞り込む**  \n   まずは厳密な文字列一致により、範囲を狭めます。  \n2. **類似度検索を使って、関連性の高い情報を取得する**  \n   候補の中から、文脈や意味に基づいて最適な結果を得ます。\n\nこのように、**キーワード検索の正確性**と**類似度検索の柔軟性**を組み合わせることで、**精度と効率**を両立できます。検索の目的やデータの特性に応じて、どの手法をどのように組み合わせるかを検討しましょう。\n\n## 5. リランキング\nRAGのプロセスでは、まず検索段階で関連性の高い文書を取得し、それらを元に最終的な文章生成を行います。しかし、一次検索（ファーストパス）で取り出した文書が必ずしも最適とは限りません。そこで、**リランキング**という手法を用いて、検索結果を再評価し、最も関連性の高い文書を上位に並べ直すことが重要になります。\n\n**＜RAGにおけるリランキングの流れ＞**\n\n1. **一次検索（Retrieval）**  \n   - 大まかな手法（たとえばベクトル類似度検索）で候補文書を抽出し、その中からある程度数を絞り込みます。\n\n2. **リランキング（Re-ranking）**  \n   - 抽出された候補文書を、より高度な手法や追加の特徴量を用いて**再スコアリング**し、順位付けをやり直します。  \n   - 例：大規模言語モデル（LLM）の埋め込みを使ったより詳細な類似度計算など\n\n3. **最終的な文章生成（Augmented Generation）**  \n   - リランキングによって厳選された文書をコンテキストとして、LLMに文章生成を行わせます。\n\n\n**＜リランキングの必要性とメリット＞**\n\n- **検索精度の向上**    \n  LLMは入力の先頭部分に注目しやすい特性があります。そのを活かし、関連性の高い文書を上位に配置することで、最適な回答生成を促します。\n\n- **追加の特徴量やアルゴリズムを反映**  \n  ファーストパスで単純なベクトル検索を行い、リランキング段階で高度なテキスト埋め込みやモデル推論結果を取り入れることで、計算コストを抑えながらも精度を向上させられます。\n\n- **柔軟なフィルタリング**  \n  不要な情報やドメインが違う文書を後から除外するなど、リランキングで再度スコアリングすることでノイズ除去がしやすくなります。\n\n**＜リランキングの実装例＞**\n\n1. **BM25 + LLM埋め込み再評価**  \n   - ファーストパス：BM25などの伝統的な検索手法で候補を絞り込み  \n   - リランキング：LLMを使って埋め込みベクトルを生成し、コサイン類似度などで再スコアリング  \n   - 生成：厳選した上位文書をプロンプトに組み込み、回答を生成\n\n2. **キーワード検索 + 類似度計算モデルの組み合わせ**  \n   - ファーストパス：キーワード検索で高速に候補抽出  \n   - リランキング：BERT系モデルなどの文章埋め込みを使い、文脈に応じた類似度を計算して順位を付け直す  \n   - 生成：リランキング後の文書を追加コンテキストにして生成モデルを動作させる\n\n**＜注意点＞**\n\n- **計算コスト**  \n  リランキングに高負荷なモデルを使うと、その分計算コストが上がります。適度に絞り込んだ上でリランキングを行う戦略が重要です。\n\n- **過剰なフィルタリング**  \n  絞り込みすぎると、本来意味がある文書を逃してしまう可能性があります。設定するスコアの閾値や候補数をチューニングすることが大切です。\n\n\nRAGワークフローで高品質な回答を得るには、**ファーストパス検索**で幅広く候補文書を取得しつつ、**リランキング**でより高度に関連度を評価して最終的に厳選した文書をLLMに渡すことで、ノイズを抑えた、より正確な生成結果が期待できます。\n\n## 6. プロンプトエンジニアリング\nRAGの精度を向上させるためには、**検索結果の適切な利用と、LLMへの指示の最適化**が不可欠です。  \n適切なプロンプトエンジニアリングを行うことで、データに基づいた正確な回答を得ることが可能になります。以下に例を示します。\n\n**＜プロンプトエンジニアリングの例＞**\n\n**1. 質問と参考情報の構造を明確にする**\n検索結果をそのまま並べるのではなく、**質問部分と関連文書（参考情報）を区別**して提示することで、モデルがどの情報をもとに回答すべきかを理解しやすくします。 \n\n **例：構造化プロンプト**\n```plaintext\n【質問】  \n2024年の所得税控除の変更点は？  \n\n【参考情報】  \n- 所得税控除は2024年に5万円増額されました。  \n- 医療費控除には小規模な変更がありました。\n```\n**なぜ有効か？**\n- モデルが情報の役割（質問・情報源）を明確に把握できる\n- 検索結果に基づく回答を促し、余計な推論を抑制できる\n\n---\n**2. 「分からない」と答えることを許可する**\nLLMは情報が不足していても、推測を交えて回答を生成しようとする傾向があります。そこで、**明確な情報がない場合は「分かりません」と答えるように促す**と、誤った回答を減らすことができます。\n\n**例：不確実な回答を避けるプロンプト**\n```\n以下の情報のみを基に質問に回答してください。\nもし、明確な情報が見つからない場合は、「分かりません」と回答してください。\n\n【質問】  \n ~~~~~\n【参考情報】\n ~~~~~\n```\n**なぜ有効か？**\n- 不確実な情報を無理に生成するリスクを低減\n- データが不足している際に誤答が生じる可能性を抑制\n\n---\n**3. データベースの情報のみを基に判断させる**\nLLMは、事前学習された知識をもとに推論しがちです。そこで、**検索結果のみに基づいて回答するよう、あらかじめ指示する**ことで、RAGの目的に沿った回答を得やすくなります。\n\n**例：推測を抑制するプロンプト**\n```\nあなたはデータベースから取得した情報のみを基に回答するアシスタントです。\n事前知識や推測を加えず、以下の参考情報のみをもとに質問に回答してください。\n\n【質問】  \n ~~~~~\n【参考情報】 \n ~~~~~\n```\n\n**なぜ有効か？**\n- 事前学習された知識による推測を抑え、検索結果ベースの回答を生成できる。\n\n ---\n**4. 回答のフォーマットを指定する**\nモデルが不要な情報を追加しないよう、回答形式（箇条書き・表形式・最大文字数など）を指定すると、情報が整理され、誤答や冗長な回答が減る傾向があります\n\n**例：箇条書きでの回答を指示**\n```\n次の情報を参考に、質問に対して 「簡潔な箇条書き」 で回答してください。\n\n【質問】  \n ~~~~~\n【参考情報】 \n ~~~~~\n```\n**なぜ有効か？**\n- 冗長な回答や背景知識の羅列を防ぎ、要点を簡潔にまとめさせやすい\n---\n**5. 生成のステップを分割し、段階的に回答させる**\n一度に結論を出させるのではなく、「**関連情報の要約**」→「**最終回答**」 のように複数ステップに分けると、モデルが文脈を整理しやすくなり、回答の精度が向上します。\n**例：ステップを分割する**\n```\n① まず、以下の参考情報を要約してください。\n② その要約をもとに、質問に回答してください。\n【質問】  \n ~~~~~\n【参考情報】 \n ~~~~~\n```\n**なぜ有効か？**\n- 長い文章や複雑な情報を段階的に処理し、より正確な回答を導きやすい。\n---\n上記のようなプロンプトエンジニアリングを行うことで、RAGの検索結果を最大限に活用し、モデルの不必要な推測を抑えながら正確な回答を引き出せます。\nこれらはあくまで一例であり、実際のユースケースに応じてさらに細かい指示や別の工夫を取り入れることで、RAGの精度と信頼性を一層高めることが可能です。\n\n## 7. クエリ拡張\n\nRAGの精度を向上させるためには、検索クエリを適切に拡張し、より関連性の高い情報を取得することが重要です。**クエリ拡張（Query Expansion）** は、元のクエリをより詳細にリフレーズしたり、関連する表現を追加したりすることで、検索結果の精度を向上させる手法です。\n\n例えば、以下のような状況が考えられます。\n- ユーザーの入力が曖昧な場合（例：「控除の申請はどうするの？」 → 「年末調整における扶養控除の申請方法を教えてください。」）\n- 同義語や類義語が考えられる場合（例：「マイナンバー」 → 「個人番号」）\n- クエリが短すぎて検索対象を十分にカバーできない場合（例：「控除の方法」 → 「2024年の所得税における控除の申請方法」）\n\nクエリ拡張についての詳細な情報は、[こちら](https://speakerdeck.com/smiyawaki0820/retrieval-based-lm-rag-system-zatukurili-jie-suru)のサイトの資料で非常にわかりやすくまとめられており、参考になりました。\n\n---\n### **＜実装例＞**\n\n以下のコードでは、ユーザーが入力したクエリを、LLM（大規模言語モデル）を用いてリフレーズし、検索に適した形に変換する方法を示しています。\n\n**コード例：クエリのリフレーズ**\n```python:query_expansion_1.py\nfrom langchain_openai.embeddings import AzureOpenAIEmbeddings\nfrom langchain_chroma import Chroma\nfrom langchain.prompts import PromptTemplate\nfrom langchain_openai.chat_models import AzureChatOpenAI\nfrom langchain.schema import HumanMessage\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\n# Azure OpenAI の設定\nAZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\nAZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\nAPI_VERSION = os.getenv(\"API_VERSION\")\nDEPLOYMENT_ID_FOR_EMBEDDING = os.getenv(\"DEPLOYMENT_ID_FOR_EMBEDDING\")\nDEPLOYMENT_ID_FOR_CHAT_COMPLETION = os.getenv(\"DEPLOYMENT_ID_FOR_CHAT_COMPLETION\")\n\n# LangChain の埋め込みクラスを初期化\nembedding_model = AzureOpenAIEmbeddings(\n    deployment=DEPLOYMENT_ID_FOR_EMBEDDING,\n    model=\"text-embedding-ada-002\",\n    openai_api_key=AZURE_OPENAI_API_KEY,\n    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    openai_api_version=API_VERSION,\n    chunk_size=300\n)\n\n# Azure ChatOpenAI を初期化\nchat = AzureChatOpenAI(\n    deployment_name=DEPLOYMENT_ID_FOR_CHAT_COMPLETION,\n    openai_api_key=AZURE_OPENAI_API_KEY,\n    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n    openai_api_version=API_VERSION,\n)\n\n# Chroma データベースの初期化\noutput_db_folder = \"./chroma_db\"\ndb = Chroma(persist_directory=output_db_folder, embedding_function=embedding_model)\n\n# 質問の定義\noriginal_query = \"令和6年分の年末調整は前年と異なる部分がありますか？\"\n\n# 質問をリフレーズするプロンプト\nrephrase_prompt = f\"\"\"\n以下の質問をリフレーズしてください。できるだけ詳細で検索に適した形にしてください。\n質問: {original_query}\n\"\"\"\n# 質問をリフレーズ\nrephrased_query = chat.invoke([HumanMessage(content=rephrase_prompt)]).content.strip()\n\n# リフレーズされた質問を拡張クエリとして使用し、データベースから類似度の高いドキュメントを取得\ndocuments = db.similarity_search(rephrased_query, k=5)\n\n# ドキュメントの内容を結合\ndocuments_string = \"\\n\".join([f\"---------------------------\\n{doc.page_content}\" for doc in documents])\n\n# プロンプトテンプレートを初期化\ncombined_prompt = PromptTemplate(\n    template=\"\"\"以下の文章と質問を基にして、質問に対する答えを日本語で出力してください。\n\n1. 文章のみから判断してください。\n2. 文章から全く判断できない場合、「分かりません」と答えてください。\n3. ソースとなった文章を提示してください。\n\n文章:\n{document}\n\n質問: {query}\"\"\",\n    input_variables=[\"document\", \"query\"]\n)\n\n# チャットモデルに問い合わせ\nresponse = chat.invoke([\n    HumanMessage(content=combined_prompt.format(document=documents_string, query=original_query))\n])\n\nanswer = response.content.strip()\n\n# 結果を出力\nprint(f\"元の質問: {original_query}\")\nprint(f\"リフレーズされた質問: {rephrased_query}\")\nprint(f\"回答: {answer}\")\n\n```\n\n**リフレーズによる出力結果**\n```\n元の質問: 令和6年分の年末調整は前年と異なる部分がありますか？\nリフレーズされた質問: 令和6年分の年末調整において、前年とは異なる点があるかどうか、具体的な変更内容や新たな制度、手続きの違いについて詳しく教えてください。\n回答: はい、令和6年分の年末調整には前年と異なる部分があります。具体的には、定額減税の実施があり、年末調整時点での定額減税の額を算出する必要があります。\n```\n\n---\n**＜クエリ拡張のメリット＞**\n**1. 検索精度の向上**\nLLMを活用してクエリ拡張することで、より具体的で詳細なクエリが生成され、検索エンジンやベクトルデータベースが適切な結果を返しやすくなります。\n\n**2. ユーザーの意図を明確にできる**\nユーザーが入力するクエリは、主観的で曖昧な表現を含むことが多く、検索対象に適さない場合があります。クエリ拡張を行うことで、意図を明確にしたクエリを生成できます。\n\n**3. 検索対象の幅を広げる**\nクエリを拡張することで、単一の表現だけでは得られない多面的な情報を取得しやすくなります。単純なキーワードに依存している場合、特定のソースからの情報に偏りがちですが、複数の言い回しや関連用語を加えることで、検索結果の網羅性が向上します。\n\n**＜応用例＞**\n\n以下では、リフレーズ手法をさらに発展させる形で、さまざまなクエリ拡張の方法を紹介します。各手法を実行するための Python ファイルは、Git リポジトリに公開されていますので、興味のある方はぜひ試してみてください。\n\n 1．**複数のクエリを作成し検索する方法（`query_expansion_2.py`）**  \n   - **概要**: 元のクエリを複数のバリエーションにリフレーズし、それぞれのクエリで検索を行います。その後、複数の検索結果を統合（たとえば RRF：Reciprocal Rank Fusion）することで、検索精度を高める手法です。  \n   - **メリット**: 一つのクエリだけでは見落としてしまう情報を、複数のクエリで補い合う形で検索できるため、情報の網羅性が向上します。  \n```:クエリ改善例\n元の質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n リフレーズされた質問: ['1. 令和6年の年末調整には、前年と比べて変更点がありますか？', '2. 令和6年の年末調整で、昨年とは異なる点がございますか？', '3. 令和6年分の年末調整には、前年とは異なる要素が含まれていますか？', '4. 令和6年度の年末調整は、前年度と違う部分がありますか？', '5. 令和6年の年末調整に関して、前年と異なる部分はありますか？']\n```  \n---\n2． **検索結果に基づいてクエリを動的に改善する手法（`query_expansion_3.py`）**  \n   - **概要**: まずは元のクエリで検索を行い、結果が不十分と判断された場合に、LLM（大規模言語モデル）を用いてクエリそのものを再構築（リフレーズや補足）し、再検索を行います。  \n   - **メリット**: 検索結果をフィードバックとして活用することで、段階的にクエリを最適化し、適切な情報にたどり着くまでクエリを改善できます。特に、不明点や不足が明確になった場合に有効です。  \n```:クエリ改善例\n元の質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n改良された質問: 令和6年分の年末調整の方法と、前年との違いについて詳しく解説している情報を探しているのですが、特に定額減税に関する変更点や手続きの特徴、注意すべきポイントについて具体的な情報が知りたいです。\n```\n\n3． **元の質問を抽象化して検索を行い、その後具体的な回答を導出する手法（`query_expansion_4.py`）**  \n   - **概要**: 質問が具体的すぎる場合、最初に質問を抽象化して検索することで、より汎用的な文書を取得します。その後、取得した情報をもとに、再度質問に合わせた具体的な回答を生成します。  \n   - **メリット**: あまりにも細分化されたクエリでは十分な検索結果が得られない場合がありますが、抽象化することで必要な背景知識や関連する一般的な情報を得やすくなります。最終的には元の質問に合った具体的な回答へ落とし込むため、柔軟性と網羅性を両立できます。\n```:query_rephrase_4.py実行結果\n元の質問: 令和6年分の年末調整は前年と異なる部分がありますか？\n抽象化された質問: 質問: 年末調整における変更点や新たな要素は何ですか？\n抽象的な回答: 年末調整における変更点や新たな要素として、令和6年分の所得税において**定額減税**が実施されることが挙げられます。これに伴い、年末調整の際には定額減税の額を算出し、それを基に年間の所得税額を計算する必要があります。この新しい要素により、年末調整に関連する事務が従来とは異なり 、特別控除の考慮が求められるようになります。詳細や手続きについては、国税庁のホームページでも案内されているため、参照することが推奨されます。   \n具体的な回答: 令和6年分の年末調整には、前年と異なる部分として定額減税が実施されます。これにより、年末調整の際に定額減税の額を算出し、それに基づいて年間の所得税額を計算する必要があります。この新しい要素により、年末調整に関連する事務が従来とは異なり、特別控除の考慮が求められるようになり ます。詳細や手続きについては、国税庁のホームページを参照することが推奨されます。\n```\nこれらの手法を組み合わせることで、**クエリ拡張の効果を最大化し、より正確かつ豊富な検索結果を得ることが可能**になります。特に、検索結果の活用や抽象化・具体化といったアプローチは、RAG（Retrieval-Augmented Generation）の精度向上にも大いに役立ちます。ぜひ、各スクリプトを実行して試してみてください。\n\n---\nクエリ拡張は、RAGの検索プロセスにおいて精度を向上させる重要な手法の一つです。\nLLMを用いたリフレーズを活用することで、より具体的で意味の明確なクエリを生成し、適切な検索結果を取得できるようになります。また、類義語の追加や補足情報の挿入など、他の手法と組み合わせることで、さらなる精度向上が期待できます。\n\n# 第6章 まとめ\n本記事では、RAG（Retrieval-Augmented Generation）の基本的な仕組みから、LangChainを活用した実装方法、検索精度を向上させる工夫までを紹介しました。検索の精度を高めるには、チャンクサイズの調整やメタデータの活用、リランキング、クエリ拡張など、さまざまなアプローチがあります。特に、クエリのリフレーズや動的な改善、抽象化を取り入れることで、より的確な検索結果を得ることができることが分かりました。\n\nRAGは、大規模言語モデルをより効果的に活用し、最新の情報や専門知識を柔軟に組み込める点で非常に有用な技術です。本記事の内容が、RAGを活用したシステムの構築や検索精度向上の参考になれば幸いです。\n今回利用した環境やコードは[こちら](gitのURL)にまとめております。\n\n# 第7章 参考資料\n**＜書籍＞**\n- [大規模言語モデル入門](https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297136333/ref=sr_1_2_sspa?adgrpid=156801342469&dib=eyJ2IjoiMSJ9.iUa4odO5qgcS966iULz3HtcTc2uqQu0lVWd7hosPI6MA14Y-B61aYYPUS92JRkE7ne1PM9ASU07HNCHOLA7b1AsP5nf8Gh2K4u3cUCYWpGMiXa29adm6yymKO0P-1WR6VnXtocbGKS0d8INl1SPdfa5u1aOdbFvoXbp9E2Sut-FgGxy0C62OoSxk-c-gySknbxhccbn5FJI_YueYoZpgzhHgaRPkZz1fPt5my_oHa6Kl79IQQ_j4gJ8CdY1RLlSIBn-j8TDYLwQsYIgQzSgEe6fwgmFjZlM5VrBO_srMSL4.vLuK9aFlV34yjO3H2Gtu1sQMjDi3V6hnVZJjQABhgfY&dib_tag=se&gad_source=1&hvadid=685864338618&hvdev=c&hvlocphy=9166121&hvnetw=g&hvqmt=e&hvrand=18040040681330066797&hvtargid=kwd-2270348275400&hydadcr=1792_13657028&jp-ad-ap=0&keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1+%E8%A8%80%E8%AA%9E+%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&mcid=753301ec7f8030b2bd433cf43109b8ff&qid=1738131408&sr=8-2-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1)\n- [大規模言語モデル入門Ⅱ〜生成型LLMの実装と評価](https://www.amazon.co.jp/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E5%85%A5%E9%96%80%E2%85%A1%E3%80%9C%E7%94%9F%E6%88%90%E5%9E%8BLLM%E3%81%AE%E5%AE%9F%E8%A3%85%E3%81%A8%E8%A9%95%E4%BE%A1-%E5%B1%B1%E7%94%B0-%E8%82%B2%E7%9F%A2/dp/4297143933/ref=sr_1_3_sspa?adgrpid=156801342469&dib=eyJ2IjoiMSJ9.iUa4odO5qgcS966iULz3HtcTc2uqQu0lVWd7hosPI6MA14Y-B61aYYPUS92JRkE7ne1PM9ASU07HNCHOLA7b1AsP5nf8Gh2K4u3cUCYWpGMiXa29adm6yymKO0P-1WR6VnXtocbGKS0d8INl1SPdfa5u1aOdbFvoXbp9E2Sut-FgGxy0C62OoSxk-c-gySknbxhccbn5FJI_YueYoZpgzhHgaRPkZz1fPt5my_oHa6Kl79IQQ_j4gJ8CdY1RLlSIBn-j8TDYLwQsYIgQzSgEe6fwgmFjZlM5VrBO_srMSL4.vLuK9aFlV34yjO3H2Gtu1sQMjDi3V6hnVZJjQABhgfY&dib_tag=se&gad_source=1&hvadid=685864338618&hvdev=c&hvlocphy=9166121&hvnetw=g&hvqmt=e&hvrand=18040040681330066797&hvtargid=kwd-2270348275400&hydadcr=1792_13657028&jp-ad-ap=0&keywords=%E5%A4%A7%E8%A6%8F%E6%A8%A1+%E8%A8%80%E8%AA%9E+%E3%83%A2%E3%83%87%E3%83%AB+%E5%85%A5%E9%96%80&mcid=753301ec7f8030b2bd433cf43109b8ff&qid=1738131408&sr=8-3-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1)\n- [LangChain完全入門　生成AIアプリケーション開発がはかどる大規模言語モデルの操り方](https://www.amazon.co.jp/LangChain%E5%AE%8C%E5%85%A8%E5%85%A5%E9%96%80-%E7%94%9F%E6%88%90AI%E3%82%A2%E3%83%97%E3%83%AA%E3%82%B1%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E9%96%8B%E7%99%BA%E3%81%8C%E3%81%AF%E3%81%8B%E3%81%A9%E3%82%8B%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%93%8D%E3%82%8A%E6%96%B9-%E7%94%B0%E6%9D%91-%E6%82%A0-ebook/dp/B0CL49K74N/ref=sr_1_3?adgrpid=155650325217&dib=eyJ2IjoiMSJ9.cb2DSolyrUa1Vyx5yimjomJ1vXv1THhF84w7hqgn1EWm79jYA-dlRkZFRriUBiDcCN0wWd_8FlJQUoA6PABkQc8jDeAvpRCy2hEvYb2D6jBtphdV55ES3cLn7HRE7rsHByWpT0fGiXMms5XqNDkAr-dYdd51-7TIYV5vpNiYMbvDPzFYWjjsuDWOWashvkcVtj8hymSMKIBIuwNySVfURa7akWg3i8r4CMMLMMrF0Rk.4rI-motw7xKRBs6batXzXS7BUIJgQW0EZ3LCP-EeBmo&dib_tag=se&gad_source=1&hvadid=687716088906&hvdev=c&hvlocphy=9166121&hvnetw=g&hvqmt=e&hvrand=3850225481485846046&hvtargid=kwd-2278701652466&hydadcr=27486_14701136&jp-ad-ap=0&keywords=lang+chain+%E6%9C%AC&mcid=df081cd160633ae2851a77addccc61f5&qid=1738130620&sr=8-3)\n\n**＜Webサイト＞**\n- [生成AIのビジネス活用で注目されるRAG（検索拡張生成）とは？ - 仕組みや活用例、精度向上のノウハウなどを紹介](https://www.dir.co.jp/world/entry/solution/rag)\n- [Retrieval-based LM (RAG system) ざっくり理解する](https://speakerdeck.com/smiyawaki0820/retrieval-based-lm-rag-system-zatukurili-jie-suru)\n\n\n\n\n",
      "coediting": false,
      "comments_count": 0,
      "created_at": "2025-01-29T15:22:17+09:00",
      "group": null,
      "id": "ae9ac4860968389804bd",
      "likes_count": 15,
      "private": false,
      "reactions_count": 0,
      "stocks_count": 12,
      "tags": [
        {
          "name": "Python",
          "versions": []
        },
        {
          "name": "rag",
          "versions": []
        },
        {
          "name": "生成AI",
          "versions": []
        },
        {
          "name": "LangChain",
          "versions": []
        },
        {
          "name": "LLM",
          "versions": []
        }
      ],
      "title": "PDFデータを活用したLangChainでのRAG構築",
      "updated_at": "2025-01-29T15:45:03+09:00",
      "url": "https://qiita.com/camcam/items/ae9ac4860968389804bd",
      "user": {
        "description": null,
        "facebook_id": null,
        "followees_count": 1,
        "followers_count": 1,
        "github_login_name": null,
        "id": "camcam",
        "items_count": 3,
        "linkedin_id": null,
        "location": null,
        "name": "",
        "organization": null,
        "permanent_id": 3985295,
        "profile_image_url": "https://lh3.googleusercontent.com/a/ACg8ocILRUMmau7k-eBfgp20EKLz6gmdz2fAaQ_VAE_N9MnsZQANan4=s96-c",
        "team_only": false,
        "twitter_screen_name": null,
        "website_url": null
      },
      "page_views_count": null,
      "team_membership": null,
      "organization_url_name": null,
      "slide": false,
      "semantic_similarity": 0.7970733046531677,
      "quality_score": 25,
      "python_code_score": 4,
      "python_code_blocks": 5
    },
    {
      "rendered_body": "<p data-sourcepos=\"1:1-1:145\"><a href=\"\" target=\"_blank\"><img width=\"A%\" loading=\"lazy\"></a><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2Ff920c76e-3c56-75b0-ea7d-4ea0fd880818.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=03f025e615f1d82355f53f85b5df6b3e\" target=\"_blank\" rel=\"nofollow noopener\"><img width=\"100%\" src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2Ff920c76e-3c56-75b0-ea7d-4ea0fd880818.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=03f025e615f1d82355f53f85b5df6b3e\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2Ff920c76e-3c56-75b0-ea7d-4ea0fd880818.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=83203a003e7c747952b9fdf054a83194 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/f920c76e-3c56-75b0-ea7d-4ea0fd880818.png\" loading=\"lazy\"></a></p>\n<h1 data-sourcepos=\"3:1-3:14\">\n<span id=\"はじめに\" class=\"fragment\"></span><a href=\"#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\"><i class=\"fa fa-link\"></i></a>はじめに</h1>\n<p data-sourcepos=\"4:1-6:135\">　情報システム部にいると、「OCRを試してみたい」とか「紙の帳票はやめないが、効率化を図りたい」などといろいろな引き合いが舞い込んできます。そのためOCRを小さなプロジェクトやPoCで試すことも多いのですが、文字認識の精度のせいなのか、ほとんどは立ち消えになってしまっています。<br>\n　一方で、最近リリースされた<code>gpt-4o</code>は画像認識が可能であり、OCRよりも精度が高いのではないか？と思い始めました。<br>\n今回は、OCRと<code>gpt-4o</code>(お金がないので正確には<code>gpt-4o-mini</code>）の読み取り精度を確認したいと思います。</p>\n<h1 data-sourcepos=\"8:1-8:8\">\n<span id=\"gpt-4o\" class=\"fragment\"></span><a href=\"#gpt-4o\"><i class=\"fa fa-link\"></i></a>GPT-4o</h1>\n<p data-sourcepos=\"9:1-10:107\">今回紹介するプログラムは、指定した画像を生成AIに渡して、その結果を出力するというプログラムです。<br>\n<code>base64</code>は標準ライブラリなので、改めてインストールする必要は無いようです。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"12:1-50:3\"><div class=\"highlight\"><pre><code><span class=\"kn\">from</span> <span class=\"n\">langchain_openai</span> <span class=\"kn\">import</span> <span class=\"n\">ChatOpenAI</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain_core.prompts</span> <span class=\"kn\">import</span> <span class=\"n\">ChatPromptTemplate</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain_core.prompts.chat</span> <span class=\"kn\">import</span> <span class=\"n\">HumanMessagePromptTemplate</span>\n<span class=\"kn\">import</span> <span class=\"n\">base64</span>\n<span class=\"kn\">from</span> <span class=\"n\">langchain_core.prompts.image</span> <span class=\"kn\">import</span> <span class=\"n\">ImagePromptTemplate</span>\n\n<span class=\"n\">question</span> <span class=\"o\">=</span> <span class=\"sh\">\"\"\"</span><span class=\"s\">\n図の中の文字を答えて\n</span><span class=\"sh\">\"\"\"</span>\n<span class=\"n\">image_path</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">C:</span><span class=\"se\">\\\\</span><span class=\"s\">Users</span><span class=\"se\">\\\\</span><span class=\"s\">ogiki</span><span class=\"se\">\\\\</span><span class=\"s\">Desktop</span><span class=\"se\">\\\\</span><span class=\"s\">data</span><span class=\"se\">\\\\</span><span class=\"s\">大阪ばんざい.jpg</span><span class=\"sh\">\"</span>\n<span class=\"n\">system</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"sh\">\"</span><span class=\"s\">あなたは有能なアシスタントです。ユーザーの問いに回答してください</span><span class=\"sh\">\"</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\">## =================================================================\n</span>\n<span class=\"c1\">#画像ファイルをbase64エンコードする\n</span><span class=\"k\">def</span> <span class=\"nf\">encode_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">):</span>\n  <span class=\"k\">with</span> <span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">rb</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">image_file</span><span class=\"p\">:</span>\n    <span class=\"k\">return</span> <span class=\"n\">base64</span><span class=\"p\">.</span><span class=\"nf\">b64encode</span><span class=\"p\">(</span><span class=\"n\">image_file</span><span class=\"p\">.</span><span class=\"nf\">read</span><span class=\"p\">()).</span><span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">utf-8</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n\n<span class=\"n\">base64_image</span> <span class=\"o\">=</span> <span class=\"nf\">encode_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)</span>\n\n<span class=\"n\">image_template</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"sh\">\"</span><span class=\"s\">image_url</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"sh\">\"</span><span class=\"s\">url</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">data:image/png;base64,</span><span class=\"si\">{</span><span class=\"n\">base64_image</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">}}</span>\n\n<span class=\"n\">chat</span> <span class=\"o\">=</span> <span class=\"nc\">ChatOpenAI</span><span class=\"p\">(</span><span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">model_name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">gpt-4o-mini</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n<span class=\"c1\">#chat = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n</span>\n<span class=\"n\">human_prompt</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">{question}</span><span class=\"sh\">\"</span>\n<span class=\"n\">human_message_template</span> <span class=\"o\">=</span> <span class=\"n\">HumanMessagePromptTemplate</span><span class=\"p\">.</span><span class=\"nf\">from_template</span><span class=\"p\">([</span><span class=\"n\">human_prompt</span><span class=\"p\">,</span> <span class=\"n\">image_template</span><span class=\"p\">])</span>\n\n<span class=\"n\">prompt</span> <span class=\"o\">=</span> <span class=\"n\">ChatPromptTemplate</span><span class=\"p\">.</span><span class=\"nf\">from_messages</span><span class=\"p\">([(</span><span class=\"sh\">\"</span><span class=\"s\">system</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">system</span><span class=\"p\">),</span> <span class=\"n\">human_message_template</span><span class=\"p\">])</span>\n\n<span class=\"n\">chain</span> <span class=\"o\">=</span> <span class=\"n\">prompt</span> <span class=\"o\">|</span> <span class=\"n\">chat</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">chain</span><span class=\"p\">.</span><span class=\"nf\">invoke</span><span class=\"p\">({</span><span class=\"sh\">\"</span><span class=\"s\">question</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">question</span><span class=\"p\">})</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<p data-sourcepos=\"52:1-52:264\">プログラム上に<code>gpt-3.5-turbo</code>が記述されているコード（コメントアウトしている）がありますが、もちろんこれを実行するとエラーが出ました。<code>gpt-3.5-tureb</code>には画像認識の処理はありませんものね。</p>\n<h1 data-sourcepos=\"54:1-54:5\">\n<span id=\"ocr\" class=\"fragment\"></span><a href=\"#ocr\"><i class=\"fa fa-link\"></i></a>OCR</h1>\n<p data-sourcepos=\"55:1-57:72\">次にオープンソースのOCRを利用するためのプログラムを書きます。<br>\n今回は<code>Tesseract</code>というライブラリを使うことにしました。Windowsでのインストールができるという事で、それをPythonのプログラムで操作することにしました。<br>\nインストールは、以下の投稿記事を参考にしました。</p>\n<p data-sourcepos=\"59:1-59:55\"><iframe id=\"qiita-embed-content__449951ee6c1460335eeff15f1825654f\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__449951ee6c1460335eeff15f1825654f\" data-content=\"https%3A%2F%2Fqiita.com%2Fhenjiganai%2Fitems%2F7a5e871f652b32b41a18\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<p data-sourcepos=\"61:1-61:104\"><a href=\"/henjiganai\" class=\"user-mention js-hovercard\" title=\"henjiganai\" data-hovercard-target-type=\"user\" data-hovercard-target-name=\"henjiganai\">@henjiganai</a>さん、わかりやすい記事を書いていただきありがとうございました。</p>\n<p data-sourcepos=\"63:1-63:98\">また、最初に<code>Tesseract</code>を理解するために、以下の記事も参考にしました。</p>\n<p data-sourcepos=\"65:1-65:51\"><iframe id=\"qiita-embed-content__cb2cf61b14ce66aa07ccb1a73c6c8829\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__cb2cf61b14ce66aa07ccb1a73c6c8829\" data-content=\"https%3A%2F%2Fqiita.com%2Fku_a_i%2Fitems%2F93fdbd75edacb34ec610\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<p data-sourcepos=\"67:1-67:99\"><a href=\"/ku_a_i\" class=\"user-mention js-hovercard\" title=\"ku_a_i\" data-hovercard-target-type=\"user\" data-hovercard-target-name=\"ku_a_i\">@ku_a_i</a>(ku_a_i)さんもありがとうございます。非常にわかりやすかったです。</p>\n<p data-sourcepos=\"69:1-70:103\">インストールはうまくいったのですが、その後Pythonのプログラムを実行しようとするとエラーが出ました。<br>\nいろいろ確認すると、<code>tesseract.exe</code>のパスがうまく通っていないようです。</p>\n<p data-sourcepos=\"72:1-72:43\"><iframe id=\"qiita-embed-content__e79c9a69d2deb3512628fb33cb8e9818\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__e79c9a69d2deb3512628fb33cb8e9818\" data-content=\"https%3A%2F%2Fnote.com%2Fmurasamejo%2Fn%2Fnb314171faea2\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<p data-sourcepos=\"74:1-75:321\">上記の投稿記事を基に（kinopi さんありがとうございます）、Pythonライブラリの中の<code>pytesseract.py</code>の中にある<code>tesseract_cmd</code>の値を自分のパソコンのパスに置き換えて、<code>pytesseract.py</code>を実行ファイルと同一ディレクトリに置くことでプログラムをうまく動かすことができました。一旦はこの形で進めていきます。<br>\n　以下に<code>pytesseract.py</code>を転記します。もし私と同じ状況に陥った場合、これをそのままコピーして31行目の<code>tesseract_com</code>の部分を皆様のインストールされた場所に変更し、実行ファイルと同一ディレクトリに置いていただけると実行できます。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"77:1-659:3\">\n<div class=\"code-lang\"><span class=\"bold\">pytesseract.py</span></div>\n<div class=\"highlight\"><pre><code><span class=\"c1\">#!/usr/bin/env python\n</span><span class=\"kn\">import</span> <span class=\"n\">re</span>\n<span class=\"kn\">import</span> <span class=\"n\">shlex</span>\n<span class=\"kn\">import</span> <span class=\"n\">string</span>\n<span class=\"kn\">import</span> <span class=\"n\">subprocess</span>\n<span class=\"kn\">import</span> <span class=\"n\">sys</span>\n<span class=\"kn\">from</span> <span class=\"n\">contextlib</span> <span class=\"kn\">import</span> <span class=\"n\">contextmanager</span>\n<span class=\"kn\">from</span> <span class=\"n\">csv</span> <span class=\"kn\">import</span> <span class=\"n\">QUOTE_NONE</span>\n<span class=\"kn\">from</span> <span class=\"n\">errno</span> <span class=\"kn\">import</span> <span class=\"n\">ENOENT</span>\n<span class=\"kn\">from</span> <span class=\"n\">functools</span> <span class=\"kn\">import</span> <span class=\"n\">wraps</span>\n<span class=\"kn\">from</span> <span class=\"n\">glob</span> <span class=\"kn\">import</span> <span class=\"n\">iglob</span>\n<span class=\"kn\">from</span> <span class=\"n\">io</span> <span class=\"kn\">import</span> <span class=\"n\">BytesIO</span>\n<span class=\"kn\">from</span> <span class=\"n\">os</span> <span class=\"kn\">import</span> <span class=\"n\">environ</span>\n<span class=\"kn\">from</span> <span class=\"n\">os</span> <span class=\"kn\">import</span> <span class=\"n\">extsep</span>\n<span class=\"kn\">from</span> <span class=\"n\">os</span> <span class=\"kn\">import</span> <span class=\"n\">linesep</span>\n<span class=\"kn\">from</span> <span class=\"n\">os</span> <span class=\"kn\">import</span> <span class=\"n\">remove</span>\n<span class=\"kn\">from</span> <span class=\"n\">os.path</span> <span class=\"kn\">import</span> <span class=\"n\">normcase</span>\n<span class=\"kn\">from</span> <span class=\"n\">os.path</span> <span class=\"kn\">import</span> <span class=\"n\">normpath</span>\n<span class=\"kn\">from</span> <span class=\"n\">os.path</span> <span class=\"kn\">import</span> <span class=\"n\">realpath</span>\n<span class=\"kn\">from</span> <span class=\"n\">pkgutil</span> <span class=\"kn\">import</span> <span class=\"n\">find_loader</span>\n<span class=\"kn\">from</span> <span class=\"n\">tempfile</span> <span class=\"kn\">import</span> <span class=\"n\">NamedTemporaryFile</span>\n<span class=\"kn\">from</span> <span class=\"n\">time</span> <span class=\"kn\">import</span> <span class=\"n\">sleep</span>\n\n<span class=\"kn\">from</span> <span class=\"n\">packaging.version</span> <span class=\"kn\">import</span> <span class=\"n\">InvalidVersion</span>\n<span class=\"kn\">from</span> <span class=\"n\">packaging.version</span> <span class=\"kn\">import</span> <span class=\"n\">parse</span>\n<span class=\"kn\">from</span> <span class=\"n\">packaging.version</span> <span class=\"kn\">import</span> <span class=\"n\">Version</span>\n<span class=\"kn\">from</span> <span class=\"n\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n\n\n<span class=\"c1\">#tesseract_cmd = 'tesseract'\n</span><span class=\"n\">tesseract_cmd</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">C:</span><span class=\"se\">\\\\</span><span class=\"s\">Program Files</span><span class=\"se\">\\\\</span><span class=\"s\">Tesseract-OCR</span><span class=\"se\">\\\\</span><span class=\"s\">tesseract.exe</span><span class=\"sh\">'</span>\n\n<span class=\"n\">numpy_installed</span> <span class=\"o\">=</span> <span class=\"nf\">find_loader</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">numpy</span><span class=\"sh\">'</span><span class=\"p\">)</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"bp\">None</span>\n<span class=\"k\">if</span> <span class=\"n\">numpy_installed</span><span class=\"p\">:</span>\n    <span class=\"kn\">from</span> <span class=\"n\">numpy</span> <span class=\"kn\">import</span> <span class=\"n\">ndarray</span>\n\n<span class=\"n\">pandas_installed</span> <span class=\"o\">=</span> <span class=\"nf\">find_loader</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">pandas</span><span class=\"sh\">'</span><span class=\"p\">)</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"bp\">None</span>\n<span class=\"k\">if</span> <span class=\"n\">pandas_installed</span><span class=\"p\">:</span>\n    <span class=\"kn\">import</span> <span class=\"n\">pandas</span> <span class=\"k\">as</span> <span class=\"n\">pd</span>\n\n<span class=\"n\">DEFAULT_ENCODING</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">utf-8</span><span class=\"sh\">'</span>\n<span class=\"n\">LANG_PATTERN</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"p\">.</span><span class=\"nf\">compile</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">^[a-z_]+$</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n<span class=\"n\">RGB_MODE</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">RGB</span><span class=\"sh\">'</span>\n<span class=\"n\">SUPPORTED_FORMATS</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"sh\">'</span><span class=\"s\">JPEG</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"sh\">'</span><span class=\"s\">JPEG2000</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"sh\">'</span><span class=\"s\">PNG</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"sh\">'</span><span class=\"s\">PBM</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"sh\">'</span><span class=\"s\">PGM</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"sh\">'</span><span class=\"s\">PPM</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"sh\">'</span><span class=\"s\">TIFF</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"sh\">'</span><span class=\"s\">BMP</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"sh\">'</span><span class=\"s\">GIF</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"sh\">'</span><span class=\"s\">WEBP</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">OSD_KEYS</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"sh\">'</span><span class=\"s\">Page number</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">page_num</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">),</span>\n    <span class=\"sh\">'</span><span class=\"s\">Orientation in degrees</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">orientation</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">),</span>\n    <span class=\"sh\">'</span><span class=\"s\">Rotate</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">rotate</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">),</span>\n    <span class=\"sh\">'</span><span class=\"s\">Orientation confidence</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">orientation_conf</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">),</span>\n    <span class=\"sh\">'</span><span class=\"s\">Script</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">script</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">),</span>\n    <span class=\"sh\">'</span><span class=\"s\">Script confidence</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">script_conf</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">),</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">TESSERACT_MIN_VERSION</span> <span class=\"o\">=</span> <span class=\"nc\">Version</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">3.05</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n<span class=\"n\">TESSERACT_ALTO_VERSION</span> <span class=\"o\">=</span> <span class=\"nc\">Version</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">4.1.0</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">Output</span><span class=\"p\">:</span>\n    <span class=\"n\">BYTES</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">bytes</span><span class=\"sh\">'</span>\n    <span class=\"n\">DATAFRAME</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">data.frame</span><span class=\"sh\">'</span>\n    <span class=\"n\">DICT</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">dict</span><span class=\"sh\">'</span>\n    <span class=\"n\">STRING</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">string</span><span class=\"sh\">'</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">PandasNotSupported</span><span class=\"p\">(</span><span class=\"nb\">EnvironmentError</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"n\">self</span><span class=\"p\">):</span>\n        <span class=\"nf\">super</span><span class=\"p\">().</span><span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">Missing pandas package</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">TesseractError</span><span class=\"p\">(</span><span class=\"nb\">RuntimeError</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"n\">self</span><span class=\"p\">,</span> <span class=\"n\">status</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"p\">):</span>\n        <span class=\"n\">self</span><span class=\"p\">.</span><span class=\"n\">status</span> <span class=\"o\">=</span> <span class=\"n\">status</span>\n        <span class=\"n\">self</span><span class=\"p\">.</span><span class=\"n\">message</span> <span class=\"o\">=</span> <span class=\"n\">message</span>\n        <span class=\"n\">self</span><span class=\"p\">.</span><span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">status</span><span class=\"p\">,</span> <span class=\"n\">message</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">TesseractNotFoundError</span><span class=\"p\">(</span><span class=\"nb\">EnvironmentError</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"n\">self</span><span class=\"p\">):</span>\n        <span class=\"nf\">super</span><span class=\"p\">().</span><span class=\"nf\">__init__</span><span class=\"p\">(</span>\n            <span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"si\">{</span><span class=\"n\">tesseract_cmd</span><span class=\"si\">}</span><span class=\"s\"> is not installed or it</span><span class=\"sh\">'</span><span class=\"s\">s not in your PATH.</span><span class=\"sh\">\"</span>\n            <span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"s\"> See README file for more information.</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">TSVNotSupported</span><span class=\"p\">(</span><span class=\"nb\">EnvironmentError</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"n\">self</span><span class=\"p\">):</span>\n        <span class=\"nf\">super</span><span class=\"p\">().</span><span class=\"nf\">__init__</span><span class=\"p\">(</span>\n            <span class=\"sh\">'</span><span class=\"s\">TSV output not supported. Tesseract &gt;= 3.05 required</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">ALTONotSupported</span><span class=\"p\">(</span><span class=\"nb\">EnvironmentError</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"n\">self</span><span class=\"p\">):</span>\n        <span class=\"nf\">super</span><span class=\"p\">().</span><span class=\"nf\">__init__</span><span class=\"p\">(</span>\n            <span class=\"sh\">'</span><span class=\"s\">ALTO output not supported. Tesseract &gt;= 4.1.0 required</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">kill</span><span class=\"p\">(</span><span class=\"n\">process</span><span class=\"p\">,</span> <span class=\"n\">code</span><span class=\"p\">):</span>\n    <span class=\"n\">process</span><span class=\"p\">.</span><span class=\"nf\">terminate</span><span class=\"p\">()</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">process</span><span class=\"p\">.</span><span class=\"nf\">wait</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"k\">except</span> <span class=\"nb\">TypeError</span><span class=\"p\">:</span>  <span class=\"c1\"># python2 Popen.wait(1) fallback\n</span>        <span class=\"nf\">sleep</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"k\">except</span> <span class=\"nb\">Exception</span><span class=\"p\">:</span>  <span class=\"c1\"># python3 subprocess.TimeoutExpired\n</span>        <span class=\"k\">pass</span>\n    <span class=\"k\">finally</span><span class=\"p\">:</span>\n        <span class=\"n\">process</span><span class=\"p\">.</span><span class=\"nf\">kill</span><span class=\"p\">()</span>\n        <span class=\"n\">process</span><span class=\"p\">.</span><span class=\"n\">returncode</span> <span class=\"o\">=</span> <span class=\"n\">code</span>\n\n\n<span class=\"nd\">@contextmanager</span>\n<span class=\"k\">def</span> <span class=\"nf\">timeout_manager</span><span class=\"p\">(</span><span class=\"n\">proc</span><span class=\"p\">,</span> <span class=\"n\">seconds</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">seconds</span><span class=\"p\">:</span>\n            <span class=\"k\">yield</span> <span class=\"n\">proc</span><span class=\"p\">.</span><span class=\"nf\">communicate</span><span class=\"p\">()[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n            <span class=\"k\">return</span>\n\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">error_string</span> <span class=\"o\">=</span> <span class=\"n\">proc</span><span class=\"p\">.</span><span class=\"nf\">communicate</span><span class=\"p\">(</span><span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"n\">seconds</span><span class=\"p\">)</span>\n            <span class=\"k\">yield</span> <span class=\"n\">error_string</span>\n        <span class=\"k\">except</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">TimeoutExpired</span><span class=\"p\">:</span>\n            <span class=\"nf\">kill</span><span class=\"p\">(</span><span class=\"n\">proc</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n            <span class=\"k\">raise</span> <span class=\"nc\">RuntimeError</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">Tesseract process timeout</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n    <span class=\"k\">finally</span><span class=\"p\">:</span>\n        <span class=\"n\">proc</span><span class=\"p\">.</span><span class=\"n\">stdin</span><span class=\"p\">.</span><span class=\"nf\">close</span><span class=\"p\">()</span>\n        <span class=\"n\">proc</span><span class=\"p\">.</span><span class=\"n\">stdout</span><span class=\"p\">.</span><span class=\"nf\">close</span><span class=\"p\">()</span>\n        <span class=\"n\">proc</span><span class=\"p\">.</span><span class=\"n\">stderr</span><span class=\"p\">.</span><span class=\"nf\">close</span><span class=\"p\">()</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">run_once</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">):</span>\n    <span class=\"nd\">@wraps</span><span class=\"p\">(</span><span class=\"n\">func</span><span class=\"p\">)</span>\n    <span class=\"k\">def</span> <span class=\"nf\">wrapper</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"k\">if</span> <span class=\"n\">wrapper</span><span class=\"p\">.</span><span class=\"n\">_result</span> <span class=\"ow\">is</span> <span class=\"n\">wrapper</span><span class=\"p\">:</span>\n            <span class=\"n\">wrapper</span><span class=\"p\">.</span><span class=\"n\">_result</span> <span class=\"o\">=</span> <span class=\"nf\">func</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">wrapper</span><span class=\"p\">.</span><span class=\"n\">_result</span>\n\n    <span class=\"n\">wrapper</span><span class=\"p\">.</span><span class=\"n\">_result</span> <span class=\"o\">=</span> <span class=\"n\">wrapper</span>\n    <span class=\"k\">return</span> <span class=\"n\">wrapper</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">get_errors</span><span class=\"p\">(</span><span class=\"n\">error_string</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"sh\">'</span><span class=\"s\"> </span><span class=\"sh\">'</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">(</span>\n        <span class=\"n\">line</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">error_string</span><span class=\"p\">.</span><span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"n\">DEFAULT_ENCODING</span><span class=\"p\">).</span><span class=\"nf\">splitlines</span><span class=\"p\">()</span>\n    <span class=\"p\">).</span><span class=\"nf\">strip</span><span class=\"p\">()</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">cleanup</span><span class=\"p\">(</span><span class=\"n\">temp_name</span><span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">Tries to remove temp files by filename wildcard path.</span><span class=\"sh\">\"\"\"</span>\n    <span class=\"k\">for</span> <span class=\"n\">filename</span> <span class=\"ow\">in</span> <span class=\"nf\">iglob</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"si\">{</span><span class=\"n\">temp_name</span><span class=\"si\">}</span><span class=\"s\">*</span><span class=\"sh\">'</span> <span class=\"k\">if</span> <span class=\"n\">temp_name</span> <span class=\"k\">else</span> <span class=\"n\">temp_name</span><span class=\"p\">):</span>\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"nf\">remove</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">)</span>\n        <span class=\"k\">except</span> <span class=\"nb\">OSError</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">errno</span> <span class=\"o\">!=</span> <span class=\"n\">ENOENT</span><span class=\"p\">:</span>\n                <span class=\"k\">raise</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">prepare</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">numpy_installed</span> <span class=\"ow\">and</span> <span class=\"nf\">isinstance</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">ndarray</span><span class=\"p\">):</span>\n        <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">fromarray</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"nf\">isinstance</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"n\">Image</span><span class=\"p\">):</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">TypeError</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">Unsupported image object</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">extension</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">PNG</span><span class=\"sh\">'</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nb\">format</span> <span class=\"k\">else</span> <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nb\">format</span>\n    <span class=\"k\">if</span> <span class=\"n\">extension</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">SUPPORTED_FORMATS</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">TypeError</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">Unsupported image format/type</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"sh\">'</span><span class=\"s\">A</span><span class=\"sh\">'</span> <span class=\"ow\">in</span> <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">getbands</span><span class=\"p\">():</span>\n        <span class=\"c1\"># discard and replace the alpha channel with white background\n</span>        <span class=\"n\">background</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">new</span><span class=\"p\">(</span><span class=\"n\">RGB_MODE</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"mi\">255</span><span class=\"p\">))</span>\n        <span class=\"n\">background</span><span class=\"p\">.</span><span class=\"nf\">paste</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">getchannel</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">A</span><span class=\"sh\">'</span><span class=\"p\">))</span>\n        <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">background</span>\n\n    <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nb\">format</span> <span class=\"o\">=</span> <span class=\"n\">extension</span>\n    <span class=\"k\">return</span> <span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">extension</span>\n\n\n<span class=\"nd\">@contextmanager</span>\n<span class=\"k\">def</span> <span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">):</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"k\">with</span> <span class=\"nc\">NamedTemporaryFile</span><span class=\"p\">(</span><span class=\"n\">prefix</span><span class=\"o\">=</span><span class=\"sh\">'</span><span class=\"s\">tess_</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">delete</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"nf\">isinstance</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"nb\">str</span><span class=\"p\">):</span>\n                <span class=\"k\">yield</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"nf\">realpath</span><span class=\"p\">(</span><span class=\"nf\">normpath</span><span class=\"p\">(</span><span class=\"nf\">normcase</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)))</span>\n                <span class=\"k\">return</span>\n            <span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">extension</span> <span class=\"o\">=</span> <span class=\"nf\">prepare</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\n            <span class=\"n\">input_file_name</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"si\">{</span><span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"si\">}</span><span class=\"s\">_input</span><span class=\"si\">{</span><span class=\"n\">extsep</span><span class=\"si\">}{</span><span class=\"n\">extension</span><span class=\"si\">}</span><span class=\"sh\">'</span>\n            <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">input_file_name</span><span class=\"p\">,</span> <span class=\"nb\">format</span><span class=\"o\">=</span><span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nb\">format</span><span class=\"p\">)</span>\n            <span class=\"k\">yield</span> <span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">input_file_name</span>\n    <span class=\"k\">finally</span><span class=\"p\">:</span>\n        <span class=\"nf\">cleanup</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">.</span><span class=\"n\">name</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">subprocess_args</span><span class=\"p\">(</span><span class=\"n\">include_stdout</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">):</span>\n    <span class=\"c1\"># See https://github.com/pyinstaller/pyinstaller/wiki/Recipe-subprocess\n</span>    <span class=\"c1\"># for reference and comments.\n</span>\n    <span class=\"n\">kwargs</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"sh\">'</span><span class=\"s\">stdin</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">PIPE</span><span class=\"p\">,</span>\n        <span class=\"sh\">'</span><span class=\"s\">stderr</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">PIPE</span><span class=\"p\">,</span>\n        <span class=\"sh\">'</span><span class=\"s\">startupinfo</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"bp\">None</span><span class=\"p\">,</span>\n        <span class=\"sh\">'</span><span class=\"s\">env</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">environ</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">if</span> <span class=\"nf\">hasattr</span><span class=\"p\">(</span><span class=\"n\">subprocess</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">STARTUPINFO</span><span class=\"sh\">'</span><span class=\"p\">):</span>\n        <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">startupinfo</span><span class=\"sh\">'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"nc\">STARTUPINFO</span><span class=\"p\">()</span>\n        <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">startupinfo</span><span class=\"sh\">'</span><span class=\"p\">].</span><span class=\"n\">dwFlags</span> <span class=\"o\">|=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">STARTF_USESHOWWINDOW</span>\n        <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">startupinfo</span><span class=\"sh\">'</span><span class=\"p\">].</span><span class=\"n\">wShowWindow</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">SW_HIDE</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">include_stdout</span><span class=\"p\">:</span>\n        <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">stdout</span><span class=\"sh\">'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">PIPE</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">stdout</span><span class=\"sh\">'</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">DEVNULL</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">kwargs</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">run_tesseract</span><span class=\"p\">(</span>\n    <span class=\"n\">input_filename</span><span class=\"p\">,</span>\n    <span class=\"n\">output_filename_base</span><span class=\"p\">,</span>\n    <span class=\"n\">extension</span><span class=\"p\">,</span>\n    <span class=\"n\">lang</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">,</span>\n    <span class=\"n\">nice</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">):</span>\n    <span class=\"n\">cmd_args</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">platform</span><span class=\"p\">.</span><span class=\"nf\">startswith</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">win32</span><span class=\"sh\">'</span><span class=\"p\">)</span> <span class=\"ow\">and</span> <span class=\"n\">nice</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">cmd_args</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">nice</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">-n</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"n\">nice</span><span class=\"p\">))</span>\n\n    <span class=\"n\">cmd_args</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"n\">tesseract_cmd</span><span class=\"p\">,</span> <span class=\"n\">input_filename</span><span class=\"p\">,</span> <span class=\"n\">output_filename_base</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">lang</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">cmd_args</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">-l</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">config</span><span class=\"p\">:</span>\n        <span class=\"n\">cmd_args</span> <span class=\"o\">+=</span> <span class=\"n\">shlex</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">extension</span> <span class=\"ow\">and</span> <span class=\"n\">extension</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"p\">{</span><span class=\"sh\">'</span><span class=\"s\">box</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">osd</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">tsv</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">xml</span><span class=\"sh\">'</span><span class=\"p\">}:</span>\n        <span class=\"n\">cmd_args</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">extension</span><span class=\"p\">)</span>\n\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">proc</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"nc\">Popen</span><span class=\"p\">(</span><span class=\"n\">cmd_args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"nf\">subprocess_args</span><span class=\"p\">())</span>\n    <span class=\"k\">except</span> <span class=\"nb\">OSError</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">e</span><span class=\"p\">.</span><span class=\"n\">errno</span> <span class=\"o\">!=</span> <span class=\"n\">ENOENT</span><span class=\"p\">:</span>\n            <span class=\"k\">raise</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"k\">raise</span> <span class=\"nc\">TesseractNotFoundError</span><span class=\"p\">()</span>\n\n    <span class=\"k\">with</span> <span class=\"nf\">timeout_manager</span><span class=\"p\">(</span><span class=\"n\">proc</span><span class=\"p\">,</span> <span class=\"n\">timeout</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">error_string</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">proc</span><span class=\"p\">.</span><span class=\"n\">returncode</span><span class=\"p\">:</span>\n            <span class=\"k\">raise</span> <span class=\"nc\">TesseractError</span><span class=\"p\">(</span><span class=\"n\">proc</span><span class=\"p\">.</span><span class=\"n\">returncode</span><span class=\"p\">,</span> <span class=\"nf\">get_errors</span><span class=\"p\">(</span><span class=\"n\">error_string</span><span class=\"p\">))</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span>\n    <span class=\"n\">image</span><span class=\"p\">,</span>\n    <span class=\"n\">extension</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">,</span>\n    <span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">,</span>\n    <span class=\"n\">nice</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">return_bytes</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span>\n<span class=\"p\">):</span>\n\n    <span class=\"k\">with</span> <span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span> <span class=\"nf\">as </span><span class=\"p\">(</span><span class=\"n\">temp_name</span><span class=\"p\">,</span> <span class=\"n\">input_filename</span><span class=\"p\">):</span>\n        <span class=\"n\">kwargs</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n            <span class=\"sh\">'</span><span class=\"s\">input_filename</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">input_filename</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">output_filename_base</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">temp_name</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">extension</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">extension</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">lang</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">lang</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">config</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">config</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">nice</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">nice</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">timeout</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">timeout</span><span class=\"p\">,</span>\n        <span class=\"p\">}</span>\n\n        <span class=\"nf\">run_tesseract</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n        <span class=\"n\">filename</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"si\">{</span><span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">output_filename_base</span><span class=\"sh\">'</span><span class=\"p\">]</span><span class=\"si\">}{</span><span class=\"n\">extsep</span><span class=\"si\">}{</span><span class=\"n\">extension</span><span class=\"si\">}</span><span class=\"sh\">\"</span>\n        <span class=\"k\">with</span> <span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">rb</span><span class=\"sh\">'</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">output_file</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">return_bytes</span><span class=\"p\">:</span>\n                <span class=\"k\">return</span> <span class=\"n\">output_file</span><span class=\"p\">.</span><span class=\"nf\">read</span><span class=\"p\">()</span>\n            <span class=\"k\">return</span> <span class=\"n\">output_file</span><span class=\"p\">.</span><span class=\"nf\">read</span><span class=\"p\">().</span><span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"n\">DEFAULT_ENCODING</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">file_to_dict</span><span class=\"p\">(</span><span class=\"n\">tsv</span><span class=\"p\">,</span> <span class=\"n\">cell_delimiter</span><span class=\"p\">,</span> <span class=\"n\">str_col_idx</span><span class=\"p\">):</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">row</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"n\">cell_delimiter</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">tsv</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">().</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"se\">\\n</span><span class=\"sh\">'</span><span class=\"p\">)]</span>\n    <span class=\"k\">if</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">rows</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"mi\">2</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">result</span>\n\n    <span class=\"n\">header</span> <span class=\"o\">=</span> <span class=\"n\">rows</span><span class=\"p\">.</span><span class=\"nf\">pop</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">length</span> <span class=\"o\">=</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">header</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">rows</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">&lt;</span> <span class=\"n\">length</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Fixes bug that occurs when last text string in TSV is null, and\n</span>        <span class=\"c1\"># last row is missing a final cell in TSV file\n</span>        <span class=\"n\">rows</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">].</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"sh\">''</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">str_col_idx</span> <span class=\"o\">&lt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">str_col_idx</span> <span class=\"o\">+=</span> <span class=\"n\">length</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">head</span> <span class=\"ow\">in</span> <span class=\"nf\">enumerate</span><span class=\"p\">(</span><span class=\"n\">header</span><span class=\"p\">):</span>\n        <span class=\"n\">result</span><span class=\"p\">[</span><span class=\"n\">head</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nf\">list</span><span class=\"p\">()</span>\n        <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">rows</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">)</span> <span class=\"o\">&lt;=</span> <span class=\"n\">i</span><span class=\"p\">:</span>\n                <span class=\"k\">continue</span>\n\n            <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">!=</span> <span class=\"n\">str_col_idx</span><span class=\"p\">:</span>\n                <span class=\"k\">try</span><span class=\"p\">:</span>\n                    <span class=\"n\">val</span> <span class=\"o\">=</span> <span class=\"nf\">int</span><span class=\"p\">(</span><span class=\"nf\">float</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]))</span>\n                <span class=\"k\">except</span> <span class=\"nb\">ValueError</span><span class=\"p\">:</span>\n                    <span class=\"n\">val</span> <span class=\"o\">=</span> <span class=\"n\">row</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"n\">val</span> <span class=\"o\">=</span> <span class=\"n\">row</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n\n            <span class=\"n\">result</span><span class=\"p\">[</span><span class=\"n\">head</span><span class=\"p\">].</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">val</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">result</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">is_valid</span><span class=\"p\">(</span><span class=\"n\">val</span><span class=\"p\">,</span> <span class=\"n\">_type</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">_type</span> <span class=\"ow\">is</span> <span class=\"nb\">int</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">val</span><span class=\"p\">.</span><span class=\"nf\">isdigit</span><span class=\"p\">()</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">_type</span> <span class=\"ow\">is</span> <span class=\"nb\">float</span><span class=\"p\">:</span>\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"nf\">float</span><span class=\"p\">(</span><span class=\"n\">val</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"bp\">True</span>\n        <span class=\"k\">except</span> <span class=\"nb\">ValueError</span><span class=\"p\">:</span>\n            <span class=\"k\">return</span> <span class=\"bp\">False</span>\n\n    <span class=\"k\">return</span> <span class=\"bp\">True</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">osd_to_dict</span><span class=\"p\">(</span><span class=\"n\">osd</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"n\">OSD_KEYS</span><span class=\"p\">[</span><span class=\"n\">kv</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]][</span><span class=\"mi\">0</span><span class=\"p\">]:</span> <span class=\"n\">OSD_KEYS</span><span class=\"p\">[</span><span class=\"n\">kv</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]][</span><span class=\"mi\">1</span><span class=\"p\">](</span><span class=\"n\">kv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n        <span class=\"k\">for</span> <span class=\"n\">kv</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">: </span><span class=\"sh\">'</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">osd</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"se\">\\n</span><span class=\"sh\">'</span><span class=\"p\">))</span>\n        <span class=\"k\">if</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">kv</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">2</span> <span class=\"ow\">and</span> <span class=\"nf\">is_valid</span><span class=\"p\">(</span><span class=\"n\">kv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">OSD_KEYS</span><span class=\"p\">[</span><span class=\"n\">kv</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]][</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n    <span class=\"p\">}</span>\n\n\n<span class=\"nd\">@run_once</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_languages</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">):</span>\n    <span class=\"n\">cmd_args</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">tesseract_cmd</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">--list-langs</span><span class=\"sh\">'</span><span class=\"p\">]</span>\n    <span class=\"k\">if</span> <span class=\"n\">config</span><span class=\"p\">:</span>\n        <span class=\"n\">cmd_args</span> <span class=\"o\">+=</span> <span class=\"n\">shlex</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)</span>\n\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"nf\">run</span><span class=\"p\">(</span>\n            <span class=\"n\">cmd_args</span><span class=\"p\">,</span>\n            <span class=\"n\">stdout</span><span class=\"o\">=</span><span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">PIPE</span><span class=\"p\">,</span>\n            <span class=\"n\">stderr</span><span class=\"o\">=</span><span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">STDOUT</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n    <span class=\"k\">except</span> <span class=\"nb\">OSError</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">TesseractNotFoundError</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># tesseract 3.x\n</span>    <span class=\"k\">if</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">returncode</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">TesseractNotFoundError</span><span class=\"p\">()</span>\n\n    <span class=\"n\">languages</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">if</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">stdout</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">stdout</span><span class=\"p\">.</span><span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"n\">DEFAULT_ENCODING</span><span class=\"p\">).</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"n\">linesep</span><span class=\"p\">):</span>\n            <span class=\"n\">lang</span> <span class=\"o\">=</span> <span class=\"n\">line</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">()</span>\n            <span class=\"k\">if</span> <span class=\"n\">LANG_PATTERN</span><span class=\"p\">.</span><span class=\"nf\">match</span><span class=\"p\">(</span><span class=\"n\">lang</span><span class=\"p\">):</span>\n                <span class=\"n\">languages</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">lang</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">languages</span>\n\n\n<span class=\"nd\">@run_once</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_tesseract_version</span><span class=\"p\">():</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Returns Version object of the Tesseract version\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"nf\">check_output</span><span class=\"p\">(</span>\n            <span class=\"p\">[</span><span class=\"n\">tesseract_cmd</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">--version</span><span class=\"sh\">'</span><span class=\"p\">],</span>\n            <span class=\"n\">stderr</span><span class=\"o\">=</span><span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">STDOUT</span><span class=\"p\">,</span>\n            <span class=\"n\">env</span><span class=\"o\">=</span><span class=\"n\">environ</span><span class=\"p\">,</span>\n            <span class=\"n\">stdin</span><span class=\"o\">=</span><span class=\"n\">subprocess</span><span class=\"p\">.</span><span class=\"n\">DEVNULL</span><span class=\"p\">,</span>\n        <span class=\"p\">)</span>\n    <span class=\"k\">except</span> <span class=\"nb\">OSError</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">TesseractNotFoundError</span><span class=\"p\">()</span>\n\n    <span class=\"n\">raw_version</span> <span class=\"o\">=</span> <span class=\"n\">output</span><span class=\"p\">.</span><span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"n\">DEFAULT_ENCODING</span><span class=\"p\">)</span>\n    <span class=\"n\">str_version</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">raw_version</span><span class=\"p\">.</span><span class=\"nf\">lstrip</span><span class=\"p\">(</span><span class=\"n\">string</span><span class=\"p\">.</span><span class=\"n\">printable</span><span class=\"p\">[</span><span class=\"mi\">10</span><span class=\"p\">:]).</span><span class=\"nf\">partition</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\"> </span><span class=\"sh\">'</span><span class=\"p\">)</span>\n    <span class=\"n\">str_version</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">str_version</span><span class=\"p\">.</span><span class=\"nf\">partition</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">-</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">version</span> <span class=\"o\">=</span> <span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">str_version</span><span class=\"p\">)</span>\n        <span class=\"k\">assert</span> <span class=\"n\">version</span> <span class=\"o\">&gt;=</span> <span class=\"n\">TESSERACT_MIN_VERSION</span>\n    <span class=\"nf\">except </span><span class=\"p\">(</span><span class=\"nb\">AssertionError</span><span class=\"p\">,</span> <span class=\"n\">InvalidVersion</span><span class=\"p\">):</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">SystemExit</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"s\">Invalid tesseract version: </span><span class=\"sh\">\"</span><span class=\"si\">{</span><span class=\"n\">raw_version</span><span class=\"si\">}</span><span class=\"sh\">\"'</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">version</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">image_to_string</span><span class=\"p\">(</span>\n    <span class=\"n\">image</span><span class=\"p\">,</span>\n    <span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">,</span>\n    <span class=\"n\">nice</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">output_type</span><span class=\"o\">=</span><span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">STRING</span><span class=\"p\">,</span>\n    <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Returns the result of a Tesseract OCR run on the provided image to string\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">txt</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">nice</span><span class=\"p\">,</span> <span class=\"n\">timeout</span><span class=\"p\">]</span>\n\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">BYTES</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">args</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"bp\">True</span><span class=\"p\">])),</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">DICT</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"p\">{</span><span class=\"sh\">'</span><span class=\"s\">text</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">)},</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">STRING</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">),</span>\n    <span class=\"p\">}[</span><span class=\"n\">output_type</span><span class=\"p\">]()</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">image_to_pdf_or_hocr</span><span class=\"p\">(</span>\n    <span class=\"n\">image</span><span class=\"p\">,</span>\n    <span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">,</span>\n    <span class=\"n\">nice</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">extension</span><span class=\"o\">=</span><span class=\"sh\">'</span><span class=\"s\">pdf</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Returns the result of a Tesseract OCR run on the provided image to pdf/hocr\n    </span><span class=\"sh\">\"\"\"</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">extension</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"p\">{</span><span class=\"sh\">'</span><span class=\"s\">pdf</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">hocr</span><span class=\"sh\">'</span><span class=\"p\">}:</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">ValueError</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"s\">Unsupported extension: </span><span class=\"si\">{</span><span class=\"n\">extension</span><span class=\"si\">}</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n    <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">extension</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">nice</span><span class=\"p\">,</span> <span class=\"n\">timeout</span><span class=\"p\">,</span> <span class=\"bp\">True</span><span class=\"p\">]</span>\n\n    <span class=\"k\">return</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">image_to_alto_xml</span><span class=\"p\">(</span>\n    <span class=\"n\">image</span><span class=\"p\">,</span>\n    <span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">,</span>\n    <span class=\"n\">nice</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Returns the result of a Tesseract OCR run on the provided image to ALTO XML\n    </span><span class=\"sh\">\"\"\"</span>\n\n    <span class=\"k\">if</span> <span class=\"nf\">get_tesseract_version</span><span class=\"p\">()</span> <span class=\"o\">&lt;</span> <span class=\"n\">TESSERACT_ALTO_VERSION</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">ALTONotSupported</span><span class=\"p\">()</span>\n\n    <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"s\">-c tessedit_create_alto=1 </span><span class=\"si\">{</span><span class=\"n\">config</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">()</span><span class=\"si\">}</span><span class=\"sh\">'</span>\n    <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">xml</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">nice</span><span class=\"p\">,</span> <span class=\"n\">timeout</span><span class=\"p\">,</span> <span class=\"bp\">True</span><span class=\"p\">]</span>\n\n    <span class=\"k\">return</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">image_to_boxes</span><span class=\"p\">(</span>\n    <span class=\"n\">image</span><span class=\"p\">,</span>\n    <span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">,</span>\n    <span class=\"n\">nice</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">output_type</span><span class=\"o\">=</span><span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">STRING</span><span class=\"p\">,</span>\n    <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Returns string containing recognized characters and their box boundaries\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"si\">{</span><span class=\"n\">config</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">()</span><span class=\"si\">}</span><span class=\"s\"> batch.nochop makebox</span><span class=\"sh\">'</span>\n    <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">box</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">nice</span><span class=\"p\">,</span> <span class=\"n\">timeout</span><span class=\"p\">]</span>\n\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">BYTES</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">args</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"bp\">True</span><span class=\"p\">])),</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">DICT</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">file_to_dict</span><span class=\"p\">(</span>\n            <span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"s\">char left bottom right top page</span><span class=\"se\">\\n</span><span class=\"si\">{</span><span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\"> </span><span class=\"sh\">'</span><span class=\"p\">,</span>\n            <span class=\"mi\">0</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">STRING</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">),</span>\n    <span class=\"p\">}[</span><span class=\"n\">output_type</span><span class=\"p\">]()</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">get_pandas_output</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">pandas_installed</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">PandasNotSupported</span><span class=\"p\">()</span>\n\n    <span class=\"n\">kwargs</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"sh\">'</span><span class=\"s\">quoting</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">QUOTE_NONE</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">sep</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"sh\">'</span><span class=\"se\">\\t</span><span class=\"sh\">'</span><span class=\"p\">}</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">kwargs</span><span class=\"p\">.</span><span class=\"nf\">update</span><span class=\"p\">(</span><span class=\"n\">config</span><span class=\"p\">)</span>\n    <span class=\"nf\">except </span><span class=\"p\">(</span><span class=\"nb\">TypeError</span><span class=\"p\">,</span> <span class=\"nb\">ValueError</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"nf\">read_csv</span><span class=\"p\">(</span><span class=\"nc\">BytesIO</span><span class=\"p\">(</span><span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">)),</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">image_to_data</span><span class=\"p\">(</span>\n    <span class=\"n\">image</span><span class=\"p\">,</span>\n    <span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">,</span>\n    <span class=\"n\">nice</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">output_type</span><span class=\"o\">=</span><span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">STRING</span><span class=\"p\">,</span>\n    <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">pandas_config</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n<span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Returns string containing box boundaries, confidences,\n    and other information. Requires Tesseract 3.05+\n    </span><span class=\"sh\">\"\"\"</span>\n\n    <span class=\"k\">if</span> <span class=\"nf\">get_tesseract_version</span><span class=\"p\">()</span> <span class=\"o\">&lt;</span> <span class=\"n\">TESSERACT_MIN_VERSION</span><span class=\"p\">:</span>\n        <span class=\"k\">raise</span> <span class=\"nc\">TSVNotSupported</span><span class=\"p\">()</span>\n\n    <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"s\">-c tessedit_create_tsv=1 </span><span class=\"si\">{</span><span class=\"n\">config</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">()</span><span class=\"si\">}</span><span class=\"sh\">'</span>\n    <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">tsv</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">nice</span><span class=\"p\">,</span> <span class=\"n\">timeout</span><span class=\"p\">]</span>\n\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">BYTES</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">args</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"bp\">True</span><span class=\"p\">])),</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">DATAFRAME</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">get_pandas_output</span><span class=\"p\">(</span>\n            <span class=\"n\">args</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"bp\">True</span><span class=\"p\">],</span>\n            <span class=\"n\">pandas_config</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">DICT</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">file_to_dict</span><span class=\"p\">(</span><span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">),</span> <span class=\"sh\">'</span><span class=\"se\">\\t</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">),</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">STRING</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">),</span>\n    <span class=\"p\">}[</span><span class=\"n\">output_type</span><span class=\"p\">]()</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">image_to_osd</span><span class=\"p\">(</span>\n    <span class=\"n\">image</span><span class=\"p\">,</span>\n    <span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"sh\">'</span><span class=\"s\">osd</span><span class=\"sh\">'</span><span class=\"p\">,</span>\n    <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"sh\">''</span><span class=\"p\">,</span>\n    <span class=\"n\">nice</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"n\">output_type</span><span class=\"o\">=</span><span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">STRING</span><span class=\"p\">,</span>\n    <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n<span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Returns string containing the orientation and script detection (OSD)\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"n\">config</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"s\">--psm 0 </span><span class=\"si\">{</span><span class=\"n\">config</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">()</span><span class=\"si\">}</span><span class=\"sh\">'</span>\n    <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">osd</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"p\">,</span> <span class=\"n\">nice</span><span class=\"p\">,</span> <span class=\"n\">timeout</span><span class=\"p\">]</span>\n\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">BYTES</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">args</span> <span class=\"o\">+</span> <span class=\"p\">[</span><span class=\"bp\">True</span><span class=\"p\">])),</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">DICT</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">osd_to_dict</span><span class=\"p\">(</span><span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">)),</span>\n        <span class=\"n\">Output</span><span class=\"p\">.</span><span class=\"n\">STRING</span><span class=\"p\">:</span> <span class=\"k\">lambda</span><span class=\"p\">:</span> <span class=\"nf\">run_and_get_output</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">),</span>\n    <span class=\"p\">}[</span><span class=\"n\">output_type</span><span class=\"p\">]()</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">():</span>\n    <span class=\"k\">if</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">argv</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">2</span><span class=\"p\">:</span>\n        <span class=\"n\">filename</span><span class=\"p\">,</span> <span class=\"n\">lang</span> <span class=\"o\">=</span> <span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"bp\">None</span>\n    <span class=\"k\">elif</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">argv</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">4</span> <span class=\"ow\">and</span> <span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"sh\">'</span><span class=\"s\">-l</span><span class=\"sh\">'</span><span class=\"p\">:</span>\n        <span class=\"n\">filename</span><span class=\"p\">,</span> <span class=\"n\">lang</span> <span class=\"o\">=</span> <span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">Usage: pytesseract [-l lang] input_file</span><span class=\"se\">\\n</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nb\">file</span><span class=\"o\">=</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stderr</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"mi\">2</span>\n\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"k\">with</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">filename</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">img</span><span class=\"p\">:</span>\n            <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"nf\">image_to_string</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"n\">lang</span><span class=\"p\">))</span>\n    <span class=\"k\">except</span> <span class=\"n\">TesseractNotFoundError</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"si\">{</span><span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"se\">\\n</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nb\">file</span><span class=\"o\">=</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stderr</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"mi\">1</span>\n    <span class=\"k\">except</span> <span class=\"nb\">OSError</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">'</span><span class=\"si\">{</span><span class=\"nf\">type</span><span class=\"p\">(</span><span class=\"n\">e</span><span class=\"p\">).</span><span class=\"n\">__name__</span><span class=\"si\">}</span><span class=\"s\">: </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"nb\">file</span><span class=\"o\">=</span><span class=\"n\">sys</span><span class=\"p\">.</span><span class=\"n\">stderr</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"mi\">1</span>\n\n\n<span class=\"k\">if</span> <span class=\"n\">__name__</span> <span class=\"o\">==</span> <span class=\"sh\">'</span><span class=\"s\">__main__</span><span class=\"sh\">'</span><span class=\"p\">:</span>\n    <span class=\"nf\">exit</span><span class=\"p\">(</span><span class=\"nf\">main</span><span class=\"p\">())</span>\n</code></pre></div>\n</div>\n<p data-sourcepos=\"661:1-661:54\">また、実行ファイルは以下となります。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"663:1-689:3\"><div class=\"highlight\"><pre><code><span class=\"kn\">import</span> <span class=\"n\">pytesseract</span>\n<span class=\"kn\">from</span> <span class=\"n\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n<span class=\"kn\">import</span> <span class=\"n\">pandas</span> <span class=\"k\">as</span> <span class=\"n\">pd</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">image_to_text</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">):</span>\n    <span class=\"c1\"># 画像を読み込む\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)</span>\n    <span class=\"c1\"># TesseractでOCRを実行\n</span>    <span class=\"n\">custom_config</span> <span class=\"o\">=</span> <span class=\"sa\">r</span><span class=\"sh\">'</span><span class=\"s\">--oem 1 --psm 6</span><span class=\"sh\">'</span>\n    <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">pytesseract</span><span class=\"p\">.</span><span class=\"nf\">image_to_string</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">custom_config</span><span class=\"p\">,</span> <span class=\"n\">lang</span><span class=\"o\">=</span><span class=\"sh\">'</span><span class=\"s\">jpn</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">text</span>\n\n<span class=\"k\">if</span> <span class=\"n\">__name__</span> <span class=\"o\">==</span> <span class=\"sh\">\"</span><span class=\"s\">__main__</span><span class=\"sh\">\"</span><span class=\"p\">:</span>\n        <span class=\"n\">image_path</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">C:/Users/ogiki/Desktop/data/大阪ばんざい.jpg</span><span class=\"sh\">'</span>\n        <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"nf\">image_to_text</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n        <span class=\"c1\"># ファイル保存\n</span>        <span class=\"n\">csv_path</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">output_ocr.csv</span><span class=\"sh\">'</span>\n        <span class=\"n\">rows</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"se\">\\n\\n</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n        <span class=\"n\">table_data</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        <span class=\"k\">for</span> <span class=\"n\">row</span> <span class=\"ow\">in</span> <span class=\"n\">rows</span><span class=\"p\">:</span>\n            <span class=\"c1\">#if row.strip():\n</span>            <span class=\"n\">table_data</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">row</span><span class=\"p\">)</span>\n        <span class=\"n\">df</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"p\">.</span><span class=\"nc\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">table_data</span><span class=\"p\">)</span>\n        <span class=\"n\">df</span><span class=\"p\">.</span><span class=\"nf\">to_csv</span><span class=\"p\">(</span><span class=\"n\">csv_path</span><span class=\"p\">,</span> <span class=\"n\">index</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"n\">header</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<p data-sourcepos=\"690:1-690:124\">ここに<code>custom_config = r'--oem 1 --psm 6'</code>とあるのですが、<code>oem</code>と<code>psm</code>は以下の意味があるようです。</p>\n<h3 data-sourcepos=\"692:1-692:33\">\n<span id=\"oem-ocrエンジン切替\" class=\"fragment\"></span><a href=\"#oem-ocr%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%B3%E5%88%87%E6%9B%BF\"><i class=\"fa fa-link\"></i></a>oem (OCRエンジン切替）</h3>\n<table data-sourcepos=\"693:1-698:88\">\n<thead>\n<tr data-sourcepos=\"693:1-693:28\">\n<th style=\"text-align: center\" data-sourcepos=\"693:2-693:18\">オプション</th>\n<th style=\"text-align: left\" data-sourcepos=\"693:20-693:27\">説明</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"695:1-695:72\">\n<td style=\"text-align: center\" data-sourcepos=\"695:2-695:5\">0</td>\n<td style=\"text-align: left\" data-sourcepos=\"695:7-695:71\">以前(3.5まで)のTesseractエンジンのみを使用する</td>\n</tr>\n<tr data-sourcepos=\"696:1-696:59\">\n<td style=\"text-align: center\" data-sourcepos=\"696:2-696:5\">1</td>\n<td style=\"text-align: left\" data-sourcepos=\"696:7-696:58\">ニューラルネットLSTMのみを使用する</td>\n</tr>\n<tr data-sourcepos=\"697:1-697:56\">\n<td style=\"text-align: center\" data-sourcepos=\"697:2-697:5\">2</td>\n<td style=\"text-align: left\" data-sourcepos=\"697:7-697:55\">TesseractエンジンとLSTM両方使用する</td>\n</tr>\n<tr data-sourcepos=\"698:1-698:88\">\n<td style=\"text-align: center\" data-sourcepos=\"698:2-698:5\">3</td>\n<td style=\"text-align: left\" data-sourcepos=\"698:7-698:87\">デフォルト。LSTMとTesseractエンジンを状況に応じて使用する</td>\n</tr>\n</tbody>\n</table>\n<h3 data-sourcepos=\"700:1-700:57\">\n<span id=\"psm-ページセグメンテーションモード\" class=\"fragment\"></span><a href=\"#psm-%E3%83%9A%E3%83%BC%E3%82%B8%E3%82%BB%E3%82%B0%E3%83%A1%E3%83%B3%E3%83%86%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3%E3%83%A2%E3%83%BC%E3%83%89\"><i class=\"fa fa-link\"></i></a>psm (ページセグメンテーションモード）</h3>\n<table data-sourcepos=\"701:1-716:81\">\n<thead>\n<tr data-sourcepos=\"701:1-701:28\">\n<th style=\"text-align: center\" data-sourcepos=\"701:2-701:18\">オプション</th>\n<th style=\"text-align: left\" data-sourcepos=\"701:20-701:27\">説明</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"703:1-703:134\">\n<td style=\"text-align: center\" data-sourcepos=\"703:2-703:5\">0</td>\n<td style=\"text-align: left\" data-sourcepos=\"703:7-703:133\">文字角度の識別と書字系のみの認識(OSD)のみ実施（outputbase.osdが出力され、OCRは行われない）</td>\n</tr>\n<tr data-sourcepos=\"704:1-704:58\">\n<td style=\"text-align: center\" data-sourcepos=\"704:2-704:5\">1</td>\n<td style=\"text-align: left\" data-sourcepos=\"704:7-704:57\">OSDと自動ページセグメンテーション</td>\n</tr>\n<tr data-sourcepos=\"705:1-705:82\">\n<td style=\"text-align: center\" data-sourcepos=\"705:2-705:5\">2</td>\n<td style=\"text-align: left\" data-sourcepos=\"705:7-705:81\">OSDなしの自動セグメンテーション（OCRは行われない）</td>\n</tr>\n<tr data-sourcepos=\"706:1-706:81\">\n<td style=\"text-align: center\" data-sourcepos=\"706:2-706:5\">3</td>\n<td style=\"text-align: left\" data-sourcepos=\"706:7-706:80\">OSDなしの完全自動セグメンテーション（デフォルト）</td>\n</tr>\n<tr data-sourcepos=\"707:1-707:58\">\n<td style=\"text-align: center\" data-sourcepos=\"707:2-707:5\">4</td>\n<td style=\"text-align: left\" data-sourcepos=\"707:7-707:57\">可変サイズの1列テキストを想定する</td>\n</tr>\n<tr data-sourcepos=\"708:1-708:66\">\n<td style=\"text-align: center\" data-sourcepos=\"708:2-708:5\">5</td>\n<td style=\"text-align: left\" data-sourcepos=\"708:7-708:65\">縦書きの単一のテキストブロックとみなす</td>\n</tr>\n<tr data-sourcepos=\"709:1-709:94\">\n<td style=\"text-align: center\" data-sourcepos=\"709:2-709:5\">6</td>\n<td style=\"text-align: left\" data-sourcepos=\"709:7-709:93\">単一のテキストブロックとみなす（5と異なる点は横書きのみ）</td>\n</tr>\n<tr data-sourcepos=\"710:1-710:49\">\n<td style=\"text-align: center\" data-sourcepos=\"710:2-710:5\">7</td>\n<td style=\"text-align: left\" data-sourcepos=\"710:7-710:48\">画像を1行のテキストとみなす</td>\n</tr>\n<tr data-sourcepos=\"711:1-711:36\">\n<td style=\"text-align: center\" data-sourcepos=\"711:2-711:5\">8</td>\n<td style=\"text-align: left\" data-sourcepos=\"711:7-711:35\">画像を単語とみなす</td>\n</tr>\n<tr data-sourcepos=\"712:1-712:82\">\n<td style=\"text-align: center\" data-sourcepos=\"712:2-712:5\">9</td>\n<td style=\"text-align: left\" data-sourcepos=\"712:7-712:81\">円の中に記載された1単語とみなす（例：①、⑥など）</td>\n</tr>\n<tr data-sourcepos=\"713:1-713:38\">\n<td style=\"text-align: center\" data-sourcepos=\"713:2-713:6\">10</td>\n<td style=\"text-align: left\" data-sourcepos=\"713:8-713:37\">画像を1文字とみなす</td>\n</tr>\n<tr data-sourcepos=\"714:1-714:115\">\n<td style=\"text-align: center\" data-sourcepos=\"714:2-714:6\">11</td>\n<td style=\"text-align: left\" data-sourcepos=\"714:8-714:114\">まだらなテキスト。特定の順序でなるべく多くの単語を検出する（角度無し）</td>\n</tr>\n<tr data-sourcepos=\"715:1-715:129\">\n<td style=\"text-align: center\" data-sourcepos=\"715:2-715:6\">12</td>\n<td style=\"text-align: left\" data-sourcepos=\"715:8-715:128\">文字角度検出を実施(OSD)しかつ、まだらなテキストとしてなるべく多くの単語を検出する</td>\n</tr>\n<tr data-sourcepos=\"716:1-716:81\">\n<td style=\"text-align: center\" data-sourcepos=\"716:2-716:6\">13</td>\n<td style=\"text-align: left\" data-sourcepos=\"716:8-716:80\">Tesseract固有の処理を回避して1行のテキストとみなす</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"719:1-719:51\">以下の投稿記事を参考にしています。</p>\n<p data-sourcepos=\"721:1-721:55\"><iframe id=\"qiita-embed-content__803ddb5011c108b78d9486c9c0d4f976\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__803ddb5011c108b78d9486c9c0d4f976\" data-content=\"https%3A%2F%2Fqiita.com%2Fhenjiganai%2Fitems%2F7a5e871f652b32b41a18\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<p data-sourcepos=\"723:1-723:239\">次章（「比較するサンプル」）で説明しますが、今回はjpeg形式の文字列(英語・数字・日本語の文字）の認識精度を比較するため、<code>psm</code>は「6」、<code>oem</code>は「1」として設定しました。</p>\n<h1 data-sourcepos=\"725:1-725:26\">\n<span id=\"比較するサンプル\" class=\"fragment\"></span><a href=\"#%E6%AF%94%E8%BC%83%E3%81%99%E3%82%8B%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB\"><i class=\"fa fa-link\"></i></a>比較するサンプル</h1>\n<p data-sourcepos=\"727:1-729:122\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2F185e8537-2cbb-2def-1782-b8de1598fb89.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=158d2904e65090614ff38bcafec427c8\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2F185e8537-2cbb-2def-1782-b8de1598fb89.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=158d2904e65090614ff38bcafec427c8\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2F185e8537-2cbb-2def-1782-b8de1598fb89.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=3062e25d065626c90d6816211db9c9f5 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/185e8537-2cbb-2def-1782-b8de1598fb89.png\" loading=\"lazy\"></a><br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2F45633ebe-63b9-de3c-cd34-330a1f22c9ca.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=41a60b32e71506fce92227b3cdf2a060\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2F45633ebe-63b9-de3c-cd34-330a1f22c9ca.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=41a60b32e71506fce92227b3cdf2a060\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2F45633ebe-63b9-de3c-cd34-330a1f22c9ca.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=b0c0dc5fdf9175bd8854e8314d52b394 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/45633ebe-63b9-de3c-cd34-330a1f22c9ca.png\" loading=\"lazy\"></a><br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2Fae23c3c8-4fc1-9745-83c9-fc040a848a0c.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=d689ee0d3de1aa93b4e1343ac5a3be81\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2Fae23c3c8-4fc1-9745-83c9-fc040a848a0c.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=d689ee0d3de1aa93b4e1343ac5a3be81\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F3784222%2Fae23c3c8-4fc1-9745-83c9-fc040a848a0c.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=c8029ea6efcd5bb6a30ecb0e0551d448 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/ae23c3c8-4fc1-9745-83c9-fc040a848a0c.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"731:1-732:171\">以上3つの画像ファイルをOCRとOpenAIに読み込ませてみることにしました。<br>\n最後の画像の「しらんけど」は大阪のおばちゃんが自信満々に話をした後に発する定型文です。あまり気になさらぬよう・・・</p>\n<h1 data-sourcepos=\"735:1-735:14\">\n<span id=\"比較結果\" class=\"fragment\"></span><a href=\"#%E6%AF%94%E8%BC%83%E7%B5%90%E6%9E%9C\"><i class=\"fa fa-link\"></i></a>比較結果</h1>\n<p data-sourcepos=\"737:1-737:39\">以下が比較結果となります。</p>\n<table data-sourcepos=\"739:1-743:148\">\n<thead>\n<tr data-sourcepos=\"739:1-739:33\">\n<th style=\"text-align: center\" data-sourcepos=\"739:2-739:12\">文字列</th>\n<th style=\"text-align: left\" data-sourcepos=\"739:14-739:18\">OCR</th>\n<th style=\"text-align: left\" data-sourcepos=\"739:20-739:32\">gpt-4o-mini</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"741:1-741:114\">\n<td style=\"text-align: center\" data-sourcepos=\"741:2-741:22\">65ABCZあっぱれ</td>\n<td style=\"text-align: left\" data-sourcepos=\"741:24-741:53\">6 ダを作 6てCあ.ばれ</td>\n<td style=\"text-align: left\" data-sourcepos=\"741:55-741:113\">図の中の文字は「65 ABCZ あっぱれ」です。</td>\n</tr>\n<tr data-sourcepos=\"742:1-742:77\">\n<td style=\"text-align: center\" data-sourcepos=\"742:2-742:11\">5329157</td>\n<td style=\"text-align: left\" data-sourcepos=\"742:13-742:29\">2コ2ブ7/ら7</td>\n<td style=\"text-align: left\" data-sourcepos=\"742:31-742:76\">図の中の数値は「5329/57」です。</td>\n</tr>\n<tr data-sourcepos=\"743:1-743:148\">\n<td style=\"text-align: center\" data-sourcepos=\"743:2-743:22\">大阪ばんざい</td>\n<td style=\"text-align: left\" data-sourcepos=\"743:24-743:65\">大、阪 1はんざい し5ん1けと\"</td>\n<td style=\"text-align: left\" data-sourcepos=\"743:67-743:147\">図の中の文字は「大阪ばんざい」と「しらんけど」です。</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"745:1-746:161\">どうでしょうか。今回に限っては断然<code>gpt-4o-mini</code>に軍配が上がりました。<br>\nただ、今回の<code>OCR</code>はオープンソースのものを使っているため、有償OCRなどであればもっと精度が高まるかもしれません。</p>\n<h1 data-sourcepos=\"748:1-748:14\">\n<span id=\"おわりに\" class=\"fragment\"></span><a href=\"#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB\"><i class=\"fa fa-link\"></i></a>おわりに</h1>\n<p data-sourcepos=\"749:1-750:209\">　今回の比較はあくまで<code>Tesseract</code>と<code>gpt-4o-mini</code>ということで記憶にとどめていただけると幸いです。<br>\n　今後、社内で「OCRをやりたいんだけど」という引き合いがあったら、<code>gpt-4o</code>のことも少し頭に入れておいて、選択肢の１つにしていただけると幸いです。</p>\n<p data-sourcepos=\"752:1-752:666\">　次回の記事では、宝くじ券の番号をOCRで認識させるプログラムを紹介します。私事で恐縮なのですが、先日宝くじを150枚買ったのですが、券を1つ1つ確認すると歳のせいか手がカサカサになり、紙で切れて血が出てしまいました。OCRを使って当選した券を瞬時に見分けられないか・・・ということで、宝くじ番号を大量に読み込んで、当たり券を判定するプログラムの記事を投稿したいと思います。（券売所の機械で確認してもらえばいいのに、プログラムで実装する必要あるか？・・・💦）</p>\n",
      "body": "<img width=A%><img width=100% src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/f920c76e-3c56-75b0-ea7d-4ea0fd880818.png\">\n\n# はじめに\n　情報システム部にいると、「OCRを試してみたい」とか「紙の帳票はやめないが、効率化を図りたい」などといろいろな引き合いが舞い込んできます。そのためOCRを小さなプロジェクトやPoCで試すことも多いのですが、文字認識の精度のせいなのか、ほとんどは立ち消えになってしまっています。\n 　一方で、最近リリースされた`gpt-4o`は画像認識が可能であり、OCRよりも精度が高いのではないか？と思い始めました。\n  今回は、OCRと`gpt-4o`(お金がないので正確には`gpt-4o-mini`）の読み取り精度を確認したいと思います。\n\n# GPT-4o\n今回紹介するプログラムは、指定した画像を生成AIに渡して、その結果を出力するというプログラムです。\n`base64`は標準ライブラリなので、改めてインストールする必要は無いようです。\n\n```python:\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.prompts.chat import HumanMessagePromptTemplate\nimport base64\nfrom langchain_core.prompts.image import ImagePromptTemplate\n\nquestion = \"\"\"\n図の中の文字を答えて\n\"\"\"\nimage_path = \"C:\\\\Users\\\\ogiki\\\\Desktop\\\\data\\\\大阪ばんざい.jpg\"\nsystem = (\n    \"あなたは有能なアシスタントです。ユーザーの問いに回答してください\"\n)\n\n## =================================================================\n\n#画像ファイルをbase64エンコードする\ndef encode_image(image_path):\n  with open(image_path, \"rb\") as image_file:\n    return base64.b64encode(image_file.read()).decode('utf-8')\n\n\nbase64_image = encode_image(image_path)\n\nimage_template = {\"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}}\n\nchat = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n#chat = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n\nhuman_prompt = \"{question}\"\nhuman_message_template = HumanMessagePromptTemplate.from_template([human_prompt, image_template])\n\nprompt = ChatPromptTemplate.from_messages([(\"system\", system), human_message_template])\n\nchain = prompt | chat\nresult = chain.invoke({\"question\": question})\nprint(result)\n```\n\nプログラム上に`gpt-3.5-turbo`が記述されているコード（コメントアウトしている）がありますが、もちろんこれを実行するとエラーが出ました。`gpt-3.5-tureb`には画像認識の処理はありませんものね。\n\n# OCR\n次にオープンソースのOCRを利用するためのプログラムを書きます。\n今回は`Tesseract`というライブラリを使うことにしました。Windowsでのインストールができるという事で、それをPythonのプログラムで操作することにしました。\nインストールは、以下の投稿記事を参考にしました。\n\nhttps://qiita.com/henjiganai/items/7a5e871f652b32b41a18\n\n@henjiganaiさん、わかりやすい記事を書いていただきありがとうございました。\n\nまた、最初に`Tesseract`を理解するために、以下の記事も参考にしました。\n\nhttps://qiita.com/ku_a_i/items/93fdbd75edacb34ec610\n\n@ku_a_i(ku_a_i)さんもありがとうございます。非常にわかりやすかったです。\n\nインストールはうまくいったのですが、その後Pythonのプログラムを実行しようとするとエラーが出ました。\n    いろいろ確認すると、`tesseract.exe`のパスがうまく通っていないようです。\n\nhttps://note.com/murasamejo/n/nb314171faea2\n\n上記の投稿記事を基に（kinopi さんありがとうございます）、Pythonライブラリの中の`pytesseract.py`の中にある`tesseract_cmd`の値を自分のパソコンのパスに置き換えて、`pytesseract.py`を実行ファイルと同一ディレクトリに置くことでプログラムをうまく動かすことができました。一旦はこの形で進めていきます。\n　以下に`pytesseract.py`を転記します。もし私と同じ状況に陥った場合、これをそのままコピーして31行目の`tesseract_com`の部分を皆様のインストールされた場所に変更し、実行ファイルと同一ディレクトリに置いていただけると実行できます。\n\n```python:pytesseract.py\n#!/usr/bin/env python\nimport re\nimport shlex\nimport string\nimport subprocess\nimport sys\nfrom contextlib import contextmanager\nfrom csv import QUOTE_NONE\nfrom errno import ENOENT\nfrom functools import wraps\nfrom glob import iglob\nfrom io import BytesIO\nfrom os import environ\nfrom os import extsep\nfrom os import linesep\nfrom os import remove\nfrom os.path import normcase\nfrom os.path import normpath\nfrom os.path import realpath\nfrom pkgutil import find_loader\nfrom tempfile import NamedTemporaryFile\nfrom time import sleep\n\nfrom packaging.version import InvalidVersion\nfrom packaging.version import parse\nfrom packaging.version import Version\nfrom PIL import Image\n\n\n#tesseract_cmd = 'tesseract'\ntesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n\nnumpy_installed = find_loader('numpy') is not None\nif numpy_installed:\n    from numpy import ndarray\n\npandas_installed = find_loader('pandas') is not None\nif pandas_installed:\n    import pandas as pd\n\nDEFAULT_ENCODING = 'utf-8'\nLANG_PATTERN = re.compile('^[a-z_]+$')\nRGB_MODE = 'RGB'\nSUPPORTED_FORMATS = {\n    'JPEG',\n    'JPEG2000',\n    'PNG',\n    'PBM',\n    'PGM',\n    'PPM',\n    'TIFF',\n    'BMP',\n    'GIF',\n    'WEBP',\n}\n\nOSD_KEYS = {\n    'Page number': ('page_num', int),\n    'Orientation in degrees': ('orientation', int),\n    'Rotate': ('rotate', int),\n    'Orientation confidence': ('orientation_conf', float),\n    'Script': ('script', str),\n    'Script confidence': ('script_conf', float),\n}\n\nTESSERACT_MIN_VERSION = Version('3.05')\nTESSERACT_ALTO_VERSION = Version('4.1.0')\n\n\nclass Output:\n    BYTES = 'bytes'\n    DATAFRAME = 'data.frame'\n    DICT = 'dict'\n    STRING = 'string'\n\n\nclass PandasNotSupported(EnvironmentError):\n    def __init__(self):\n        super().__init__('Missing pandas package')\n\n\nclass TesseractError(RuntimeError):\n    def __init__(self, status, message):\n        self.status = status\n        self.message = message\n        self.args = (status, message)\n\n\nclass TesseractNotFoundError(EnvironmentError):\n    def __init__(self):\n        super().__init__(\n            f\"{tesseract_cmd} is not installed or it's not in your PATH.\"\n            f' See README file for more information.',\n        )\n\n\nclass TSVNotSupported(EnvironmentError):\n    def __init__(self):\n        super().__init__(\n            'TSV output not supported. Tesseract >= 3.05 required',\n        )\n\n\nclass ALTONotSupported(EnvironmentError):\n    def __init__(self):\n        super().__init__(\n            'ALTO output not supported. Tesseract >= 4.1.0 required',\n        )\n\n\ndef kill(process, code):\n    process.terminate()\n    try:\n        process.wait(1)\n    except TypeError:  # python2 Popen.wait(1) fallback\n        sleep(1)\n    except Exception:  # python3 subprocess.TimeoutExpired\n        pass\n    finally:\n        process.kill()\n        process.returncode = code\n\n\n@contextmanager\ndef timeout_manager(proc, seconds=None):\n    try:\n        if not seconds:\n            yield proc.communicate()[1]\n            return\n\n        try:\n            _, error_string = proc.communicate(timeout=seconds)\n            yield error_string\n        except subprocess.TimeoutExpired:\n            kill(proc, -1)\n            raise RuntimeError('Tesseract process timeout')\n    finally:\n        proc.stdin.close()\n        proc.stdout.close()\n        proc.stderr.close()\n\n\ndef run_once(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        if wrapper._result is wrapper:\n            wrapper._result = func(*args, **kwargs)\n        return wrapper._result\n\n    wrapper._result = wrapper\n    return wrapper\n\n\ndef get_errors(error_string):\n    return ' '.join(\n        line for line in error_string.decode(DEFAULT_ENCODING).splitlines()\n    ).strip()\n\n\ndef cleanup(temp_name):\n    \"\"\"Tries to remove temp files by filename wildcard path.\"\"\"\n    for filename in iglob(f'{temp_name}*' if temp_name else temp_name):\n        try:\n            remove(filename)\n        except OSError as e:\n            if e.errno != ENOENT:\n                raise\n\n\ndef prepare(image):\n    if numpy_installed and isinstance(image, ndarray):\n        image = Image.fromarray(image)\n\n    if not isinstance(image, Image.Image):\n        raise TypeError('Unsupported image object')\n\n    extension = 'PNG' if not image.format else image.format\n    if extension not in SUPPORTED_FORMATS:\n        raise TypeError('Unsupported image format/type')\n\n    if 'A' in image.getbands():\n        # discard and replace the alpha channel with white background\n        background = Image.new(RGB_MODE, image.size, (255, 255, 255))\n        background.paste(image, (0, 0), image.getchannel('A'))\n        image = background\n\n    image.format = extension\n    return image, extension\n\n\n@contextmanager\ndef save(image):\n    try:\n        with NamedTemporaryFile(prefix='tess_', delete=False) as f:\n            if isinstance(image, str):\n                yield f.name, realpath(normpath(normcase(image)))\n                return\n            image, extension = prepare(image)\n            input_file_name = f'{f.name}_input{extsep}{extension}'\n            image.save(input_file_name, format=image.format)\n            yield f.name, input_file_name\n    finally:\n        cleanup(f.name)\n\n\ndef subprocess_args(include_stdout=True):\n    # See https://github.com/pyinstaller/pyinstaller/wiki/Recipe-subprocess\n    # for reference and comments.\n\n    kwargs = {\n        'stdin': subprocess.PIPE,\n        'stderr': subprocess.PIPE,\n        'startupinfo': None,\n        'env': environ,\n    }\n\n    if hasattr(subprocess, 'STARTUPINFO'):\n        kwargs['startupinfo'] = subprocess.STARTUPINFO()\n        kwargs['startupinfo'].dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        kwargs['startupinfo'].wShowWindow = subprocess.SW_HIDE\n\n    if include_stdout:\n        kwargs['stdout'] = subprocess.PIPE\n    else:\n        kwargs['stdout'] = subprocess.DEVNULL\n\n    return kwargs\n\n\ndef run_tesseract(\n    input_filename,\n    output_filename_base,\n    extension,\n    lang,\n    config='',\n    nice=0,\n    timeout=0,\n):\n    cmd_args = []\n\n    if not sys.platform.startswith('win32') and nice != 0:\n        cmd_args += ('nice', '-n', str(nice))\n\n    cmd_args += (tesseract_cmd, input_filename, output_filename_base)\n\n    if lang is not None:\n        cmd_args += ('-l', lang)\n\n    if config:\n        cmd_args += shlex.split(config)\n\n    if extension and extension not in {'box', 'osd', 'tsv', 'xml'}:\n        cmd_args.append(extension)\n\n    try:\n        proc = subprocess.Popen(cmd_args, **subprocess_args())\n    except OSError as e:\n        if e.errno != ENOENT:\n            raise\n        else:\n            raise TesseractNotFoundError()\n\n    with timeout_manager(proc, timeout) as error_string:\n        if proc.returncode:\n            raise TesseractError(proc.returncode, get_errors(error_string))\n\n\ndef run_and_get_output(\n    image,\n    extension='',\n    lang=None,\n    config='',\n    nice=0,\n    timeout=0,\n    return_bytes=False,\n):\n\n    with save(image) as (temp_name, input_filename):\n        kwargs = {\n            'input_filename': input_filename,\n            'output_filename_base': temp_name,\n            'extension': extension,\n            'lang': lang,\n            'config': config,\n            'nice': nice,\n            'timeout': timeout,\n        }\n\n        run_tesseract(**kwargs)\n        filename = f\"{kwargs['output_filename_base']}{extsep}{extension}\"\n        with open(filename, 'rb') as output_file:\n            if return_bytes:\n                return output_file.read()\n            return output_file.read().decode(DEFAULT_ENCODING)\n\n\ndef file_to_dict(tsv, cell_delimiter, str_col_idx):\n    result = {}\n    rows = [row.split(cell_delimiter) for row in tsv.strip().split('\\n')]\n    if len(rows) < 2:\n        return result\n\n    header = rows.pop(0)\n    length = len(header)\n    if len(rows[-1]) < length:\n        # Fixes bug that occurs when last text string in TSV is null, and\n        # last row is missing a final cell in TSV file\n        rows[-1].append('')\n\n    if str_col_idx < 0:\n        str_col_idx += length\n\n    for i, head in enumerate(header):\n        result[head] = list()\n        for row in rows:\n            if len(row) <= i:\n                continue\n\n            if i != str_col_idx:\n                try:\n                    val = int(float(row[i]))\n                except ValueError:\n                    val = row[i]\n            else:\n                val = row[i]\n\n            result[head].append(val)\n\n    return result\n\n\ndef is_valid(val, _type):\n    if _type is int:\n        return val.isdigit()\n\n    if _type is float:\n        try:\n            float(val)\n            return True\n        except ValueError:\n            return False\n\n    return True\n\n\ndef osd_to_dict(osd):\n    return {\n        OSD_KEYS[kv[0]][0]: OSD_KEYS[kv[0]][1](kv[1])\n        for kv in (line.split(': ') for line in osd.split('\\n'))\n        if len(kv) == 2 and is_valid(kv[1], OSD_KEYS[kv[0]][1])\n    }\n\n\n@run_once\ndef get_languages(config=''):\n    cmd_args = [tesseract_cmd, '--list-langs']\n    if config:\n        cmd_args += shlex.split(config)\n\n    try:\n        result = subprocess.run(\n            cmd_args,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n        )\n    except OSError:\n        raise TesseractNotFoundError()\n\n    # tesseract 3.x\n    if result.returncode not in (0, 1):\n        raise TesseractNotFoundError()\n\n    languages = []\n    if result.stdout:\n        for line in result.stdout.decode(DEFAULT_ENCODING).split(linesep):\n            lang = line.strip()\n            if LANG_PATTERN.match(lang):\n                languages.append(lang)\n\n    return languages\n\n\n@run_once\ndef get_tesseract_version():\n    \"\"\"\n    Returns Version object of the Tesseract version\n    \"\"\"\n    try:\n        output = subprocess.check_output(\n            [tesseract_cmd, '--version'],\n            stderr=subprocess.STDOUT,\n            env=environ,\n            stdin=subprocess.DEVNULL,\n        )\n    except OSError:\n        raise TesseractNotFoundError()\n\n    raw_version = output.decode(DEFAULT_ENCODING)\n    str_version, *_ = raw_version.lstrip(string.printable[10:]).partition(' ')\n    str_version, *_ = str_version.partition('-')\n\n    try:\n        version = parse(str_version)\n        assert version >= TESSERACT_MIN_VERSION\n    except (AssertionError, InvalidVersion):\n        raise SystemExit(f'Invalid tesseract version: \"{raw_version}\"')\n\n    return version\n\n\ndef image_to_string(\n    image,\n    lang=None,\n    config='',\n    nice=0,\n    output_type=Output.STRING,\n    timeout=0,\n):\n    \"\"\"\n    Returns the result of a Tesseract OCR run on the provided image to string\n    \"\"\"\n    args = [image, 'txt', lang, config, nice, timeout]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DICT: lambda: {'text': run_and_get_output(*args)},\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()\n\n\ndef image_to_pdf_or_hocr(\n    image,\n    lang=None,\n    config='',\n    nice=0,\n    extension='pdf',\n    timeout=0,\n):\n    \"\"\"\n    Returns the result of a Tesseract OCR run on the provided image to pdf/hocr\n    \"\"\"\n\n    if extension not in {'pdf', 'hocr'}:\n        raise ValueError(f'Unsupported extension: {extension}')\n    args = [image, extension, lang, config, nice, timeout, True]\n\n    return run_and_get_output(*args)\n\n\ndef image_to_alto_xml(\n    image,\n    lang=None,\n    config='',\n    nice=0,\n    timeout=0,\n):\n    \"\"\"\n    Returns the result of a Tesseract OCR run on the provided image to ALTO XML\n    \"\"\"\n\n    if get_tesseract_version() < TESSERACT_ALTO_VERSION:\n        raise ALTONotSupported()\n\n    config = f'-c tessedit_create_alto=1 {config.strip()}'\n    args = [image, 'xml', lang, config, nice, timeout, True]\n\n    return run_and_get_output(*args)\n\n\ndef image_to_boxes(\n    image,\n    lang=None,\n    config='',\n    nice=0,\n    output_type=Output.STRING,\n    timeout=0,\n):\n    \"\"\"\n    Returns string containing recognized characters and their box boundaries\n    \"\"\"\n    config = f'{config.strip()} batch.nochop makebox'\n    args = [image, 'box', lang, config, nice, timeout]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DICT: lambda: file_to_dict(\n            f'char left bottom right top page\\n{run_and_get_output(*args)}',\n            ' ',\n            0,\n        ),\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()\n\n\ndef get_pandas_output(args, config=None):\n    if not pandas_installed:\n        raise PandasNotSupported()\n\n    kwargs = {'quoting': QUOTE_NONE, 'sep': '\\t'}\n    try:\n        kwargs.update(config)\n    except (TypeError, ValueError):\n        pass\n\n    return pd.read_csv(BytesIO(run_and_get_output(*args)), **kwargs)\n\n\ndef image_to_data(\n    image,\n    lang=None,\n    config='',\n    nice=0,\n    output_type=Output.STRING,\n    timeout=0,\n    pandas_config=None,\n):\n    \"\"\"\n    Returns string containing box boundaries, confidences,\n    and other information. Requires Tesseract 3.05+\n    \"\"\"\n\n    if get_tesseract_version() < TESSERACT_MIN_VERSION:\n        raise TSVNotSupported()\n\n    config = f'-c tessedit_create_tsv=1 {config.strip()}'\n    args = [image, 'tsv', lang, config, nice, timeout]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DATAFRAME: lambda: get_pandas_output(\n            args + [True],\n            pandas_config,\n        ),\n        Output.DICT: lambda: file_to_dict(run_and_get_output(*args), '\\t', -1),\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()\n\n\ndef image_to_osd(\n    image,\n    lang='osd',\n    config='',\n    nice=0,\n    output_type=Output.STRING,\n    timeout=0,\n):\n    \"\"\"\n    Returns string containing the orientation and script detection (OSD)\n    \"\"\"\n    config = f'--psm 0 {config.strip()}'\n    args = [image, 'osd', lang, config, nice, timeout]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DICT: lambda: osd_to_dict(run_and_get_output(*args)),\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()\n\n\ndef main():\n    if len(sys.argv) == 2:\n        filename, lang = sys.argv[1], None\n    elif len(sys.argv) == 4 and sys.argv[1] == '-l':\n        filename, lang = sys.argv[3], sys.argv[2]\n    else:\n        print('Usage: pytesseract [-l lang] input_file\\n', file=sys.stderr)\n        return 2\n\n    try:\n        with Image.open(filename) as img:\n            print(image_to_string(img, lang=lang))\n    except TesseractNotFoundError as e:\n        print(f'{str(e)}\\n', file=sys.stderr)\n        return 1\n    except OSError as e:\n        print(f'{type(e).__name__}: {e}', file=sys.stderr)\n        return 1\n\n\nif __name__ == '__main__':\n    exit(main())\n```\n\nまた、実行ファイルは以下となります。\n\n```python\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\n\ndef image_to_text(image_path):\n    # 画像を読み込む\n    img = Image.open(image_path)\n    # TesseractでOCRを実行\n    custom_config = r'--oem 1 --psm 6'\n    text = pytesseract.image_to_string(img, config=custom_config, lang='jpn')\n    return text\n\nif __name__ == \"__main__\":\n        image_path = 'C:/Users/ogiki/Desktop/data/大阪ばんざい.jpg'\n        text = image_to_text(image_path)\n        print(text)\n        # ファイル保存\n        csv_path = 'output_ocr.csv'\n        rows = text.split('\\n\\n')\n        table_data = []\n        for row in rows:\n            #if row.strip():\n            table_data.append(row)\n        df = pd.DataFrame(table_data)\n        df.to_csv(csv_path, index=False, header=False)\n```\nここに`custom_config = r'--oem 1 --psm 6'`とあるのですが、`oem`と`psm`は以下の意味があるようです。\n\n### oem (OCRエンジン切替）\n| オプション | 説明 |\n|:-:|:-|\n| 0  | 以前(3.5まで)のTesseractエンジンのみを使用する  |\n| 1  | ニューラルネットLSTMのみを使用する  |\n| 2  | TesseractエンジンとLSTM両方使用する  |\n| 3  | デフォルト。LSTMとTesseractエンジンを状況に応じて使用する |\n\n### psm (ページセグメンテーションモード）\n| オプション | 説明 |\n|:-:|:-|\n| 0  | 文字角度の識別と書字系のみの認識(OSD)のみ実施（outputbase.osdが出力され、OCRは行われない）  |\n| 1  | OSDと自動ページセグメンテーション  |\n| 2  | OSDなしの自動セグメンテーション（OCRは行われない）  |\n| 3  | OSDなしの完全自動セグメンテーション（デフォルト） |\n| 4  | 可変サイズの1列テキストを想定する |\n| 5  | 縦書きの単一のテキストブロックとみなす |\n| 6  | 単一のテキストブロックとみなす（5と異なる点は横書きのみ） |\n| 7  | 画像を1行のテキストとみなす |\n| 8  | 画像を単語とみなす |\n| 9  | 円の中に記載された1単語とみなす（例：①、⑥など） |\n| 10  | 画像を1文字とみなす |\n| 11  | まだらなテキスト。特定の順序でなるべく多くの単語を検出する（角度無し） |\n| 12  | 文字角度検出を実施(OSD)しかつ、まだらなテキストとしてなるべく多くの単語を検出する |\n| 13  | \tTesseract固有の処理を回避して1行のテキストとみなす |\n\n\n以下の投稿記事を参考にしています。\n\nhttps://qiita.com/henjiganai/items/7a5e871f652b32b41a18\n\n次章（「比較するサンプル」）で説明しますが、今回はjpeg形式の文字列(英語・数字・日本語の文字）の認識精度を比較するため、`psm`は「6」、`oem`は「1」として設定しました。\n\n# 比較するサンプル\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/185e8537-2cbb-2def-1782-b8de1598fb89.png)\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/45633ebe-63b9-de3c-cd34-330a1f22c9ca.png)\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/3784222/ae23c3c8-4fc1-9745-83c9-fc040a848a0c.png)\n\n以上3つの画像ファイルをOCRとOpenAIに読み込ませてみることにしました。\n最後の画像の「しらんけど」は大阪のおばちゃんが自信満々に話をした後に発する定型文です。あまり気になさらぬよう・・・\n\n\n# 比較結果\n\n以下が比較結果となります。\n\n| 文字列 | OCR | gpt-4o-mini |\n|:-:|:-|:-|\n| 65ABCZあっぱれ  | 6 ダを作 6てCあ.ばれ  | 図の中の文字は「65 ABCZ あっぱれ」です。  |\n| 5329157  | 2コ2ブ7/ら7  | 図の中の数値は「5329/57」です。  |\n| 大阪ばんざい  | 大、阪 1はんざい し5ん1けと\"  | 図の中の文字は「大阪ばんざい」と「しらんけど」です。  |\n\nどうでしょうか。今回に限っては断然`gpt-4o-mini`に軍配が上がりました。\nただ、今回の`OCR`はオープンソースのものを使っているため、有償OCRなどであればもっと精度が高まるかもしれません。\n\n# おわりに\n　今回の比較はあくまで`Tesseract`と`gpt-4o-mini`ということで記憶にとどめていただけると幸いです。\n　今後、社内で「OCRをやりたいんだけど」という引き合いがあったら、`gpt-4o`のことも少し頭に入れておいて、選択肢の１つにしていただけると幸いです。\n\n　次回の記事では、宝くじ券の番号をOCRで認識させるプログラムを紹介します。私事で恐縮なのですが、先日宝くじを150枚買ったのですが、券を1つ1つ確認すると歳のせいか手がカサカサになり、紙で切れて血が出てしまいました。OCRを使って当選した券を瞬時に見分けられないか・・・ということで、宝くじ番号を大量に読み込んで、当たり券を判定するプログラムの記事を投稿したいと思います。（券売所の機械で確認してもらえばいいのに、プログラムで実装する必要あるか？・・・💦）\n",
      "coediting": false,
      "comments_count": 0,
      "created_at": "2024-09-14T11:05:12+09:00",
      "group": null,
      "id": "f6b8e3426349767e8f7b",
      "likes_count": 4,
      "private": false,
      "reactions_count": 0,
      "stocks_count": 4,
      "tags": [
        {
          "name": "Python",
          "versions": []
        },
        {
          "name": "OCR",
          "versions": []
        },
        {
          "name": "OpenAI",
          "versions": []
        },
        {
          "name": "ChatGPT",
          "versions": []
        },
        {
          "name": "LangChain",
          "versions": []
        }
      ],
      "title": "OCRとOpenAIを比較してみた",
      "updated_at": "2024-09-14T11:19:57+09:00",
      "url": "https://qiita.com/ogi_kimura/items/f6b8e3426349767e8f7b",
      "user": {
        "description": "現在、某ユーザ企業の情報システム部門、いわゆる「情シス」に所属。\r\n\r\n以前はソフトウェア会社に勤務をしてプログラミングやプロジェクト・リーダーなどをやってきたが、それほどプログラミングをできるわけでもなく、また歳もとってきており、ソフトウェア開発で働き続けることに未来への漠然な不安から現在の会社に所属。\r\n\r\nhttps://www.npwitys.com/",
        "facebook_id": "ryusuke.kimura.16",
        "followees_count": 2,
        "followers_count": 22,
        "github_login_name": "kimkimkim5",
        "id": "ogi_kimura",
        "items_count": 51,
        "linkedin_id": "ryusuke-kimura-026051119/",
        "location": "大阪府",
        "name": "木村 たろう",
        "organization": "",
        "permanent_id": 3784222,
        "profile_image_url": "https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/3784222/9d2007926943a8a75c136d95de10bd95ff75df76/x_large.png?1714961191",
        "team_only": false,
        "twitter_screen_name": "ogi_kimura",
        "website_url": "https://www.npwitys.com/"
      },
      "page_views_count": null,
      "team_membership": null,
      "organization_url_name": null,
      "slide": false,
      "semantic_similarity": 0.8282209038734436,
      "quality_score": 24,
      "python_code_score": 7,
      "python_code_blocks": 3
    },
    {
      "rendered_body": "<h1 data-sourcepos=\"1:1-1:14\">\n<span id=\"はじめに\" class=\"fragment\"></span><a href=\"#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\"><i class=\"fa fa-link\"></i></a>はじめに</h1>\n<p data-sourcepos=\"3:1-3:149\">ChatGPTブームがひと段落した感がありますが、周りのエンジニアでChatGPTを活用している姿をあまり見みません。</p>\n<p data-sourcepos=\"5:1-5:208\">基本的なテクニックを理解すれば、エンジニアこそChatGPTを活用できると思うので、普段使用しているテクニックをいくつかピックアップして紹介します。</p>\n<h2 data-sourcepos=\"7:1-7:33\">\n<span id=\"プロンプトの記載方法\" class=\"fragment\"></span><a href=\"#%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88%E3%81%AE%E8%A8%98%E8%BC%89%E6%96%B9%E6%B3%95\"><i class=\"fa fa-link\"></i></a>プロンプトの記載方法</h2>\n<h3 data-sourcepos=\"9:1-9:33\">\n<span id=\"markdown記法で指示する\" class=\"fragment\"></span><a href=\"#markdown%E8%A8%98%E6%B3%95%E3%81%A7%E6%8C%87%E7%A4%BA%E3%81%99%E3%82%8B\"><i class=\"fa fa-link\"></i></a>Markdown記法で指示する</h3>\n<p data-sourcepos=\"11:1-11:195\">色々なところで紹介されていますが、回答や処理の精度を上げる方法としてChatGPTへの指示にMarkdown記法を使用することがオススメされています。</p>\n<p data-sourcepos=\"13:1-13:75\">例えば下記のような文章による指示を行おうとした場合</p>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"15:1-18:3\">\n<div class=\"code-lang\"><span class=\"bold\">文章による指示</span></div>\n<div class=\"highlight\"><pre><code>あなたはChatGPTのプロンプトエンジニアです。ChatGPTの基本的なテクニックを教えてください。\nまた、テクニックはリスト形式で10個提示してください。\n</code></pre></div>\n</div>\n<p data-sourcepos=\"20:1-20:62\">Markdown記法に書き直すと下記の形になります。</p>\n<div class=\"code-frame\" data-lang=\"markdown\" data-sourcepos=\"22:1-31:3\">\n<div class=\"code-lang\"><span class=\"bold\">Markdown記法による指示</span></div>\n<div class=\"highlight\"><pre><code><span class=\"gh\"># 指示内容</span>\nChatGPTの基本的なテクニックを教えてください。\n\n<span class=\"gh\"># 役割</span>\nあなたはChatGPTのプロンプトエンジニアです。\n\n<span class=\"gh\"># 出力形式</span>\nリスト形式で10項目提示します。\n</code></pre></div>\n</div>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F02849036-05be-a486-f13d-36a14f3b9218.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=9e9c930dcab840a41a0aa241a763b6a6\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F02849036-05be-a486-f13d-36a14f3b9218.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=9e9c930dcab840a41a0aa241a763b6a6\" width=\"400\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F02849036-05be-a486-f13d-36a14f3b9218.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=1a19262958f26c2e6d5d545deaade812 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/02849036-05be-a486-f13d-36a14f3b9218.png\" loading=\"lazy\"></a>\n<p data-sourcepos=\"34:1-34:106\">なお、Markdown記法についてはQiitaでも紹介されているので参考にしてください。</p>\n<p data-sourcepos=\"36:1-36:50\"><iframe id=\"qiita-embed-content__c1f9970d29d3192d6cb0ecddaad68cac\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__c1f9970d29d3192d6cb0ecddaad68cac\" data-content=\"https%3A%2F%2Fqiita.com%2Ftbpgr%2Fitems%2F989c6badefff69377da7\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<h3 data-sourcepos=\"38:1-38:19\">\n<span id=\"変数を使う\" class=\"fragment\"></span><a href=\"#%E5%A4%89%E6%95%B0%E3%82%92%E4%BD%BF%E3%81%86\"><i class=\"fa fa-link\"></i></a>変数を使う</h3>\n<p data-sourcepos=\"40:1-40:133\">ChatGPTへの指示で変数を扱うことができます。変数は変数名を中括弧(<code>{}</code>)で囲う事で使用できます。</p>\n<p data-sourcepos=\"42:1-42:78\">例えば指示内容や出力方法などにも変数を使用できます。</p>\n<div class=\"code-frame\" data-lang=\"markdown\" data-sourcepos=\"44:1-55:3\"><div class=\"highlight\"><pre><code><span class=\"gh\"># 指示</span>\n<span class=\"p\">-</span> {名前}を使用した会話のサンプルを出力してください。\n<span class=\"p\">-</span> ChatGPTの基本的なテクニックについて会話を行ってください。\n\n<span class=\"gh\"># 名前</span>\n<span class=\"p\">-</span> 山田太郎\n<span class=\"p\">-</span> 山田花子\n\n<span class=\"gh\"># 出力フォーマット</span>\n<span class=\"p\">-</span> <span class=\"sb\">`{名前}:{会話}`</span>の形式で出力してください\n</code></pre></div></div>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4d43cc2f-5aff-8ed2-24a8-0dbde60eaf99.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=7d05f8aac1bae09ad1d8f04110f29313\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4d43cc2f-5aff-8ed2-24a8-0dbde60eaf99.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=7d05f8aac1bae09ad1d8f04110f29313\" width=\"400\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4d43cc2f-5aff-8ed2-24a8-0dbde60eaf99.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=022f8cd000c5c24a78281946282104de 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/4d43cc2f-5aff-8ed2-24a8-0dbde60eaf99.png\" loading=\"lazy\"></a>\n<p data-sourcepos=\"59:1-59:69\">また、入力内容を変数に格納させる事もできます。</p>\n<div class=\"code-frame\" data-lang=\"markdown\" data-sourcepos=\"61:1-74:3\"><div class=\"highlight\"><pre><code><span class=\"gh\"># 指示</span>\n<span class=\"p\">-</span> {会話テキスト}を要約してください。\n<span class=\"p\">-</span> {会話テキスト}のフォーマットは<span class=\"sb\">`{{名前}}:{{会話}}`</span>の形式で記載されています。\n\n<span class=\"gh\"># 会話テキスト</span>\n\"\"\"\n山田太郎:まず、一つ目は「具体的な指示を与えること」だよ。例えば、何かを説明してもらいたいときには、具体的に何について説明してほしいかを明確にするんだ。\n山田花子:なるほど、それは確かに重要ね。具体的に指示を出すことで、より正確な答えが得られるんだね。\n山田太郎:そうそう。そして、二つ目は「段階的な質問をすること」。複雑な問題を解決するときには、一度に全部を聞くのではなく、段階を追って質問することで、より詳細な情報を得ることができるんだ。\n山田花子:それはいいアイデアね。例えば、料理のレシピを知りたいときに、最初に材料を聞いて、その次に作り方を聞くって感じかな。\n山田太郎:まさにその通り。それから、三つ目は「コンテキストを提供すること」。質問をする前に、少し背景を説明すると、より適切な答えが返ってくることが多いんだ。\n\"\"\"\n</code></pre></div></div>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F81cc7cb1-ddbb-8f57-5e9a-388b5f71c3d8.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=3879c072da38de586eec9fca11b5dcc8\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F81cc7cb1-ddbb-8f57-5e9a-388b5f71c3d8.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=3879c072da38de586eec9fca11b5dcc8\" width=\"400\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F81cc7cb1-ddbb-8f57-5e9a-388b5f71c3d8.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=d422a4022d95e340005399345cf03f5d 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/81cc7cb1-ddbb-8f57-5e9a-388b5f71c3d8.png\" loading=\"lazy\"></a>\n<h3 data-sourcepos=\"78:1-78:34\">\n<span id=\"パラメータを設定する\" class=\"fragment\"></span><a href=\"#%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%82%92%E8%A8%AD%E5%AE%9A%E3%81%99%E3%82%8B\"><i class=\"fa fa-link\"></i></a>パラメータを設定する</h3>\n<p data-sourcepos=\"80:1-81:184\">独自で作成した変数とは別に、ChatGPT独自が持っているパラメータがあります。<br>\n代表的なパラメータはOpenAI APIでも使用しているパラメータになりますが、その中でも有用なパラメータをピックアップして紹介します。</p>\n<p data-sourcepos=\"83:1-83:51\"><iframe id=\"qiita-embed-content__49029a4fec4356573449fb10b2fc1e5f\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__49029a4fec4356573449fb10b2fc1e5f\" data-content=\"https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fapi-reference%2Fchat\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<table data-sourcepos=\"85:1-93:119\">\n<thead>\n<tr data-sourcepos=\"85:1-85:54\">\n<th data-sourcepos=\"85:2-85:16\">パラメータ</th>\n<th data-sourcepos=\"85:18-85:20\">型</th>\n<th data-sourcepos=\"85:22-85:27\">範囲</th>\n<th data-sourcepos=\"85:29-85:46\">デフォルト値</th>\n<th data-sourcepos=\"85:48-85:53\">説明</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"87:1-87:119\">\n<td data-sourcepos=\"87:2-87:18\">frequency_penalty</td>\n<td data-sourcepos=\"87:20-87:24\">float</td>\n<td data-sourcepos=\"87:26-87:34\">0.0～2.0</td>\n<td data-sourcepos=\"87:36-87:36\">0</td>\n<td data-sourcepos=\"87:38-87:118\">同じ単語やフレーズの繰り返しを避けるためのペナルティ。</td>\n</tr>\n<tr data-sourcepos=\"88:1-88:85\">\n<td data-sourcepos=\"88:2-88:11\">max_tokens</td>\n<td data-sourcepos=\"88:13-88:15\">int</td>\n<td data-sourcepos=\"88:17-88:24\">1～4096</td>\n<td data-sourcepos=\"88:26-88:29\">4096</td>\n<td data-sourcepos=\"88:31-88:84\">生成されるテキストの最大トークン数。</td>\n</tr>\n<tr data-sourcepos=\"89:1-89:47\">\n<td data-sourcepos=\"89:2-89:2\">n</td>\n<td data-sourcepos=\"89:4-89:6\">int</td>\n<td data-sourcepos=\"89:8-89:13\">1～20</td>\n<td data-sourcepos=\"89:15-89:15\">1</td>\n<td data-sourcepos=\"89:17-89:46\">生成される応答の数。</td>\n</tr>\n<tr data-sourcepos=\"90:1-90:121\">\n<td data-sourcepos=\"90:2-90:17\">presence_penalty</td>\n<td data-sourcepos=\"90:19-90:23\">float</td>\n<td data-sourcepos=\"90:25-90:33\">0.0～2.0</td>\n<td data-sourcepos=\"90:35-90:35\">0</td>\n<td data-sourcepos=\"90:37-90:120\">特定のトピックやアイデアの頻出を避けるためのペナルティ。</td>\n</tr>\n<tr data-sourcepos=\"91:1-91:94\">\n<td data-sourcepos=\"91:2-91:5\">seed</td>\n<td data-sourcepos=\"91:7-91:9\">int</td>\n<td data-sourcepos=\"91:11-91:25\">任意の整数</td>\n<td data-sourcepos=\"91:27-91:32\">なし</td>\n<td data-sourcepos=\"91:34-91:93\">再現性を持たせるためのランダムシード値。</td>\n</tr>\n<tr data-sourcepos=\"92:1-92:110\">\n<td data-sourcepos=\"92:2-92:12\">temperature</td>\n<td data-sourcepos=\"92:14-92:18\">float</td>\n<td data-sourcepos=\"92:20-92:28\">0.0～2.0</td>\n<td data-sourcepos=\"92:30-92:30\">1</td>\n<td data-sourcepos=\"92:32-92:109\">出力の創造性やランダム性を制御する。(<code>top_p</code>と併用不可)</td>\n</tr>\n<tr data-sourcepos=\"93:1-93:119\">\n<td data-sourcepos=\"93:2-93:6\">top_p</td>\n<td data-sourcepos=\"93:8-93:12\">float</td>\n<td data-sourcepos=\"93:14-93:22\">0.0～1.0</td>\n<td data-sourcepos=\"93:24-93:24\">1</td>\n<td data-sourcepos=\"93:26-93:118\">生成される単語の選択肢の多様性を制御する。(<code>temperature</code>と併用不可)</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"95:1-95:84\">例えば下記のパラメータを変えて出力結果を比較してみます。</p>\n<div class=\"code-frame\" data-lang=\"markdown\" data-sourcepos=\"97:1-107:3\"><div class=\"highlight\"><pre><code><span class=\"gh\"># 指示</span>\nChatGPTの良いところを箇条書きで出力してください\n\n<span class=\"gh\"># パラメータ</span>\n<span class=\"p\">-</span> frequency_penalty:0.0\n<span class=\"p\">-</span> max_tokens:10\n<span class=\"p\">-</span> n:5\n<span class=\"p\">-</span> presence_penalty:0.0\n<span class=\"p\">-</span> temperature:0.0\n</code></pre></div></div>\n<p data-sourcepos=\"109:1-109:121\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2Fabb1a635-12d1-2ac7-0410-9c8228ae67c9.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=880fb539d1ff1d5f22603ca5e42ee6c6\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2Fabb1a635-12d1-2ac7-0410-9c8228ae67c9.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=880fb539d1ff1d5f22603ca5e42ee6c6\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2Fabb1a635-12d1-2ac7-0410-9c8228ae67c9.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=a47107b6d5144e083b3ad3331eb46a78 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/abb1a635-12d1-2ac7-0410-9c8228ae67c9.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"111:1-111:195\"><code>max_tokens</code>による出力文字列の変化(10→100)と、<code>n</code>による生成されるリスト数の変化(5個→10個)についてはわかりやすく出力結果が変化しています。</p>\n<div data-sourcepos=\"113:1-117:3\" class=\"note info\">\n<span class=\"fa fa-fw fa-check-circle\"></span><div>\n<p data-sourcepos=\"115:1-115:278\">Azure OpenAIの情報になりますが、<a href=\"https://learn.microsoft.com/ja-jp/azure/ai-services/openai/how-to/latency\" rel=\"nofollow noopener\" target=\"_blank\">パフォーマンスチューニング</a>でもパラメータの扱いについて言及しているので、興味がある方は調べてみてください。</p>\n</div>\n</div>\n<h3 data-sourcepos=\"119:1-119:34\">\n<span id=\"入出力の上限について\" class=\"fragment\"></span><a href=\"#%E5%85%A5%E5%87%BA%E5%8A%9B%E3%81%AE%E4%B8%8A%E9%99%90%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"><i class=\"fa fa-link\"></i></a>入出力の上限について</h3>\n<p data-sourcepos=\"121:1-122:178\">ChatGPTには使用しているプランやモデルによって入出力する画像サイズや文字数に制限があります。<br>\nこれらはトークンという単位に分割されてカウントされますが、OpenAIのPricingにも記載(<code>Context window</code>)があるので参考にしてください。</p>\n<p data-sourcepos=\"124:1-124:35\"><iframe id=\"qiita-embed-content__edc67fdd0d5d7e9ffa4b466388aaf85e\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__edc67fdd0d5d7e9ffa4b466388aaf85e\" data-content=\"https%3A%2F%2Fopenai.com%2Fchatgpt%2Fpricing%2F\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<p data-sourcepos=\"126:1-126:121\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F5a97eb5b-da19-fada-1541-95c68490e50d.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=737cb653c0832c7eba0700d2d57a509d\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F5a97eb5b-da19-fada-1541-95c68490e50d.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=737cb653c0832c7eba0700d2d57a509d\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F5a97eb5b-da19-fada-1541-95c68490e50d.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=aeb0b385baa6e8d24cc6b79f930bba39 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/5a97eb5b-da19-fada-1541-95c68490e50d.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"128:1-128:150\">また、具体的に入力する文字列がいくつのトークンになるのかカウントするサイトをOpenAIでも用意しています。</p>\n<p data-sourcepos=\"130:1-130:37\"><iframe id=\"qiita-embed-content__de2c8b24ff3b3e8c9fd701a18bf7fc74\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__de2c8b24ff3b3e8c9fd701a18bf7fc74\" data-content=\"https%3A%2F%2Fplatform.openai.com%2Ftokenizer\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<div data-sourcepos=\"132:1-135:3\" class=\"note info\">\n<span class=\"fa fa-fw fa-check-circle\"></span><div>\n<p data-sourcepos=\"133:1-134:278\">OpenAIではTokenizerのコードを<a href=\"https://github.com/openai/tiktoken/blob/main/README.md\" rel=\"nofollow noopener\" target=\"_blank\">Github</a>で公開しています。<br>\nまた、<a href=\"https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\" rel=\"nofollow noopener\" target=\"_blank\">cookbook</a>や<a href=\"https://community.openai.com/t/chatgpt-api-maximum-token/83321\" rel=\"nofollow noopener\" target=\"_blank\">OpenAI 開発者フォーラム</a>でも言及されているので、興味のある方は調べてみてください。</p>\n</div>\n</div>\n<h3 data-sourcepos=\"137:1-137:37\">\n<span id=\"プロンプトテクニック集\" class=\"fragment\"></span><a href=\"#%E3%83%97%E3%83%AD%E3%83%B3%E3%83%97%E3%83%88%E3%83%86%E3%82%AF%E3%83%8B%E3%83%83%E3%82%AF%E9%9B%86\"><i class=\"fa fa-link\"></i></a>プロンプトテクニック集</h3>\n<p data-sourcepos=\"139:1-139:117\">OpenAIから精度の高いプロンプトの書き方としてPrompt Engineering Guideが公開されています。</p>\n<p data-sourcepos=\"141:1-141:58\"><iframe id=\"qiita-embed-content__5012700394c006aa760acce104749a56\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__5012700394c006aa760acce104749a56\" data-content=\"https%3A%2F%2Fplatform.openai.com%2Fdocs%2Fguides%2Fprompt-engineering\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<table data-sourcepos=\"143:1-150:276\">\n<thead>\n<tr data-sourcepos=\"143:1-143:15\">\n<th data-sourcepos=\"143:2-143:7\">項目</th>\n<th data-sourcepos=\"143:9-143:14\">概要</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"145:1-145:231\">\n<td data-sourcepos=\"145:2-145:25\">明確な指示を書く</td>\n<td data-sourcepos=\"145:27-145:230\">プロンプトには具体的で明確な指示を含めることが重要です。これにより、モデルがタスクを正確に理解し、期待される応答を生成しやすくなります。</td>\n</tr>\n<tr data-sourcepos=\"146:1-146:279\">\n<td data-sourcepos=\"146:2-146:34\">参照テキストを提供する</td>\n<td data-sourcepos=\"146:36-146:278\">モデルに関連する背景情報や参照テキストを提供することで、より関連性の高い応答を引き出すことができます。これにより、モデルの理解度が向上し、適切な応答が得られます。</td>\n</tr>\n<tr data-sourcepos=\"147:1-147:255\">\n<td data-sourcepos=\"147:2-147:67\">複雑なタスクをより単純なサブタスクに分割する</td>\n<td data-sourcepos=\"147:69-147:254\">複雑なタスクをより単純なサブタスクに分割することで、モデルが各ステップを順序立てて処理しやすくなり、全体の精度が向上します。</td>\n</tr>\n<tr data-sourcepos=\"148:1-148:225\">\n<td data-sourcepos=\"148:2-148:46\">モデルに「考える」時間を与える</td>\n<td data-sourcepos=\"148:48-148:224\">プロンプト内で「考える」時間を与える表現を使用することで、モデルがより詳細で深い応答を生成するための時間を確保します。</td>\n</tr>\n<tr data-sourcepos=\"149:1-149:201\">\n<td data-sourcepos=\"149:2-149:31\">外部ツールを使用する</td>\n<td data-sourcepos=\"149:33-149:200\">必要に応じて外部ツールやデータベースを活用することで、モデルの能力を補完し、より高度な応答を得ることができます。</td>\n</tr>\n<tr data-sourcepos=\"150:1-150:276\">\n<td data-sourcepos=\"150:2-150:37\">変更を体系的にテストする</td>\n<td data-sourcepos=\"150:39-150:275\">プロンプトの効果を評価するために、変更を体系的にテストし、結果を分析して最適化を図ることが重要です。これにより、モデルのパフォーマンスを継続的に改善できます。</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"153:1-153:159\">また、Azure OpenAIになりますが、プロンプトエンジニアリング手法が公開されており、現在でも随時更新されています。</p>\n<p data-sourcepos=\"155:1-155:140\"><iframe id=\"qiita-embed-content__e82cb75132d0a413d4d1b659fed23d20\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__e82cb75132d0a413d4d1b659fed23d20\" data-content=\"https%3A%2F%2Flearn.microsoft.com%2Fja-jp%2Fazure%2Fai-services%2Fopenai%2Fconcepts%2Fadvanced-prompt-engineering%3Fpivots%3Dprogramming-language-chat-completions\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<table data-sourcepos=\"157:1-171:219\">\n<thead>\n<tr data-sourcepos=\"157:1-157:15\">\n<th data-sourcepos=\"157:2-157:7\">項目</th>\n<th data-sourcepos=\"157:9-157:14\">概要</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"159:1-159:186\">\n<td data-sourcepos=\"159:2-159:28\">システムメッセージ</td>\n<td data-sourcepos=\"159:30-159:185\">システムメッセージは、会話のトーンやスタイルを設定し、モデルに全体的な指示を提供するために使用されます。</td>\n</tr>\n<tr data-sourcepos=\"160:1-160:189\">\n<td data-sourcepos=\"160:2-160:25\">少数ショット学習</td>\n<td data-sourcepos=\"160:27-160:188\">少数の例を提供することで、モデルが特定のタスクを学習し、類似の要求に対して正確に応答する能力を向上させます。</td>\n</tr>\n<tr data-sourcepos=\"161:1-161:204\">\n<td data-sourcepos=\"161:2-161:34\">チャット以外のシナリオ</td>\n<td data-sourcepos=\"161:36-161:203\">チャット以外のユースケースでも、プロンプトエンジニアリングを活用してモデルの応答品質を向上させることができます。</td>\n</tr>\n<tr data-sourcepos=\"162:1-162:201\">\n<td data-sourcepos=\"162:2-162:31\">明確な命令から始める</td>\n<td data-sourcepos=\"162:33-162:200\">プロンプトを明確な命令で始めることで、モデルが期待されるタスクを正確に理解し、適切な応答を生成しやすくなります。</td>\n</tr>\n<tr data-sourcepos=\"163:1-163:189\">\n<td data-sourcepos=\"163:2-163:31\">最後に命令を繰り返す</td>\n<td data-sourcepos=\"163:33-163:188\">重要な命令をプロンプトの最後に繰り返すことで、モデルがその指示を見落とすことなく従う可能性が高まります。</td>\n</tr>\n<tr data-sourcepos=\"164:1-164:174\">\n<td data-sourcepos=\"164:2-164:28\">出力を事前処理する</td>\n<td data-sourcepos=\"164:30-164:173\">出力形式を指定することで、モデルが一貫したフォーマットで応答を提供するように導くことができます。</td>\n</tr>\n<tr data-sourcepos=\"165:1-165:168\">\n<td data-sourcepos=\"165:2-165:31\">明確な構文を追加する</td>\n<td data-sourcepos=\"165:33-165:167\">プロンプトに明確な構文を追加することで、モデルの理解を助け、望ましい応答を得やすくします。</td>\n</tr>\n<tr data-sourcepos=\"166:1-166:177\">\n<td data-sourcepos=\"166:2-166:25\">タスクを中断する</td>\n<td data-sourcepos=\"166:27-166:176\">タスクが複雑な場合、モデルにタスクの進捗を確認させることで、誤った方向に進むのを防ぐことができます。</td>\n</tr>\n<tr data-sourcepos=\"167:1-167:162\">\n<td data-sourcepos=\"167:2-167:31\">アフォーダンスの使用</td>\n<td data-sourcepos=\"167:33-167:161\">アフォーダンスを使用して、モデルに特定のアクションや応答を誘導する手法を取り入れます。</td>\n</tr>\n<tr data-sourcepos=\"168:1-168:177\">\n<td data-sourcepos=\"168:2-168:40\">思考の連鎖プロンプティング</td>\n<td data-sourcepos=\"168:42-168:176\">モデルが複雑な問題を解決するために、段階的に考えるプロセスを促すプロンプティング技法です。</td>\n</tr>\n<tr data-sourcepos=\"169:1-169:150\">\n<td data-sourcepos=\"169:2-169:22\">出力構造の指定</td>\n<td data-sourcepos=\"169:24-169:149\">出力の形式や構造を明確に指定することで、モデルが一貫した応答を生成するようにします。</td>\n</tr>\n<tr data-sourcepos=\"170:1-170:185\">\n<td data-sourcepos=\"170:2-170:60\">\n<code>temperature</code>パラメーターと<code>Top_p</code>パラメーター</td>\n<td data-sourcepos=\"170:62-170:184\">これらのパラメーターを調整することで、モデルの応答の多様性や創造性を制御できます。</td>\n</tr>\n<tr data-sourcepos=\"171:1-171:219\">\n<td data-sourcepos=\"171:2-171:49\">根拠付けるコンテキストを提供する</td>\n<td data-sourcepos=\"171:51-171:218\">モデルが応答を生成する際に必要な背景情報や根拠を提供することで、より正確で関連性の高い応答を得ることができます。</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"173:1-173:94\"><iframe id=\"qiita-embed-content__57cbab2d2cee47827e69bc9a4e3a4bea\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__57cbab2d2cee47827e69bc9a4e3a4bea\" data-content=\"https%3A%2F%2Flearn.microsoft.com%2Fja-jp%2Fazure%2Fai-services%2Fopenai%2Fconcepts%2Fgpt-4-v-prompt-engineering\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<table data-sourcepos=\"175:1-183:228\">\n<thead>\n<tr data-sourcepos=\"175:1-175:15\">\n<th data-sourcepos=\"175:2-175:7\">項目</th>\n<th data-sourcepos=\"175:9-175:14\">概要</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"177:1-177:219\">\n<td data-sourcepos=\"177:2-177:34\">コンテキスト的な特異性</td>\n<td data-sourcepos=\"177:36-177:218\">特定の文脈を提示することで、モデルがより関連性の高い応答を生成します。背景情報や具体的な状況を明確に伝えることが重要です。</td>\n</tr>\n<tr data-sourcepos=\"178:1-178:222\">\n<td data-sourcepos=\"178:2-178:34\">タスク指向のプロンプト</td>\n<td data-sourcepos=\"178:36-178:221\">明確な指示を含むプロンプトを使用して、モデルが特定のタスクを実行するように導きます。具体的な行動を指示することが効果的です。</td>\n</tr>\n<tr data-sourcepos=\"179:1-179:201\">\n<td data-sourcepos=\"179:2-179:16\">拒否の処理</td>\n<td data-sourcepos=\"179:18-179:200\">モデルが不適切な応答を生成しないように、拒否するべき内容や行動を明示します。不適切なリクエストを事前に防ぐことが目的です。</td>\n</tr>\n<tr data-sourcepos=\"180:1-180:165\">\n<td data-sourcepos=\"180:2-180:13\">例の追加</td>\n<td data-sourcepos=\"180:15-180:164\">望ましい出力の例をプロンプトに含めることで、モデルが求められる応答の形式や内容を理解しやすくします。</td>\n</tr>\n<tr data-sourcepos=\"181:1-181:219\">\n<td data-sourcepos=\"181:2-181:46\">プロンプトのチューニングを試す</td>\n<td data-sourcepos=\"181:48-181:218\">プロンプトの内容や形式を調整し、モデルの応答を最適化します。異なるアプローチを試し、最適なプロンプトを見つけます。</td>\n</tr>\n<tr data-sourcepos=\"182:1-182:192\">\n<td data-sourcepos=\"182:2-182:22\">要求を分割する</td>\n<td data-sourcepos=\"182:24-182:191\">複雑な要求をシンプルなサブタスクに分割し、段階的に指示を出すことで、モデルがより正確な応答を生成しやすくします。</td>\n</tr>\n<tr data-sourcepos=\"183:1-183:228\">\n<td data-sourcepos=\"183:2-183:28\">出力形式を定義する</td>\n<td data-sourcepos=\"183:30-183:227\">モデルの応答形式を明示することで、出力が一貫したフォーマットで提供されるようにします。例えば、リスト形式や箇条書きなどを指定します。</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"185:1-185:138\">具体例も記載されており、比較的使いやすい形にまとまっているので、よければ参考にしてください。</p>\n<h2 data-sourcepos=\"187:1-187:33\">\n<span id=\"マルチモーダルの活用\" class=\"fragment\"></span><a href=\"#%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AE%E6%B4%BB%E7%94%A8\"><i class=\"fa fa-link\"></i></a>マルチモーダルの活用</h2>\n<h3 data-sourcepos=\"189:1-189:40\">\n<span id=\"質問内容に画像を使用する\" class=\"fragment\"></span><a href=\"#%E8%B3%AA%E5%95%8F%E5%86%85%E5%AE%B9%E3%81%AB%E7%94%BB%E5%83%8F%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B\"><i class=\"fa fa-link\"></i></a>質問内容に画像を使用する</h3>\n<p data-sourcepos=\"191:1-191:154\">意外と使われていない手法ですが、ChatGPTはマルチモーダルに対応しているため、画像を入力として使用できます。</p>\n<p data-sourcepos=\"193:1-193:150\">例えばQiitaの通知アイコンについて質問する際も、画面キャプチャを使用してChatGPTに質問することができます。</p>\n<p data-sourcepos=\"195:1-195:121\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F2976e0f2-9d0c-7864-6380-69d5f5e13324.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=71c0b39a5df01086f102ff199da7f1cf\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F2976e0f2-9d0c-7864-6380-69d5f5e13324.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=71c0b39a5df01086f102ff199da7f1cf\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F2976e0f2-9d0c-7864-6380-69d5f5e13324.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=9848aa00ba60a22f15870b7bcb38801d 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/2976e0f2-9d0c-7864-6380-69d5f5e13324.png\" loading=\"lazy\"></a></p>\n<div data-sourcepos=\"197:1-205:3\" class=\"note info\">\n<span class=\"fa fa-fw fa-check-circle\"></span><div>\n<p data-sourcepos=\"198:1-199:121\">ファイルを入力に使用するには、入力エリアの添付ファイルアイコンをクリックしてファイルを選択します。<br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F88554c60-99c9-d1a1-6905-2049ddec971f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=6b5d6a90028bbccb8104848280990e46\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F88554c60-99c9-d1a1-6905-2049ddec971f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=6b5d6a90028bbccb8104848280990e46\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F88554c60-99c9-d1a1-6905-2049ddec971f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=918b9ae5b3c2f1b1f03e946a5185a92d 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/88554c60-99c9-d1a1-6905-2049ddec971f.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"201:1-201:279\">また、画面キャプチャなどクリップボードにファイルが格納されている場合は、入力エリアにカーソルを当てた後に貼り付け操作(Command + v / Ctrl + v)を行う事で簡単にファイルを入力に使用する事ができます。</p>\n<p data-sourcepos=\"203:1-203:121\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F61a4f2ff-68ab-5393-4ccc-2da81f95e0b6.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=bcc4580afef548de43fe5ce4a47e279e\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F61a4f2ff-68ab-5393-4ccc-2da81f95e0b6.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=bcc4580afef548de43fe5ce4a47e279e\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F61a4f2ff-68ab-5393-4ccc-2da81f95e0b6.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=c119523c74722c06d72a95b6b2f44778 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/61a4f2ff-68ab-5393-4ccc-2da81f95e0b6.png\" loading=\"lazy\"></a></p>\n</div>\n</div>\n<h3 data-sourcepos=\"207:1-207:45\">\n<span id=\"画像の文字認識ocrを使用する\" class=\"fragment\"></span><a href=\"#%E7%94%BB%E5%83%8F%E3%81%AE%E6%96%87%E5%AD%97%E8%AA%8D%E8%AD%98ocr%E3%82%92%E4%BD%BF%E7%94%A8%E3%81%99%E3%82%8B\"><i class=\"fa fa-link\"></i></a>画像の文字認識(OCR)を使用する</h3>\n<p data-sourcepos=\"209:1-209:138\">ChatGPTの日本語の文字認識(OCR)の精度は意外と高く、画像に含まれる文字列を認識させる事ができます。</p>\n<p data-sourcepos=\"211:1-211:105\">わかりやすい例として、画像に含まれる日本語の文字起こしに使用できます。</p>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4c72df1a-50e2-6053-6188-65e8fc9db18e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=d4ac757826614ef4d0684ece09c7e525\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4c72df1a-50e2-6053-6188-65e8fc9db18e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=d4ac757826614ef4d0684ece09c7e525\" width=\"400\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4c72df1a-50e2-6053-6188-65e8fc9db18e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=66b3c769ded41c0387dcc2fdea783d60 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/4c72df1a-50e2-6053-6188-65e8fc9db18e.png\" loading=\"lazy\"></a>\n<p data-sourcepos=\"215:1-215:372\">また、まれに日本語を読み取れない時がありますが、そう言った時は<code>OCRで日本語の文字列を読み取ってください</code>と明示的にお願いするか、ブレを固定したい場合は日本語のトレーニングデータをアップロードしてAdvanced Data Analysisで強制的に日本語を解析させる事もできます。</p>\n<p data-sourcepos=\"217:1-217:122\">下記の記事の<code>日本語をトレーニングする</code>で紹介していますので、よければご覧ください。</p>\n<p data-sourcepos=\"219:1-219:161\"><iframe id=\"qiita-embed-content__428b3d62ae2e846906d79fbdd44b38a1\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__428b3d62ae2e846906d79fbdd44b38a1\" data-content=\"https%3A%2F%2Fqiita.com%2Fb-mente%2Fitems%2Fb15011ab3b6c3830e732%23%25E6%2597%25A5%25E6%259C%25AC%25E8%25AA%259E%25E3%2582%2592%25E3%2583%2588%25E3%2583%25AC%25E3%2583%25BC%25E3%2583%258B%25E3%2583%25B3%25E3%2582%25B0%25E3%2581%2599%25E3%2582%258B\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<h3 data-sourcepos=\"221:1-221:49\">\n<span id=\"画像からソースコードを生成する\" class=\"fragment\"></span><a href=\"#%E7%94%BB%E5%83%8F%E3%81%8B%E3%82%89%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%B3%E3%83%BC%E3%83%89%E3%82%92%E7%94%9F%E6%88%90%E3%81%99%E3%82%8B\"><i class=\"fa fa-link\"></i></a>画像からソースコードを生成する</h3>\n<p data-sourcepos=\"223:1-224:123\">上述したように画像の文字列を認識させる事ができるため、出来ることの幅が大きく広がります。<br>\n例えば説明文を入れた画像から内容を読み取らせ、ソースコードを生成する事もできます。</p>\n<p data-sourcepos=\"226:1-226:196\">下記は説明付きWebサイトのスケッチ画像からソースコードを生成する例ですが、これだけザックリした指示でもちゃんとHTMLを出力してくれます。</p>\n<p data-sourcepos=\"228:1-228:121\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2Fdb4dcf6a-7dd2-4f0d-3b0c-fa4ce4926af9.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=3f6a354ff29ba68715d6fabeb5718df5\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2Fdb4dcf6a-7dd2-4f0d-3b0c-fa4ce4926af9.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=3f6a354ff29ba68715d6fabeb5718df5\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2Fdb4dcf6a-7dd2-4f0d-3b0c-fa4ce4926af9.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=5e1be56479117d4f53838da1a1c2b927 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/db4dcf6a-7dd2-4f0d-3b0c-fa4ce4926af9.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"230:1-230:121\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2Ffaf8be9f-a27d-d7c9-e8fc-191a839ccbb5.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=c203c0ac089e79ec4323440f39db3276\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2Ffaf8be9f-a27d-d7c9-e8fc-191a839ccbb5.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=c203c0ac089e79ec4323440f39db3276\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2Ffaf8be9f-a27d-d7c9-e8fc-191a839ccbb5.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=511dec2bba557dd7fb6b7e54ac13ddff 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/faf8be9f-a27d-d7c9-e8fc-191a839ccbb5.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"232:1-232:185\">また、領域名だけ画像に記載し、プロンプトに各領域に対する説明をMarkdownで記載することで細かな設定や動作を表現する事もできます。</p>\n<h3 data-sourcepos=\"234:1-234:34\">\n<span id=\"umlsysml画像を解析する\" class=\"fragment\"></span><a href=\"#umlsysml%E7%94%BB%E5%83%8F%E3%82%92%E8%A7%A3%E6%9E%90%E3%81%99%E3%82%8B\"><i class=\"fa fa-link\"></i></a>UML/SysML画像を解析する</h3>\n<p data-sourcepos=\"236:1-236:171\">開発で使用しているシーケンス図などのUMLやSysMLの画像をChatGPTに入力し、解析やその後の処理や出力に利用する事ができます。</p>\n<p data-sourcepos=\"238:1-238:196\">下記はシーケンス画像から内容の解説やMermaid記法に出力をしている例ですが、こちらも応用の幅が非常に広いので、ぜひ活用してみてください。</p>\n<p data-sourcepos=\"240:1-240:121\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4f759acb-828e-1de9-b263-295eaa075f50.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=7d2153bfb1641b01fde3f0812411b528\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4f759acb-828e-1de9-b263-295eaa075f50.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=7d2153bfb1641b01fde3f0812411b528\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4f759acb-828e-1de9-b263-295eaa075f50.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=2f43d68a03f556df7a5cf4d1503b3dc0 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/4f759acb-828e-1de9-b263-295eaa075f50.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"242:1-242:121\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F6db72f71-22ca-96cb-f2f7-523752241018.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=ebd255127d5a1953de4fdfa6effc203d\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F6db72f71-22ca-96cb-f2f7-523752241018.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=ebd255127d5a1953de4fdfa6effc203d\" alt=\"image.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F6db72f71-22ca-96cb-f2f7-523752241018.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=d42d1583fea47aa561d87fc8f1048d8c 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/6db72f71-22ca-96cb-f2f7-523752241018.png\" loading=\"lazy\"></a></p>\n<ul data-sourcepos=\"244:1-244:30\">\n<li data-sourcepos=\"244:1-244:30\">mermaid記法の出力結果</li>\n</ul>\n<iframe id=\"qiita-embed-content__a092a4140041a8d487a97db38552402d\" src=\"https://qiita.com/embed-contents/mermaid#qiita-embed-content__a092a4140041a8d487a97db38552402d\" style=\"width:100%;\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" data-content='{\"data\":\"sequenceDiagram\\n participant User\\n participant WebServer\\n participant AppServer\\n participant DB\\n \\n User-&gt;&gt;WebServer: HttpRequest\\n WebServer-&gt;&gt;AppServer: Forwarding Request\\n AppServer-&gt;&gt;DB: Data Inquiry\\n DB--&gt;&gt;AppServer: Inquiry Results\\n AppServer--&gt;&gt;WebServer: Processing Result\\n WebServer--&gt;&gt;User: HttpResponse\",\"key\":\"d77a84b9605f22bb74d75b7ae104bb49\"}'>\n</iframe>\n\n<p data-sourcepos=\"260:1-260:139\">また、下記でUML/SysML画像からMermaid記法で出力する方法を紹介していますので、よければご覧ください。</p>\n<p data-sourcepos=\"262:1-262:52\"><iframe id=\"qiita-embed-content__1c42dfbc9cb6bd7846940560a0416b9b\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__1c42dfbc9cb6bd7846940560a0416b9b\" data-content=\"https%3A%2F%2Fqiita.com%2Fb-mente%2Fitems%2Fb15011ab3b6c3830e732\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<h2 data-sourcepos=\"264:1-264:34\">\n<span id=\"advanced-data-analysisの活用\" class=\"fragment\"></span><a href=\"#advanced-data-analysis%E3%81%AE%E6%B4%BB%E7%94%A8\"><i class=\"fa fa-link\"></i></a>Advanced Data Analysisの活用</h2>\n<p data-sourcepos=\"266:1-267:246\">ChatGPTにはAdvanced Data Analysis(旧Code Interpreter)というPythonの実行環境が用意されています。<br>\nよく使用される例としてPDFやエクセル、CSVなどのデータ解析や、テキストやファイル操作などが挙げられますが、短時間で実行可能なPythonスクリプトであれば大体なんでもできます。</p>\n<p data-sourcepos=\"269:1-269:70\"><iframe id=\"qiita-embed-content__21ce61afb626c3a4a5cbf43af5b09d85\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__21ce61afb626c3a4a5cbf43af5b09d85\" data-content=\"https%3A%2F%2Fhelp.openai.com%2Fen%2Farticles%2F8437071-data-analysis-with-chatgpt\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<h3 data-sourcepos=\"271:1-271:16\">\n<span id=\"各種制限\" class=\"fragment\"></span><a href=\"#%E5%90%84%E7%A8%AE%E5%88%B6%E9%99%90\"><i class=\"fa fa-link\"></i></a>各種制限</h3>\n<p data-sourcepos=\"273:1-273:183\">ChatGPT本体やAdvanced Data Analysisには下記の制限がかけられていますので、この制限の中でAdvanced Data Analysisを実行していく必要があります。</p>\n<ul data-sourcepos=\"275:1-287:0\">\n<li data-sourcepos=\"275:1-276:68\">実行時間制限\n<ul data-sourcepos=\"276:5-276:68\">\n<li data-sourcepos=\"276:5-276:68\">一度のコード実行にかかる時間は通常60秒以内</li>\n</ul>\n</li>\n<li data-sourcepos=\"277:1-278:25\">メモリ使用量制限\n<ul data-sourcepos=\"278:5-278:25\">\n<li data-sourcepos=\"278:5-278:25\">1.5GB〜2GB(目安)</li>\n</ul>\n</li>\n<li data-sourcepos=\"279:1-280:166\">扱えるファイルの種類\n<ul data-sourcepos=\"280:5-280:166\">\n<li data-sourcepos=\"280:5-280:166\">テキスト ファイル、スプレッドシート、プレゼンテーション、ドキュメントによく使用されるすべてのファイル拡張子</li>\n</ul>\n</li>\n<li data-sourcepos=\"281:1-287:0\">アップロードの制限\n<ul data-sourcepos=\"282:5-287:0\">\n<li data-sourcepos=\"282:5-282:26\">最大20ファイル</li>\n<li data-sourcepos=\"283:5-287:0\">1ファイルあたり最大512MB\n<ul data-sourcepos=\"284:9-287:0\">\n<li data-sourcepos=\"284:9-284:82\">テキストやドキュメントファイル：200万トークン以内</li>\n<li data-sourcepos=\"285:9-285:65\">スプレッドシートやCSVファイル：最大50MB</li>\n<li data-sourcepos=\"286:9-287:0\">画像：最大20MB</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p data-sourcepos=\"288:1-288:60\"><iframe id=\"qiita-embed-content__646970c80c423a18fe434c79328bf42d\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__646970c80c423a18fe434c79328bf42d\" data-content=\"https%3A%2F%2Fhelp.openai.com%2Fen%2Farticles%2F8555545-file-uploads-faq\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<div data-sourcepos=\"290:1-300:3\" class=\"note info\">\n<span class=\"fa fa-fw fa-check-circle\"></span><div>\n<p data-sourcepos=\"292:1-292:228\">扱えるファイルの種類がザックリしていますが、<a href=\"https://platform.openai.com/docs/assistants/tools/code-interpreter\" rel=\"nofollow noopener\" target=\"_blank\">AssistantsのCode Interpreter</a>に記載しているファイルは使用できそうです。</p>\n<div class=\"code-frame\" data-lang=\"\" data-sourcepos=\"294:1-298:3\">\n<div class=\"code-lang\"><span class=\"bold\">ファイルの種類</span></div>\n<div class=\"highlight\"><pre><code>.c, .cs, .cpp, .doc, .docx, .html, .java, .json, .md, .pdf, .php,\n.pptx, .py, .py, .rb, .tex, .txt, .css, .js, .sh, .ts, .csv, .jpeg,\n.jpg, .gif, .png, .tar, .xlsx, .xml, .zip\n</code></pre></div>\n</div>\n</div>\n</div>\n<h3 data-sourcepos=\"302:1-302:25\">\n<span id=\"画像を加工する\" class=\"fragment\"></span><a href=\"#%E7%94%BB%E5%83%8F%E3%82%92%E5%8A%A0%E5%B7%A5%E3%81%99%E3%82%8B\"><i class=\"fa fa-link\"></i></a>画像を加工する</h3>\n<p data-sourcepos=\"304:1-304:177\">特に画像加工系などはデフォルトでパッケージやライブラリが用意されているので、文章による指示でも加工を行なってくれます。</p>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4f0487b1-6a6a-13e7-ffb3-fc2466523df3.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=a067f92fa42e4207620c34e3369291a7\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4f0487b1-6a6a-13e7-ffb3-fc2466523df3.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=a067f92fa42e4207620c34e3369291a7\" width=\"400\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F4f0487b1-6a6a-13e7-ffb3-fc2466523df3.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=b2e7dfae0f79e03cf218c395f70343eb 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/4f0487b1-6a6a-13e7-ffb3-fc2466523df3.png\" loading=\"lazy\"></a>\n<ul data-sourcepos=\"308:1-319:0\">\n<li data-sourcepos=\"308:1-319:0\">PIL(Pillow)、cv2(OpenCV)\n<ul data-sourcepos=\"309:5-319:0\">\n<li data-sourcepos=\"309:5-309:84\">リサイズやトリミングなどの基本的な処理を行いたい場合</li>\n<li data-sourcepos=\"310:5-310:21\">画像を合成</li>\n<li data-sourcepos=\"311:5-311:42\">複数の画像を連結（結合）</li>\n<li data-sourcepos=\"312:5-312:30\">透過png画像を作成</li>\n<li data-sourcepos=\"313:5-313:48\">ネガポジ反転（画素値を逆転）</li>\n<li data-sourcepos=\"314:5-314:54\">円形や正方形のサムネイル画像作成</li>\n<li data-sourcepos=\"315:5-315:18\">図形描画</li>\n<li data-sourcepos=\"316:5-316:36\">アニメーションGif作成</li>\n<li data-sourcepos=\"317:5-317:30\">顔検出（顔認識）</li>\n<li data-sourcepos=\"318:5-319:0\">モザイク処理</li>\n</ul>\n</li>\n</ul>\n<p data-sourcepos=\"320:1-320:94\">また、<code>qrcode</code>が用意されているので、QRコードの生成なども行えます。</p>\n<div class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"322:1-326:3\"><div class=\"highlight\"><pre><code>qrcodeライブラリを使用して下記URLのQRコードを生成してください。\n\nhttps://www.google.com/\n</code></pre></div></div>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F3467c262-6783-a1f1-1df3-b45e41a99f14.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=8c050afc7d842701fa5ab8d5e190075c\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F3467c262-6783-a1f1-1df3-b45e41a99f14.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=8c050afc7d842701fa5ab8d5e190075c\" width=\"400\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F3467c262-6783-a1f1-1df3-b45e41a99f14.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=45d58fa35c3318aa7249796e5bf09bee 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/3467c262-6783-a1f1-1df3-b45e41a99f14.png\" loading=\"lazy\"></a>\n<div data-sourcepos=\"329:1-331:3\" class=\"note warn\">\n<span class=\"fa fa-fw fa-exclamation-circle\"></span><div>\n<p data-sourcepos=\"330:1-330:257\">svgwriteが用意されているのでSVGも出力できますが、PNGなどの画像をSVGに変換するにはsvgwrite + cv2だけではなく、matplotlibなどの化学計算用の画像ライブラリと組み合わせる必要がありそうです。</p>\n</div>\n</div>\n<h3 data-sourcepos=\"333:1-333:52\">\n<span id=\"デフォルトで使用できるパッケージ\" class=\"fragment\"></span><a href=\"#%E3%83%87%E3%83%95%E3%82%A9%E3%83%AB%E3%83%88%E3%81%A7%E4%BD%BF%E7%94%A8%E3%81%A7%E3%81%8D%E3%82%8B%E3%83%91%E3%83%83%E3%82%B1%E3%83%BC%E3%82%B8\"><i class=\"fa fa-link\"></i></a>デフォルトで使用できるパッケージ</h3>\n<p data-sourcepos=\"335:1-335:102\">下記を入力すると、現在使用できるパッケージの一覧を表示してくれます。</p>\n<div class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"337:1-348:3\"><div class=\"highlight\"><pre><code># 指示\nCodeInterpreterでPythonコードを実行してください\n\n# Pythonコード\n```\nimport pkg_resources\n\ninstalled_packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in pkg_resources.working_set])\ninstalled_packages_list\n```\n</code></pre></div></div>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F3c57299b-753c-fe98-e73f-2d5652a2c37e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=aa6363b7486b7643b7a1dd3e6e5795fd\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F3c57299b-753c-fe98-e73f-2d5652a2c37e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=aa6363b7486b7643b7a1dd3e6e5795fd\" width=\"400\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F699056%2F3c57299b-753c-fe98-e73f-2d5652a2c37e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=736cc34e3669cb77d26641db8033b0bc 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/3c57299b-753c-fe98-e73f-2d5652a2c37e.png\" loading=\"lazy\"></a>\n<details><summary>パッケージ一覧</summary><div>\n<div class=\"code-frame\" data-lang=\"text\" data-sourcepos=\"354:1-711:3\"><div class=\"highlight\"><pre><code>'absl-py==2.1.0'\n'ace-tools==0.0.1'\n'aeppl==0.0.31'\n'aesara==2.7.3'\n'affine==2.4.0'\n'aiohttp==3.8.6'\n'aiosignal==1.3.1'\n'analytics-python==1.4.post1'\n'anyio==3.7.1'\n'anytree==2.8.0'\n'argon2-cffi-bindings==21.2.0'\n'argon2-cffi==23.1.0'\n'arviz==0.17.1'\n'asn1crypto==1.5.1'\n'asttokens==2.4.1'\n'async-timeout==4.0.3'\n'attrs==23.2.0'\n'audioread==3.0.1'\n'babel==2.14.0'\n'backoff==1.10.0'\n'basemap-data==1.3.2'\n'basemap==1.3.9'\n'bcrypt==4.1.2'\n'beautifulsoup4==4.12.3'\n'bleach==6.1.0'\n'blinker==1.7.0'\n'blis==0.7.11'\n'blosc2==2.0.0'\n'bokeh==2.4.0'\n'branca==0.7.1'\n'brotli==1.1.0'\n'cachetools==5.3.3'\n'cairocffi==1.6.1'\n'cairosvg==2.5.2'\n'camelot-py==0.10.1'\n'catalogue==2.0.10'\n'certifi==2024.2.2'\n'cffi==1.16.0'\n'chardet==3.0.4'\n'charset-normalizer==2.1.1'\n'click-plugins==1.1.1'\n'click==8.1.7'\n'cligj==0.7.2'\n'cloudpickle==3.0.0'\n'cmake==3.28.3'\n'cmudict==1.0.21'\n'comm==0.2.2'\n'confection==0.1.4'\n'cons==0.4.6'\n'contourpy==1.2.0'\n'countryinfo==0.1.2'\n'cryptography==3.4.8'\n'cssselect2==0.7.0'\n'cycler==0.12.1'\n'cymem==2.0.8'\n'cython==0.29.36'\n'databricks-sql-connector==0.9.1'\n'debugpy==1.8.1'\n'decorator==4.4.2'\n'defusedxml==0.7.1'\n'dlib==19.24.2'\n'dnspython==2.6.1'\n'docx2txt==0.8'\n'einops==0.3.2'\n'email-validator==2.1.1'\n'entrypoints==0.4'\n'et-xmlfile==1.1.0'\n'etuples==0.3.9'\n'exchange-calendars==3.4'\n'executing==2.0.1'\n'faker==8.13.2'\n'fastapi==0.95.2'\n'fastjsonschema==2.19.1'\n'fastprogress==1.0.3'\n'ffmpeg-python==0.2.0'\n'ffmpy==0.3.2'\n'filelock==3.13.1'\n'fiona==1.9.2'\n'flask-cachebuster==1.0.0'\n'flask-cors==4.0.0'\n'flask-login==0.6.3'\n'flask==3.0.2'\n'folium==0.12.1'\n'fonttools==4.49.0'\n'fpdf==1.7.2'\n'frozenlist==1.4.1'\n'future==1.0.0'\n'fuzzywuzzy==0.18.0'\n'gensim==4.3.1'\n'geographiclib==1.52'\n'geopandas==0.10.2'\n'geopy==2.2.0'\n'gradio==2.2.15'\n'graphviz==0.17'\n'gtts==2.2.3'\n'h11==0.14.0'\n'h2==4.1.0'\n'h5netcdf==1.3.0'\n'h5py==3.8.0'\n'hpack==4.0.0'\n'html5lib==1.1'\n'httpcore==1.0.4'\n'httptools==0.6.1'\n'httpx==0.27.0'\n'hypercorn==0.14.3'\n'hyperframe==6.0.1'\n'idna==3.6'\n'imageio-ffmpeg==0.4.9'\n'imageio==2.34.0'\n'imgkit==1.2.2'\n'importlib-metadata==7.0.2'\n'importlib-resources==6.3.0'\n'iniconfig==2.0.0'\n'ipykernel==6.29.3'\n'ipython-genutils==0.2.0'\n'ipython==8.22.2'\n'isodate==0.6.1'\n'itsdangerous==2.1.2'\n'jax==0.2.28'\n'jedi==0.19.1'\n'jinja2==3.1.3'\n'joblib==1.3.2'\n'json5==0.9.22'\n'jsonpickle==3.0.3'\n'jsonschema-specifications==2023.12.1'\n'jsonschema==4.21.1'\n'jupyter-client==7.4.9'\n'jupyter-core==5.1.3'\n'jupyter-server==1.23.5'\n'jupyterlab-pygments==0.2.2'\n'jupyterlab-server==2.19.0'\n'jupyterlab==3.4.8'\n'keras==2.6.0'\n'kerykeion==2.1.16'\n'kiwisolver==1.4.5'\n'korean-lunar-calendar==0.3.1'\n'langcodes==3.3.0'\n'lazy-loader==0.3'\n'librosa==0.8.1'\n'lit==18.1.1'\n'llvmlite==0.42.0'\n'logical-unification==0.4.6'\n'loguru==0.5.3'\n'lxml==5.1.0'\n'markdown2==2.4.13'\n'markdownify==0.9.3'\n'markupsafe==2.1.5'\n'matplotlib-inline==0.1.6'\n'matplotlib-venn==0.11.6'\n'matplotlib==3.6.3'\n'minikanren==1.0.3'\n'mistune==3.0.2'\n'mizani==0.10.0'\n'mne==0.23.4'\n'monotonic==1.6'\n'moviepy==1.0.3'\n'mpmath==1.3.0'\n'msgpack==1.0.8'\n'mtcnn==0.1.1'\n'multidict==6.0.5'\n'multipledispatch==1.0.0'\n'munch==4.0.0'\n'murmurhash==1.0.10'\n'mutagen==1.45.1'\n'nashpy==0.0.35'\n'nbclassic==0.4.5'\n'nbclient==0.10.0'\n'nbconvert==7.16.2'\n'nbformat==5.10.2'\n'nest-asyncio==1.6.0'\n'networkx==2.8.8'\n'nltk==3.6.3'\n'notebook-shim==0.2.4'\n'notebook==6.5.1'\n'numba==0.59.0'\n'numexpr==2.9.0'\n'numpy-financial==1.0.0'\n'numpy==1.24.0'\n'nvidia-cublas-cu11==11.10.3.66'\n'nvidia-cuda-cupti-cu11==11.7.101'\n'nvidia-cuda-nvrtc-cu11==11.7.99'\n'nvidia-cuda-runtime-cu11==11.7.99'\n'nvidia-cudnn-cu11==8.5.0.96'\n'nvidia-cufft-cu11==10.9.0.58'\n'nvidia-curand-cu11==10.2.10.91'\n'nvidia-cusolver-cu11==11.4.0.1'\n'nvidia-cusparse-cu11==11.7.4.91'\n'nvidia-nccl-cu11==2.14.3'\n'nvidia-nvtx-cu11==11.7.91'\n'odfpy==1.4.1'\n'opencv-python==4.5.5.62'\n'openpyxl==3.0.10'\n'opt-einsum==3.3.0'\n'orjson==3.9.15'\n'oscrypto==1.3.0'\n'packaging==24.0'\n'pandas==1.5.3'\n'pandocfilters==1.5.1'\n'paramiko==3.4.0'\n'parso==0.8.3'\n'pathlib-abc==0.1.1'\n'pathy==0.11.0'\n'patsy==0.5.6'\n'pdf2image==1.16.3'\n'pdfkit==0.6.1'\n'pdfminer.six==20220319'\n'pdfplumber==0.6.2'\n'pdfrw==0.4'\n'pexpect==4.9.0'\n'pillow==9.2.0'\n'pip==24.0'\n'platformdirs==4.2.0'\n'plotly==5.3.0'\n'plotnine==0.10.1'\n'pluggy==1.4.0'\n'pooch==1.8.1'\n'preshed==3.0.9'\n'priority==2.0.0'\n'proglog==0.1.10'\n'prometheus-client==0.20.0'\n'prompt-toolkit==3.0.43'\n'pronouncing==0.2.0'\n'psutil==5.9.8'\n'ptyprocess==0.7.0'\n'pure-eval==0.2.2'\n'py-cpuinfo==9.0.0'\n'py==1.11.0'\n'pycountry==20.7.3'\n'pycparser==2.21'\n'pycryptodome==3.20.0'\n'pycryptodomex==3.20.0'\n'pydantic==1.10.2'\n'pydot==1.4.2'\n'pydub==0.25.1'\n'pydyf==0.9.0'\n'pygments==2.17.2'\n'pygraphviz==1.7'\n'pyjwt==2.8.0'\n'pylog==1.1'\n'pyluach==2.2.0'\n'pymc==4.0.1'\n'pymupdf==1.21.1'\n'pynacl==1.5.0'\n'pyopenssl==21.0.0'\n'pypandoc==1.6.3'\n'pyparsing==3.1.2'\n'pypdf2==1.28.6'\n'pyphen==0.14.0'\n'pyproj==3.6.1'\n'pyprover==0.5.6'\n'pyshp==2.3.1'\n'pyswisseph==2.10.3.2'\n'pytesseract==0.3.8'\n'pytest==6.2.5'\n'pyth3==0.7'\n'python-dateutil==2.9.0.post0'\n'python-docx==0.8.11'\n'python-dotenv==1.0.1'\n'python-json-logger==2.0.7'\n'python-multipart==0.0.9'\n'python-pptx==0.6.21'\n'pyttsx3==2.90'\n'pytz==2024.1'\n'pywavelets==1.5.0'\n'pyxlsb==1.0.8'\n'pyyaml==6.0.1'\n'pyzbar==0.1.8'\n'pyzmq==25.1.2'\n'qrcode==7.3'\n'rarfile==4.0'\n'rasterio==1.3.3'\n'rdflib==6.0.0'\n'referencing==0.33.0'\n'regex==2023.12.25'\n'reportlab==3.6.12'\n'requests==2.31.0'\n'resampy==0.4.3'\n'rpds-py==0.18.0'\n'scikit-image==0.20.0'\n'scikit-learn==1.1.3'\n'scipy==1.9.3'\n'seaborn==0.11.2'\n'send2trash==1.8.2'\n'sentencepiece==0.2.0'\n'setuptools==65.5.1'\n'shap==0.39.0'\n'shapely==1.7.1'\n'six==1.16.0'\n'slicer==0.0.7'\n'smart-open==6.4.0'\n'sniffio==1.3.1'\n'snowflake-connector-python==2.7.12'\n'snuggs==1.4.7'\n'soundfile==0.10.2'\n'soupsieve==2.5'\n'spacy-legacy==3.0.12'\n'spacy-loggers==1.0.5'\n'spacy==3.4.4'\n'srsly==2.4.8'\n'stack-data==0.6.3'\n'starlette==0.27.0'\n'statsmodels==0.13.5'\n'svglib==1.1.0'\n'svgwrite==1.4.1'\n'sympy==1.8'\n'tables==3.8.0'\n'tabula==1.0.5'\n'tabulate==0.8.9'\n'tenacity==8.2.3'\n'terminado==0.18.1'\n'text-unidecode==1.3'\n'textblob==0.15.3'\n'thinc==8.1.12'\n'threadpoolctl==3.3.0'\n'thrift==0.16.0'\n'tifffile==2024.2.12'\n'tinycss2==1.2.1'\n'toml==0.10.2'\n'tomli==2.0.1'\n'toolz==0.12.1'\n'torch==2.0.1'\n'torchaudio==2.0.2'\n'torchtext==0.6.0'\n'torchvision==0.15.2'\n'tornado==6.4'\n'tqdm==4.64.0'\n'traitlets==5.14.2'\n'trimesh==3.9.29'\n'triton==2.0.0'\n'typer==0.7.0'\n'typing-extensions==4.5.0'\n'ujson==5.9.0'\n'urllib3==1.26.18'\n'uvicorn==0.19.0'\n'uvloop==0.19.0'\n'wand==0.6.13'\n'wasabi==0.10.1'\n'watchfiles==0.21.0'\n'wcwidth==0.2.13'\n'weasyprint==53.3'\n'webencodings==0.5.1'\n'websocket-client==1.7.0'\n'websockets==10.3'\n'werkzeug==3.0.1'\n'wheel==0.43.0'\n'wordcloud==1.9.2'\n'wsproto==1.2.0'\n'xarray-einstats==0.7.0'\n'xarray==2024.2.0'\n'xgboost==1.4.2'\n'xlsxwriter==3.2.0'\n'xml-python==0.4.3'\n'yarl==1.9.4'\n'zipp==3.18.1'\n'zopfli==0.2.3'\n\n</code></pre></div></div>\n</div></details>\n<h1 data-sourcepos=\"715:1-715:14\">\n<span id=\"おわりに\" class=\"fragment\"></span><a href=\"#%E3%81%8A%E3%82%8F%E3%82%8A%E3%81%AB\"><i class=\"fa fa-link\"></i></a>おわりに</h1>\n<p data-sourcepos=\"717:1-717:139\">普段ChatGPTを利用する際に使用している基本的なテクニックをいくつかピックアップして紹介しました。</p>\n<p data-sourcepos=\"719:1-719:293\">エンジニアにとってはChatGPTよりGithubCopilotなどの他の生成AIの方が親和性が高いと感じている方も多いと思いますが、ChatGPTは汎用性も高く、幅広く応用できるのでエンジニアの方にもぜひ活用して頂ければと思います。</p>\n",
      "body": "# はじめに\n\nChatGPTブームがひと段落した感がありますが、周りのエンジニアでChatGPTを活用している姿をあまり見みません。\n\n基本的なテクニックを理解すれば、エンジニアこそChatGPTを活用できると思うので、普段使用しているテクニックをいくつかピックアップして紹介します。\n\n## プロンプトの記載方法\n\n### Markdown記法で指示する\n\n色々なところで紹介されていますが、回答や処理の精度を上げる方法としてChatGPTへの指示にMarkdown記法を使用することがオススメされています。\n\n例えば下記のような文章による指示を行おうとした場合\n\n```:文章による指示\nあなたはChatGPTのプロンプトエンジニアです。ChatGPTの基本的なテクニックを教えてください。\nまた、テクニックはリスト形式で10個提示してください。\n```\n\nMarkdown記法に書き直すと下記の形になります。\n\n```markdown:Markdown記法による指示\n# 指示内容\nChatGPTの基本的なテクニックを教えてください。\n\n# 役割\nあなたはChatGPTのプロンプトエンジニアです。\n\n# 出力形式\nリスト形式で10項目提示します。\n```\n<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/02849036-05be-a486-f13d-36a14f3b9218.png\" width=\"400\">\n\nなお、Markdown記法についてはQiitaでも紹介されているので参考にしてください。\n\nhttps://qiita.com/tbpgr/items/989c6badefff69377da7\n\n### 変数を使う\n\nChatGPTへの指示で変数を扱うことができます。変数は変数名を中括弧(`{}`)で囲う事で使用できます。\n\n例えば指示内容や出力方法などにも変数を使用できます。\n\n```markdown\n# 指示\n- {名前}を使用した会話のサンプルを出力してください。\n- ChatGPTの基本的なテクニックについて会話を行ってください。\n\n# 名前\n- 山田太郎\n- 山田花子\n\n# 出力フォーマット\n- `{名前}:{会話}`の形式で出力してください\n```\n\n<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/4d43cc2f-5aff-8ed2-24a8-0dbde60eaf99.png\" width=\"400\">\n\nまた、入力内容を変数に格納させる事もできます。\n\n```markdown\n# 指示\n- {会話テキスト}を要約してください。\n- {会話テキスト}のフォーマットは`{{名前}}:{{会話}}`の形式で記載されています。\n\n# 会話テキスト\n\"\"\"\n山田太郎:まず、一つ目は「具体的な指示を与えること」だよ。例えば、何かを説明してもらいたいときには、具体的に何について説明してほしいかを明確にするんだ。\n山田花子:なるほど、それは確かに重要ね。具体的に指示を出すことで、より正確な答えが得られるんだね。\n山田太郎:そうそう。そして、二つ目は「段階的な質問をすること」。複雑な問題を解決するときには、一度に全部を聞くのではなく、段階を追って質問することで、より詳細な情報を得ることができるんだ。\n山田花子:それはいいアイデアね。例えば、料理のレシピを知りたいときに、最初に材料を聞いて、その次に作り方を聞くって感じかな。\n山田太郎:まさにその通り。それから、三つ目は「コンテキストを提供すること」。質問をする前に、少し背景を説明すると、より適切な答えが返ってくることが多いんだ。\n\"\"\"\n```\n\n<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/81cc7cb1-ddbb-8f57-5e9a-388b5f71c3d8.png\" width=\"400\">\n\n### パラメータを設定する\n\n独自で作成した変数とは別に、ChatGPT独自が持っているパラメータがあります。\n代表的なパラメータはOpenAI APIでも使用しているパラメータになりますが、その中でも有用なパラメータをピックアップして紹介します。\n\nhttps://platform.openai.com/docs/api-reference/chat\n\n|パラメータ|型|範囲|デフォルト値|説明|\n|---|---|---|---|---|\n|frequency_penalty|float|0.0～2.0|0|同じ単語やフレーズの繰り返しを避けるためのペナルティ。|\n|max_tokens|int|1～4096|4096|生成されるテキストの最大トークン数。|\n|n|int|1～20|1|生成される応答の数。|\n|presence_penalty|float|0.0～2.0|0|特定のトピックやアイデアの頻出を避けるためのペナルティ。|\n|seed|int|任意の整数|なし|再現性を持たせるためのランダムシード値。|\n|temperature|float|0.0～2.0|1|出力の創造性やランダム性を制御する。(`top_p`と併用不可)|\n|top_p|float|0.0～1.0|1|生成される単語の選択肢の多様性を制御する。(`temperature`と併用不可)|\n\n例えば下記のパラメータを変えて出力結果を比較してみます。\n\n```markdown\n# 指示\nChatGPTの良いところを箇条書きで出力してください\n\n# パラメータ\n- frequency_penalty:0.0\n- max_tokens:10\n- n:5\n- presence_penalty:0.0\n- temperature:0.0\n```\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/abb1a635-12d1-2ac7-0410-9c8228ae67c9.png)\n\n`max_tokens`による出力文字列の変化(10→100)と、`n`による生成されるリスト数の変化(5個→10個)についてはわかりやすく出力結果が変化しています。\n\n:::note info\n\nAzure OpenAIの情報になりますが、[パフォーマンスチューニング](https://learn.microsoft.com/ja-jp/azure/ai-services/openai/how-to/latency)でもパラメータの扱いについて言及しているので、興味がある方は調べてみてください。\n\n:::\n\n### 入出力の上限について\n\nChatGPTには使用しているプランやモデルによって入出力する画像サイズや文字数に制限があります。\nこれらはトークンという単位に分割されてカウントされますが、OpenAIのPricingにも記載(`Context window`)があるので参考にしてください。\n\nhttps://openai.com/chatgpt/pricing/\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/5a97eb5b-da19-fada-1541-95c68490e50d.png)\n\nまた、具体的に入力する文字列がいくつのトークンになるのかカウントするサイトをOpenAIでも用意しています。\n\nhttps://platform.openai.com/tokenizer\n\n:::note info\nOpenAIではTokenizerのコードを[Github](https://github.com/openai/tiktoken/blob/main/README.md)で公開しています。\nまた、[cookbook](https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken)や[OpenAI 開発者フォーラム](https://community.openai.com/t/chatgpt-api-maximum-token/83321)でも言及されているので、興味のある方は調べてみてください。\n:::\n\n### プロンプトテクニック集\n\nOpenAIから精度の高いプロンプトの書き方としてPrompt Engineering Guideが公開されています。\n\nhttps://platform.openai.com/docs/guides/prompt-engineering\n\n|項目|概要|\n|---|---|\n|明確な指示を書く|プロンプトには具体的で明確な指示を含めることが重要です。これにより、モデルがタスクを正確に理解し、期待される応答を生成しやすくなります。|\n|参照テキストを提供する|モデルに関連する背景情報や参照テキストを提供することで、より関連性の高い応答を引き出すことができます。これにより、モデルの理解度が向上し、適切な応答が得られます。|\n|複雑なタスクをより単純なサブタスクに分割する|複雑なタスクをより単純なサブタスクに分割することで、モデルが各ステップを順序立てて処理しやすくなり、全体の精度が向上します。|\n|モデルに「考える」時間を与える|プロンプト内で「考える」時間を与える表現を使用することで、モデルがより詳細で深い応答を生成するための時間を確保します。|\n|外部ツールを使用する|必要に応じて外部ツールやデータベースを活用することで、モデルの能力を補完し、より高度な応答を得ることができます。|\n|変更を体系的にテストする|プロンプトの効果を評価するために、変更を体系的にテストし、結果を分析して最適化を図ることが重要です。これにより、モデルのパフォーマンスを継続的に改善できます。|\n\n\nまた、Azure OpenAIになりますが、プロンプトエンジニアリング手法が公開されており、現在でも随時更新されています。\n\nhttps://learn.microsoft.com/ja-jp/azure/ai-services/openai/concepts/advanced-prompt-engineering?pivots=programming-language-chat-completions\n\n|項目|概要|\n|---|---|\n|システムメッセージ|システムメッセージは、会話のトーンやスタイルを設定し、モデルに全体的な指示を提供するために使用されます。|\n|少数ショット学習|少数の例を提供することで、モデルが特定のタスクを学習し、類似の要求に対して正確に応答する能力を向上させます。|\n|チャット以外のシナリオ|チャット以外のユースケースでも、プロンプトエンジニアリングを活用してモデルの応答品質を向上させることができます。|\n|明確な命令から始める|プロンプトを明確な命令で始めることで、モデルが期待されるタスクを正確に理解し、適切な応答を生成しやすくなります。|\n|最後に命令を繰り返す|重要な命令をプロンプトの最後に繰り返すことで、モデルがその指示を見落とすことなく従う可能性が高まります。|\n|出力を事前処理する|出力形式を指定することで、モデルが一貫したフォーマットで応答を提供するように導くことができます。|\n|明確な構文を追加する|プロンプトに明確な構文を追加することで、モデルの理解を助け、望ましい応答を得やすくします。|\n|タスクを中断する|タスクが複雑な場合、モデルにタスクの進捗を確認させることで、誤った方向に進むのを防ぐことができます。|\n|アフォーダンスの使用|アフォーダンスを使用して、モデルに特定のアクションや応答を誘導する手法を取り入れます。|\n|思考の連鎖プロンプティング|モデルが複雑な問題を解決するために、段階的に考えるプロセスを促すプロンプティング技法です。|\n|出力構造の指定|出力の形式や構造を明確に指定することで、モデルが一貫した応答を生成するようにします。|\n|`temperature`パラメーターと`Top_p`パラメーター|これらのパラメーターを調整することで、モデルの応答の多様性や創造性を制御できます。|\n|根拠付けるコンテキストを提供する|モデルが応答を生成する際に必要な背景情報や根拠を提供することで、より正確で関連性の高い応答を得ることができます。|\n\nhttps://learn.microsoft.com/ja-jp/azure/ai-services/openai/concepts/gpt-4-v-prompt-engineering\n\n|項目|概要|\n|---|---|\n|コンテキスト的な特異性|特定の文脈を提示することで、モデルがより関連性の高い応答を生成します。背景情報や具体的な状況を明確に伝えることが重要です。|\n|タスク指向のプロンプト|明確な指示を含むプロンプトを使用して、モデルが特定のタスクを実行するように導きます。具体的な行動を指示することが効果的です。|\n|拒否の処理|モデルが不適切な応答を生成しないように、拒否するべき内容や行動を明示します。不適切なリクエストを事前に防ぐことが目的です。|\n|例の追加|望ましい出力の例をプロンプトに含めることで、モデルが求められる応答の形式や内容を理解しやすくします。|\n|プロンプトのチューニングを試す|プロンプトの内容や形式を調整し、モデルの応答を最適化します。異なるアプローチを試し、最適なプロンプトを見つけます。|\n|要求を分割する|複雑な要求をシンプルなサブタスクに分割し、段階的に指示を出すことで、モデルがより正確な応答を生成しやすくします。|\n|出力形式を定義する|モデルの応答形式を明示することで、出力が一貫したフォーマットで提供されるようにします。例えば、リスト形式や箇条書きなどを指定します。|\n\n具体例も記載されており、比較的使いやすい形にまとまっているので、よければ参考にしてください。\n\n## マルチモーダルの活用\n\n### 質問内容に画像を使用する\n\n意外と使われていない手法ですが、ChatGPTはマルチモーダルに対応しているため、画像を入力として使用できます。\n\n例えばQiitaの通知アイコンについて質問する際も、画面キャプチャを使用してChatGPTに質問することができます。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/2976e0f2-9d0c-7864-6380-69d5f5e13324.png)\n\n:::note infro\nファイルを入力に使用するには、入力エリアの添付ファイルアイコンをクリックしてファイルを選択します。\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/88554c60-99c9-d1a1-6905-2049ddec971f.png)\n\nまた、画面キャプチャなどクリップボードにファイルが格納されている場合は、入力エリアにカーソルを当てた後に貼り付け操作(Command + v / Ctrl + v)を行う事で簡単にファイルを入力に使用する事ができます。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/61a4f2ff-68ab-5393-4ccc-2da81f95e0b6.png)\n\n:::\n\n### 画像の文字認識(OCR)を使用する\n\nChatGPTの日本語の文字認識(OCR)の精度は意外と高く、画像に含まれる文字列を認識させる事ができます。\n\nわかりやすい例として、画像に含まれる日本語の文字起こしに使用できます。\n\n<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/4c72df1a-50e2-6053-6188-65e8fc9db18e.png\" width=\"400\">\n\nまた、まれに日本語を読み取れない時がありますが、そう言った時は`OCRで日本語の文字列を読み取ってください`と明示的にお願いするか、ブレを固定したい場合は日本語のトレーニングデータをアップロードしてAdvanced Data Analysisで強制的に日本語を解析させる事もできます。\n\n下記の記事の`日本語をトレーニングする`で紹介していますので、よければご覧ください。\n\nhttps://qiita.com/b-mente/items/b15011ab3b6c3830e732#%E6%97%A5%E6%9C%AC%E8%AA%9E%E3%82%92%E3%83%88%E3%83%AC%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%99%E3%82%8B\n\n### 画像からソースコードを生成する\n\n上述したように画像の文字列を認識させる事ができるため、出来ることの幅が大きく広がります。\n例えば説明文を入れた画像から内容を読み取らせ、ソースコードを生成する事もできます。\n\n下記は説明付きWebサイトのスケッチ画像からソースコードを生成する例ですが、これだけザックリした指示でもちゃんとHTMLを出力してくれます。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/db4dcf6a-7dd2-4f0d-3b0c-fa4ce4926af9.png)\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/faf8be9f-a27d-d7c9-e8fc-191a839ccbb5.png)\n\nまた、領域名だけ画像に記載し、プロンプトに各領域に対する説明をMarkdownで記載することで細かな設定や動作を表現する事もできます。\n\n### UML/SysML画像を解析する\n\n開発で使用しているシーケンス図などのUMLやSysMLの画像をChatGPTに入力し、解析やその後の処理や出力に利用する事ができます。\n\n下記はシーケンス画像から内容の解説やMermaid記法に出力をしている例ですが、こちらも応用の幅が非常に広いので、ぜひ活用してみてください。\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/4f759acb-828e-1de9-b263-295eaa075f50.png)\n\n![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/6db72f71-22ca-96cb-f2f7-523752241018.png)\n\n- mermaid記法の出力結果\n```mermaid\nsequenceDiagram\n    participant User\n    participant WebServer\n    participant AppServer\n    participant DB\n    \n    User->>WebServer: HttpRequest\n    WebServer->>AppServer: Forwarding Request\n    AppServer->>DB: Data Inquiry\n    DB-->>AppServer: Inquiry Results\n    AppServer-->>WebServer: Processing Result\n    WebServer-->>User: HttpResponse\n```\n\nまた、下記でUML/SysML画像からMermaid記法で出力する方法を紹介していますので、よければご覧ください。\n\nhttps://qiita.com/b-mente/items/b15011ab3b6c3830e732\n\n## Advanced Data Analysisの活用\n\nChatGPTにはAdvanced Data Analysis(旧Code Interpreter)というPythonの実行環境が用意されています。\nよく使用される例としてPDFやエクセル、CSVなどのデータ解析や、テキストやファイル操作などが挙げられますが、短時間で実行可能なPythonスクリプトであれば大体なんでもできます。\n\nhttps://help.openai.com/en/articles/8437071-data-analysis-with-chatgpt\n\n### 各種制限\n\nChatGPT本体やAdvanced Data Analysisには下記の制限がかけられていますので、この制限の中でAdvanced Data Analysisを実行していく必要があります。\n\n- 実行時間制限\n    - 一度のコード実行にかかる時間は通常60秒以内\n- メモリ使用量制限\n    - 1.5GB〜2GB(目安)\n- 扱えるファイルの種類\n    - テキスト ファイル、スプレッドシート、プレゼンテーション、ドキュメントによく使用されるすべてのファイル拡張子\n- アップロードの制限\n    - 最大20ファイル\n    - 1ファイルあたり最大512MB\n        - テキストやドキュメントファイル：200万トークン以内\n        - スプレッドシートやCSVファイル：最大50MB\n        - 画像：最大20MB\n\nhttps://help.openai.com/en/articles/8555545-file-uploads-faq\n\n:::note info\n\n扱えるファイルの種類がザックリしていますが、[AssistantsのCode Interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter)に記載しているファイルは使用できそうです。\n\n```:ファイルの種類\n.c, .cs, .cpp, .doc, .docx, .html, .java, .json, .md, .pdf, .php,\n.pptx, .py, .py, .rb, .tex, .txt, .css, .js, .sh, .ts, .csv, .jpeg,\n.jpg, .gif, .png, .tar, .xlsx, .xml, .zip\n```\n\n:::\n\n### 画像を加工する\n\n特に画像加工系などはデフォルトでパッケージやライブラリが用意されているので、文章による指示でも加工を行なってくれます。\n\n<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/4f0487b1-6a6a-13e7-ffb3-fc2466523df3.png\" width=\"400\">\n\n- PIL(Pillow)、cv2(OpenCV)\n    - リサイズやトリミングなどの基本的な処理を行いたい場合\n    - 画像を合成\n    - 複数の画像を連結（結合）\n    - 透過png画像を作成\n    - ネガポジ反転（画素値を逆転）\n    - 円形や正方形のサムネイル画像作成\n    - 図形描画\n    - アニメーションGif作成\n    - 顔検出（顔認識）\n    - モザイク処理\n\nまた、`qrcode`が用意されているので、QRコードの生成なども行えます。\n\n```\nqrcodeライブラリを使用して下記URLのQRコードを生成してください。\n\nhttps://www.google.com/\n```\n<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/3467c262-6783-a1f1-1df3-b45e41a99f14.png\" width=\"400\">\n\n:::note warn\nsvgwriteが用意されているのでSVGも出力できますが、PNGなどの画像をSVGに変換するにはsvgwrite + cv2だけではなく、matplotlibなどの化学計算用の画像ライブラリと組み合わせる必要がありそうです。\n:::\n\n### デフォルトで使用できるパッケージ\n\n下記を入力すると、現在使用できるパッケージの一覧を表示してくれます。\n\n~~~\n# 指示\nCodeInterpreterでPythonコードを実行してください\n\n# Pythonコード\n```\nimport pkg_resources\n\ninstalled_packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in pkg_resources.working_set])\ninstalled_packages_list\n```\n~~~\n\n<img src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/699056/3c57299b-753c-fe98-e73f-2d5652a2c37e.png\" width=\"400\">\n\n<details><summary>パッケージ一覧</summary><div>\n\n```\n'absl-py==2.1.0'\n'ace-tools==0.0.1'\n'aeppl==0.0.31'\n'aesara==2.7.3'\n'affine==2.4.0'\n'aiohttp==3.8.6'\n'aiosignal==1.3.1'\n'analytics-python==1.4.post1'\n'anyio==3.7.1'\n'anytree==2.8.0'\n'argon2-cffi-bindings==21.2.0'\n'argon2-cffi==23.1.0'\n'arviz==0.17.1'\n'asn1crypto==1.5.1'\n'asttokens==2.4.1'\n'async-timeout==4.0.3'\n'attrs==23.2.0'\n'audioread==3.0.1'\n'babel==2.14.0'\n'backoff==1.10.0'\n'basemap-data==1.3.2'\n'basemap==1.3.9'\n'bcrypt==4.1.2'\n'beautifulsoup4==4.12.3'\n'bleach==6.1.0'\n'blinker==1.7.0'\n'blis==0.7.11'\n'blosc2==2.0.0'\n'bokeh==2.4.0'\n'branca==0.7.1'\n'brotli==1.1.0'\n'cachetools==5.3.3'\n'cairocffi==1.6.1'\n'cairosvg==2.5.2'\n'camelot-py==0.10.1'\n'catalogue==2.0.10'\n'certifi==2024.2.2'\n'cffi==1.16.0'\n'chardet==3.0.4'\n'charset-normalizer==2.1.1'\n'click-plugins==1.1.1'\n'click==8.1.7'\n'cligj==0.7.2'\n'cloudpickle==3.0.0'\n'cmake==3.28.3'\n'cmudict==1.0.21'\n'comm==0.2.2'\n'confection==0.1.4'\n'cons==0.4.6'\n'contourpy==1.2.0'\n'countryinfo==0.1.2'\n'cryptography==3.4.8'\n'cssselect2==0.7.0'\n'cycler==0.12.1'\n'cymem==2.0.8'\n'cython==0.29.36'\n'databricks-sql-connector==0.9.1'\n'debugpy==1.8.1'\n'decorator==4.4.2'\n'defusedxml==0.7.1'\n'dlib==19.24.2'\n'dnspython==2.6.1'\n'docx2txt==0.8'\n'einops==0.3.2'\n'email-validator==2.1.1'\n'entrypoints==0.4'\n'et-xmlfile==1.1.0'\n'etuples==0.3.9'\n'exchange-calendars==3.4'\n'executing==2.0.1'\n'faker==8.13.2'\n'fastapi==0.95.2'\n'fastjsonschema==2.19.1'\n'fastprogress==1.0.3'\n'ffmpeg-python==0.2.0'\n'ffmpy==0.3.2'\n'filelock==3.13.1'\n'fiona==1.9.2'\n'flask-cachebuster==1.0.0'\n'flask-cors==4.0.0'\n'flask-login==0.6.3'\n'flask==3.0.2'\n'folium==0.12.1'\n'fonttools==4.49.0'\n'fpdf==1.7.2'\n'frozenlist==1.4.1'\n'future==1.0.0'\n'fuzzywuzzy==0.18.0'\n'gensim==4.3.1'\n'geographiclib==1.52'\n'geopandas==0.10.2'\n'geopy==2.2.0'\n'gradio==2.2.15'\n'graphviz==0.17'\n'gtts==2.2.3'\n'h11==0.14.0'\n'h2==4.1.0'\n'h5netcdf==1.3.0'\n'h5py==3.8.0'\n'hpack==4.0.0'\n'html5lib==1.1'\n'httpcore==1.0.4'\n'httptools==0.6.1'\n'httpx==0.27.0'\n'hypercorn==0.14.3'\n'hyperframe==6.0.1'\n'idna==3.6'\n'imageio-ffmpeg==0.4.9'\n'imageio==2.34.0'\n'imgkit==1.2.2'\n'importlib-metadata==7.0.2'\n'importlib-resources==6.3.0'\n'iniconfig==2.0.0'\n'ipykernel==6.29.3'\n'ipython-genutils==0.2.0'\n'ipython==8.22.2'\n'isodate==0.6.1'\n'itsdangerous==2.1.2'\n'jax==0.2.28'\n'jedi==0.19.1'\n'jinja2==3.1.3'\n'joblib==1.3.2'\n'json5==0.9.22'\n'jsonpickle==3.0.3'\n'jsonschema-specifications==2023.12.1'\n'jsonschema==4.21.1'\n'jupyter-client==7.4.9'\n'jupyter-core==5.1.3'\n'jupyter-server==1.23.5'\n'jupyterlab-pygments==0.2.2'\n'jupyterlab-server==2.19.0'\n'jupyterlab==3.4.8'\n'keras==2.6.0'\n'kerykeion==2.1.16'\n'kiwisolver==1.4.5'\n'korean-lunar-calendar==0.3.1'\n'langcodes==3.3.0'\n'lazy-loader==0.3'\n'librosa==0.8.1'\n'lit==18.1.1'\n'llvmlite==0.42.0'\n'logical-unification==0.4.6'\n'loguru==0.5.3'\n'lxml==5.1.0'\n'markdown2==2.4.13'\n'markdownify==0.9.3'\n'markupsafe==2.1.5'\n'matplotlib-inline==0.1.6'\n'matplotlib-venn==0.11.6'\n'matplotlib==3.6.3'\n'minikanren==1.0.3'\n'mistune==3.0.2'\n'mizani==0.10.0'\n'mne==0.23.4'\n'monotonic==1.6'\n'moviepy==1.0.3'\n'mpmath==1.3.0'\n'msgpack==1.0.8'\n'mtcnn==0.1.1'\n'multidict==6.0.5'\n'multipledispatch==1.0.0'\n'munch==4.0.0'\n'murmurhash==1.0.10'\n'mutagen==1.45.1'\n'nashpy==0.0.35'\n'nbclassic==0.4.5'\n'nbclient==0.10.0'\n'nbconvert==7.16.2'\n'nbformat==5.10.2'\n'nest-asyncio==1.6.0'\n'networkx==2.8.8'\n'nltk==3.6.3'\n'notebook-shim==0.2.4'\n'notebook==6.5.1'\n'numba==0.59.0'\n'numexpr==2.9.0'\n'numpy-financial==1.0.0'\n'numpy==1.24.0'\n'nvidia-cublas-cu11==11.10.3.66'\n'nvidia-cuda-cupti-cu11==11.7.101'\n'nvidia-cuda-nvrtc-cu11==11.7.99'\n'nvidia-cuda-runtime-cu11==11.7.99'\n'nvidia-cudnn-cu11==8.5.0.96'\n'nvidia-cufft-cu11==10.9.0.58'\n'nvidia-curand-cu11==10.2.10.91'\n'nvidia-cusolver-cu11==11.4.0.1'\n'nvidia-cusparse-cu11==11.7.4.91'\n'nvidia-nccl-cu11==2.14.3'\n'nvidia-nvtx-cu11==11.7.91'\n'odfpy==1.4.1'\n'opencv-python==4.5.5.62'\n'openpyxl==3.0.10'\n'opt-einsum==3.3.0'\n'orjson==3.9.15'\n'oscrypto==1.3.0'\n'packaging==24.0'\n'pandas==1.5.3'\n'pandocfilters==1.5.1'\n'paramiko==3.4.0'\n'parso==0.8.3'\n'pathlib-abc==0.1.1'\n'pathy==0.11.0'\n'patsy==0.5.6'\n'pdf2image==1.16.3'\n'pdfkit==0.6.1'\n'pdfminer.six==20220319'\n'pdfplumber==0.6.2'\n'pdfrw==0.4'\n'pexpect==4.9.0'\n'pillow==9.2.0'\n'pip==24.0'\n'platformdirs==4.2.0'\n'plotly==5.3.0'\n'plotnine==0.10.1'\n'pluggy==1.4.0'\n'pooch==1.8.1'\n'preshed==3.0.9'\n'priority==2.0.0'\n'proglog==0.1.10'\n'prometheus-client==0.20.0'\n'prompt-toolkit==3.0.43'\n'pronouncing==0.2.0'\n'psutil==5.9.8'\n'ptyprocess==0.7.0'\n'pure-eval==0.2.2'\n'py-cpuinfo==9.0.0'\n'py==1.11.0'\n'pycountry==20.7.3'\n'pycparser==2.21'\n'pycryptodome==3.20.0'\n'pycryptodomex==3.20.0'\n'pydantic==1.10.2'\n'pydot==1.4.2'\n'pydub==0.25.1'\n'pydyf==0.9.0'\n'pygments==2.17.2'\n'pygraphviz==1.7'\n'pyjwt==2.8.0'\n'pylog==1.1'\n'pyluach==2.2.0'\n'pymc==4.0.1'\n'pymupdf==1.21.1'\n'pynacl==1.5.0'\n'pyopenssl==21.0.0'\n'pypandoc==1.6.3'\n'pyparsing==3.1.2'\n'pypdf2==1.28.6'\n'pyphen==0.14.0'\n'pyproj==3.6.1'\n'pyprover==0.5.6'\n'pyshp==2.3.1'\n'pyswisseph==2.10.3.2'\n'pytesseract==0.3.8'\n'pytest==6.2.5'\n'pyth3==0.7'\n'python-dateutil==2.9.0.post0'\n'python-docx==0.8.11'\n'python-dotenv==1.0.1'\n'python-json-logger==2.0.7'\n'python-multipart==0.0.9'\n'python-pptx==0.6.21'\n'pyttsx3==2.90'\n'pytz==2024.1'\n'pywavelets==1.5.0'\n'pyxlsb==1.0.8'\n'pyyaml==6.0.1'\n'pyzbar==0.1.8'\n'pyzmq==25.1.2'\n'qrcode==7.3'\n'rarfile==4.0'\n'rasterio==1.3.3'\n'rdflib==6.0.0'\n'referencing==0.33.0'\n'regex==2023.12.25'\n'reportlab==3.6.12'\n'requests==2.31.0'\n'resampy==0.4.3'\n'rpds-py==0.18.0'\n'scikit-image==0.20.0'\n'scikit-learn==1.1.3'\n'scipy==1.9.3'\n'seaborn==0.11.2'\n'send2trash==1.8.2'\n'sentencepiece==0.2.0'\n'setuptools==65.5.1'\n'shap==0.39.0'\n'shapely==1.7.1'\n'six==1.16.0'\n'slicer==0.0.7'\n'smart-open==6.4.0'\n'sniffio==1.3.1'\n'snowflake-connector-python==2.7.12'\n'snuggs==1.4.7'\n'soundfile==0.10.2'\n'soupsieve==2.5'\n'spacy-legacy==3.0.12'\n'spacy-loggers==1.0.5'\n'spacy==3.4.4'\n'srsly==2.4.8'\n'stack-data==0.6.3'\n'starlette==0.27.0'\n'statsmodels==0.13.5'\n'svglib==1.1.0'\n'svgwrite==1.4.1'\n'sympy==1.8'\n'tables==3.8.0'\n'tabula==1.0.5'\n'tabulate==0.8.9'\n'tenacity==8.2.3'\n'terminado==0.18.1'\n'text-unidecode==1.3'\n'textblob==0.15.3'\n'thinc==8.1.12'\n'threadpoolctl==3.3.0'\n'thrift==0.16.0'\n'tifffile==2024.2.12'\n'tinycss2==1.2.1'\n'toml==0.10.2'\n'tomli==2.0.1'\n'toolz==0.12.1'\n'torch==2.0.1'\n'torchaudio==2.0.2'\n'torchtext==0.6.0'\n'torchvision==0.15.2'\n'tornado==6.4'\n'tqdm==4.64.0'\n'traitlets==5.14.2'\n'trimesh==3.9.29'\n'triton==2.0.0'\n'typer==0.7.0'\n'typing-extensions==4.5.0'\n'ujson==5.9.0'\n'urllib3==1.26.18'\n'uvicorn==0.19.0'\n'uvloop==0.19.0'\n'wand==0.6.13'\n'wasabi==0.10.1'\n'watchfiles==0.21.0'\n'wcwidth==0.2.13'\n'weasyprint==53.3'\n'webencodings==0.5.1'\n'websocket-client==1.7.0'\n'websockets==10.3'\n'werkzeug==3.0.1'\n'wheel==0.43.0'\n'wordcloud==1.9.2'\n'wsproto==1.2.0'\n'xarray-einstats==0.7.0'\n'xarray==2024.2.0'\n'xgboost==1.4.2'\n'xlsxwriter==3.2.0'\n'xml-python==0.4.3'\n'yarl==1.9.4'\n'zipp==3.18.1'\n'zopfli==0.2.3'\n\n```\n</div></details>\n\n\n# おわりに\n\n普段ChatGPTを利用する際に使用している基本的なテクニックをいくつかピックアップして紹介しました。\n\nエンジニアにとってはChatGPTよりGithubCopilotなどの他の生成AIの方が親和性が高いと感じている方も多いと思いますが、ChatGPTは汎用性も高く、幅広く応用できるのでエンジニアの方にもぜひ活用して頂ければと思います。\n\n",
      "coediting": false,
      "comments_count": 7,
      "created_at": "2024-06-30T23:31:54+09:00",
      "group": null,
      "id": "93ea3d9a4fc33a76b949",
      "likes_count": 1853,
      "private": false,
      "reactions_count": 0,
      "stocks_count": 1991,
      "tags": [
        {
          "name": "Python",
          "versions": []
        },
        {
          "name": "初心者",
          "versions": []
        },
        {
          "name": "AI",
          "versions": []
        },
        {
          "name": "ChatGPT",
          "versions": []
        },
        {
          "name": "CodeInterpreter",
          "versions": []
        }
      ],
      "title": "エンジニアにも知って欲しいChatGPT基本テクニック",
      "updated_at": "2024-07-05T12:42:12+09:00",
      "url": "https://qiita.com/b-mente/items/93ea3d9a4fc33a76b949",
      "user": {
        "description": null,
        "facebook_id": null,
        "followees_count": 8,
        "followers_count": 513,
        "github_login_name": null,
        "id": "b-mente",
        "items_count": 14,
        "linkedin_id": null,
        "location": null,
        "name": "",
        "organization": null,
        "permanent_id": 699056,
        "profile_image_url": "https://avatars0.githubusercontent.com/u/48114944?v=4",
        "team_only": false,
        "twitter_screen_name": null,
        "website_url": null
      },
      "page_views_count": null,
      "team_membership": null,
      "organization_url_name": "paymentfor",
      "slide": false,
      "semantic_similarity": 0.7712546586990356,
      "quality_score": 24,
      "python_code_score": 6,
      "python_code_blocks": 0
    },
    {
      "rendered_body": "<h1 data-sourcepos=\"1:1-1:14\">\n<span id=\"はじめに\" class=\"fragment\"></span><a href=\"#%E3%81%AF%E3%81%98%E3%82%81%E3%81%AB\"><i class=\"fa fa-link\"></i></a>はじめに</h1>\n<p data-sourcepos=\"2:1-2:67\">こんにちは。cosumi77と申します。Qiita初投稿です。</p>\n<p data-sourcepos=\"4:1-5:153\">普段はド田舎でSEとしてvb.netの開発に従事しておりますが、PythonやAIについてはつい最近まで「なにそれ？美味しいの？」状態でした。<br>\n本記事は、そんな私がナウい（笑）言語を駆使して課題に取り組みましたので、それを紹介するものとなります。</p>\n<p data-sourcepos=\"7:1-7:187\">「こんな<del>クソ</del>記事をネット上に公開して誰が一体読むのか…？」という気持ちを抑えつつ、アウトプットの練習として公開いたします。</p>\n<p data-sourcepos=\"9:1-10:42\">記載内容に誤り等ありましたら、ご指摘いただけると大変ありがたいです。<br>\nどうぞ宜しくお願い致します。</p>\n<h1 data-sourcepos=\"13:1-13:8\">\n<span id=\"目次\" class=\"fragment\"></span><a href=\"#%E7%9B%AE%E6%AC%A1\"><i class=\"fa fa-link\"></i></a>目次</h1>\n<p data-sourcepos=\"14:1-23:35\"><a href=\"#1-%E5%AE%9F%E8%A1%8C%E7%92%B0%E5%A2%83\">1.実行環境</a><br>\n<a href=\"#2-%E6%9C%AC%E8%A8%98%E4%BA%8B%E3%81%AE%E7%9B%AE%E7%9A%84\">2.本記事の目的</a><br>\n<a href=\"#3-%E5%89%8D%E6%8F%90%E3%81%A8%E3%81%AA%E3%82%8B%E5%95%8F%E9%A1%8C%E6%84%8F%E8%AD%98\">3.前提となる問題意識</a><br>\n<a href=\"#4-%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%A6%82%E8%A6%81\">4.モデルの概要</a><br>\n<a href=\"#5-%E7%9B%AE%E7%9A%84%E3%81%AB%E8%87%B3%E3%82%8B%E3%81%BE%E3%81%A7%E3%81%AE%E9%81%93%E3%81%AE%E3%82%8A\">5.目的に至るまでの道のり</a><br>\n<a href=\"#6-%E3%82%84%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%9F\">6.やってみた</a><br>\n<a href=\"#7-%E6%AC%A1%E3%81%AE%E3%82%B9%E3%83%86%E3%83%83%E3%83%97\">7.次のステップ</a><br>\n<a href=\"#8-appendix\">8.Appendix</a><br>\n<a href=\"#9-%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A\">9.振り返り</a><br>\n<a href=\"#10-%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\">10.参考文献</a></p>\n<h1 data-sourcepos=\"25:1-25:17\">\n<span id=\"1-実行環境\" class=\"fragment\"></span><a href=\"#1-%E5%AE%9F%E8%A1%8C%E7%92%B0%E5%A2%83\"><i class=\"fa fa-link\"></i></a>1. 実行環境</h1>\n<p data-sourcepos=\"26:1-27:18\">Windows11 22H2<br>\nVisual Studio Code</p>\n<p data-sourcepos=\"29:1-29:13\">Python 3.10.4</p>\n<p data-sourcepos=\"31:1-34:34\">matplotlib                   3.5.2<br>\nnumpy                        1.22.4<br>\nkeras                        2.9.0<br>\ntensorflow                   2.9.1</p>\n<h1 data-sourcepos=\"36:1-36:23\">\n<span id=\"2-本記事の目的\" class=\"fragment\"></span><a href=\"#2-%E6%9C%AC%E8%A8%98%E4%BA%8B%E3%81%AE%E7%9B%AE%E7%9A%84\"><i class=\"fa fa-link\"></i></a>2. 本記事の目的</h1>\n<p data-sourcepos=\"37:1-37:269\">任意の長さのの整数（例：123456789、98765、12 等）の画像を「シーケンス認識によるAI-OCRモデル(<a href=\"https://keras.io/examples/vision/captcha_ocr/\" rel=\"nofollow noopener\" target=\"_blank\">OCR model for reading Captchas</a>)」を用いて読み取り、データとして吐き出す。</p>\n<h1 data-sourcepos=\"39:1-39:32\">\n<span id=\"3-前提となる問題意識\" class=\"fragment\"></span><a href=\"#3-%E5%89%8D%E6%8F%90%E3%81%A8%E3%81%AA%E3%82%8B%E5%95%8F%E9%A1%8C%E6%84%8F%E8%AD%98\"><i class=\"fa fa-link\"></i></a>3. 前提となる問題意識</h1>\n<p data-sourcepos=\"40:1-42:156\">私は、主に客先などで伝票を見る機会が多いです。<br>\n伝票は紙またはpdf形式で受け渡しがされており、受け取った伝票を見ながら別のシステムへチマチマと転記するようなシーンが多く見られます。<br>\nそこで、「もしこれをAIで読み取り、データとして起こせると<del>飯の種</del>作業効率化になるよな～…」と思いました。</p>\n<p data-sourcepos=\"44:1-44:159\">まずは第一ステップとして、手書きの数値を学習させて、どの程度読み取れるものなのかをやってみたいと思います。</p>\n<h1 data-sourcepos=\"46:1-46:23\">\n<span id=\"4-モデルの概要\" class=\"fragment\"></span><a href=\"#4-%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%A6%82%E8%A6%81\"><i class=\"fa fa-link\"></i></a>4. モデルの概要</h1>\n<p data-sourcepos=\"47:1-47:228\">文字データを認識するためのアルゴリズムはいろいろあるようですが、ここではCRNN（CNN + RNN）とCTC損失関数を使ったシーケンス認識（Sequence Recognition）を用いています。</p>\n<p data-sourcepos=\"49:1-50:40\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F9f050732-2ae6-3b2d-be4f-217ac1a94415.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=da5a0f97b7ca1dbebc30a72480496976\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F9f050732-2ae6-3b2d-be4f-217ac1a94415.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=da5a0f97b7ca1dbebc30a72480496976\" alt=\"rcnn_ctc.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F9f050732-2ae6-3b2d-be4f-217ac1a94415.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=1d5ef746520e052eda6ba5178759e800 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/9f050732-2ae6-3b2d-be4f-217ac1a94415.png\" loading=\"lazy\"></a><br>\n出典: <a href=\"https://arxiv.org/abs/1507.05717\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\">https://arxiv.org/abs/1507.05717</a></p>\n<p data-sourcepos=\"52:1-53:129\">CNNとは「畳み込みニューラルネットワーク」とも呼ばれ、人間の脳の視覚野と似た構造を持つ 「畳み込み層」 という層を使って特徴抽出を行うニューラルネットワークです。主に画像認識の分野で高い性能を発揮します。<br>\nこのモデルでは、画像内の各数字を識別して画像データ列としてRNNへ渡す役割を担っています。</p>\n<p data-sourcepos=\"55:1-56:120\">RNNとは「回帰型ニューラルネットワーク」などとも呼ばれ、数値の時系列データなどのシーケンスデータのパターンを認識するように設計されたニューラルネットワークのモデルです。<br>\nこのモデルでは、CNNから渡された画像データ列から推定される文字列の算出を行います。</p>\n<p data-sourcepos=\"58:1-58:84\">CTC損失関数はRNNの出力および正解データで最適化を行います。</p>\n<h1 data-sourcepos=\"60:1-60:38\">\n<span id=\"5-目的に至るまでの道のり\" class=\"fragment\"></span><a href=\"#5-%E7%9B%AE%E7%9A%84%E3%81%AB%E8%87%B3%E3%82%8B%E3%81%BE%E3%81%A7%E3%81%AE%E9%81%93%E3%81%AE%E3%82%8A\"><i class=\"fa fa-link\"></i></a>5. 目的に至るまでの道のり</h1>\n<ol data-sourcepos=\"61:1-64:0\">\n<li data-sourcepos=\"61:1-61:116\">MNISTから手書き文字を取得し数字のみを抜き取った後でランダムな画像数字列を生成</li>\n<li data-sourcepos=\"62:1-62:84\">シーケンス認識によるAI-OCR（文字認識）モデルを用いた学習</li>\n<li data-sourcepos=\"63:1-64:0\">テストデータによる検証</li>\n</ol>\n<p data-sourcepos=\"65:1-65:20\"><a id=\"\"></a></p>\n<h1 data-sourcepos=\"66:1-66:23\">\n<span id=\"6-やってみた\" class=\"fragment\"></span><a href=\"#6-%E3%82%84%E3%81%A3%E3%81%A6%E3%81%BF%E3%81%9F\"><i class=\"fa fa-link\"></i></a>6. やってみた！</h1>\n<h2 data-sourcepos=\"68:1-68:86\">\n<span id=\"1-mnistから手書き文字を取得しランダムな画像数字列を生成\" class=\"fragment\"></span><a href=\"#1-mnist%E3%81%8B%E3%82%89%E6%89%8B%E6%9B%B8%E3%81%8D%E6%96%87%E5%AD%97%E3%82%92%E5%8F%96%E5%BE%97%E3%81%97%E3%83%A9%E3%83%B3%E3%83%80%E3%83%A0%E3%81%AA%E7%94%BB%E5%83%8F%E6%95%B0%E5%AD%97%E5%88%97%E3%82%92%E7%94%9F%E6%88%90\"><i class=\"fa fa-link\"></i></a>1. MNISTから手書き文字を取得し、ランダムな画像数字列を生成</h2>\n<h3 data-sourcepos=\"70:1-70:16\">\n<span id=\"ポイント\" class=\"fragment\"></span><a href=\"#%E3%83%9D%E3%82%A4%E3%83%B3%E3%83%88\"><i class=\"fa fa-link\"></i></a>ポイント</h3>\n<ul data-sourcepos=\"71:1-76:0\">\n<li data-sourcepos=\"71:1-71:105\">MNISTから最大10個の画像データをランダムに結合して学習データを作成します</li>\n<li data-sourcepos=\"72:1-72:95\">生成する画像のサイズを統一するために適宜余白を埋め込んでいます</li>\n<li data-sourcepos=\"73:1-73:113\">最終目的は「伝票に記載された数値を読み取る事」なので、白黒を反転しています</li>\n<li data-sourcepos=\"74:1-76:0\">生成した画像のファイル名には、画像に記された数字列をセットします<br>\nただし、ランダムに数値を生成した場合、値が重複するとファイル名も被ってしまうため、これらを識別できるように「(数値)_(識別番号).png」のようなファイル名にしました</li>\n</ul>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"77:1-168:3\"><div class=\"highlight\"><pre><code><span class=\"c1\">#実行前に実行フォルダ直下にcreate_imagesフォルダを作成してください\n</span>\n<span class=\"c1\">#各種インポート\n</span><span class=\"kn\">import</span> <span class=\"n\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"n\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"kn\">import</span> <span class=\"n\">random</span>\n<span class=\"kn\">import</span> <span class=\"n\">cv2</span>\n<span class=\"kn\">import</span> <span class=\"n\">os</span>\n<span class=\"kn\">import</span> <span class=\"n\">re</span>\n<span class=\"kn\">import</span> <span class=\"n\">glob</span>\n\n<span class=\"kn\">from</span> <span class=\"n\">pathlib</span> <span class=\"kn\">import</span> <span class=\"n\">Path</span>\n<span class=\"kn\">from</span> <span class=\"n\">collections</span> <span class=\"kn\">import</span> <span class=\"n\">Counter</span>\n<span class=\"kn\">from</span> <span class=\"n\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n\n<span class=\"kn\">from</span> <span class=\"n\">keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n<span class=\"kn\">import</span> <span class=\"n\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"n\">tensorflow</span> <span class=\"kn\">import</span> <span class=\"n\">keras</span>\n<span class=\"kn\">from</span> <span class=\"n\">tensorflow.keras</span> <span class=\"kn\">import</span> <span class=\"n\">layers</span>\n\n<span class=\"c1\">#作成する画像数\n</span><span class=\"n\">create_data_count</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span>\n\n<span class=\"c1\">#MNISTの画像サイズ\n</span><span class=\"n\">mnist_picture_width</span> <span class=\"o\">=</span> <span class=\"mi\">28</span>\n<span class=\"n\">mnist_picture_height</span> <span class=\"o\">=</span> <span class=\"mi\">28</span>\n\n<span class=\"c1\">#数値列の最大桁数\n</span><span class=\"n\">str_max_length</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n\n<span class=\"c1\">#作成した画像の保存先\n</span><span class=\"n\">source_path</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">./create_images/</span><span class=\"sh\">\"</span>\n<span class=\"n\">data_dir</span> <span class=\"o\">=</span> <span class=\"nc\">Path</span><span class=\"p\">(</span><span class=\"n\">source_path</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#余白画像\n</span><span class=\"n\">img_white</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nf\">ones</span><span class=\"p\">((</span><span class=\"n\">mnist_picture_width</span><span class=\"p\">,</span><span class=\"n\">mnist_picture_height</span><span class=\"p\">),</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"mi\">255</span>\n\n<span class=\"c1\">#################################################################\n#データの準備\n#################################################################\n</span>\n<span class=\"c1\">#---------------------------------------------------------\n#MNISTデータの準備、および、加工\n#---------------------------------------------------------\n</span><span class=\"p\">(</span><span class=\"n\">mnist_images</span><span class=\"p\">,</span> <span class=\"n\">mnist_labels</span><span class=\"p\">)</span><span class=\"o\">=</span> <span class=\"n\">mnist</span><span class=\"p\">.</span><span class=\"nf\">load_data</span><span class=\"p\">()[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#学習データの内、0以外のインデックスを取得\n</span><span class=\"n\">not_zero_index</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nf\">where</span><span class=\"p\">(</span><span class=\"n\">mnist_labels</span><span class=\"o\">!=</span><span class=\"mi\">0</span><span class=\"p\">))[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#データセットの色の反転\n</span><span class=\"n\">mnist_images</span> <span class=\"o\">=</span> <span class=\"mi\">255</span> <span class=\"o\">-</span> <span class=\"n\">mnist_images</span>\n\n<span class=\"c1\">#---------------------------------------------------------\n#MNISTから数字を取得し、最大10桁の画像データを生成\n#---------------------------------------------------------\n</span><span class=\"k\">def</span> <span class=\"nf\">create_images</span><span class=\"p\">(</span><span class=\"n\">mnist_images</span><span class=\"p\">,</span> <span class=\"n\">mnist_labels</span><span class=\"p\">,</span> <span class=\"n\">not_zero_index</span><span class=\"p\">):</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">create_data_count</span><span class=\"p\">):</span>\n\n        <span class=\"c1\">#1から最大桁数の範囲で乱数を生成\n</span>        <span class=\"n\">digit_count</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"nf\">randint</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">str_max_length</span><span class=\"p\">)</span>\n        <span class=\"n\">label_name</span> <span class=\"o\">=</span> <span class=\"sh\">''</span>\n\n        <span class=\"c1\">#画像を結合して任意の数値列を作り出す\n</span>        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">digit_count</span><span class=\"p\">):</span>\n\n            <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"c1\">#MNISTデータを任意に取り出すための乱数(先頭の数値は0以外で)\n</span>                <span class=\"n\">choise_not_zero</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"nf\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">not_zero_index</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n                <span class=\"c1\">#初期画像をセット\n</span>                <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">mnist_images</span><span class=\"p\">[</span><span class=\"n\">not_zero_index</span><span class=\"p\">[</span><span class=\"n\">choise_not_zero</span><span class=\"p\">]]</span>\n                <span class=\"c1\">#ラベル情報をセット\n</span>                <span class=\"n\">label_name</span> <span class=\"o\">=</span> <span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"n\">mnist_labels</span><span class=\"p\">[</span><span class=\"n\">not_zero_index</span><span class=\"p\">[</span><span class=\"n\">choise_not_zero</span><span class=\"p\">]])</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"c1\">#MNISTデータを任意に取り出すための乱数\n</span>                <span class=\"n\">choise_image_index</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"nf\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">mnist_images</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n                <span class=\"c1\">#画像を横に結合\n</span>                <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">cv2</span><span class=\"p\">.</span><span class=\"nf\">hconcat</span><span class=\"p\">([</span><span class=\"n\">img</span><span class=\"p\">,</span><span class=\"n\">mnist_images</span><span class=\"p\">[</span><span class=\"n\">choise_image_index</span><span class=\"p\">]])</span>\n                <span class=\"c1\">#ラベル情報をセット\n</span>                <span class=\"n\">label_name</span> <span class=\"o\">+=</span> <span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"n\">mnist_labels</span><span class=\"p\">[</span><span class=\"n\">choise_image_index</span><span class=\"p\">])</span>\n        \n        <span class=\"c1\">#余白の挿入\n</span>        <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"n\">digit_count</span><span class=\"p\">,</span><span class=\"n\">str_max_length</span><span class=\"p\">):</span>\n                <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">cv2</span><span class=\"p\">.</span><span class=\"nf\">hconcat</span><span class=\"p\">([</span><span class=\"n\">img</span><span class=\"p\">,</span><span class=\"n\">img_white</span><span class=\"p\">])</span>\n\n        <span class=\"c1\">#画像データを保存\n</span>        <span class=\"n\">cv2</span><span class=\"p\">.</span><span class=\"nf\">imwrite</span><span class=\"p\">(</span><span class=\"n\">source_path</span> <span class=\"o\">+</span> <span class=\"n\">label_name</span> <span class=\"o\">+</span> <span class=\"sh\">'</span><span class=\"s\">_</span><span class=\"sh\">'</span> <span class=\"o\">+</span> <span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"n\">j</span><span class=\"p\">)</span> <span class=\"o\">+</span><span class=\"sh\">'</span><span class=\"s\">.png</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">img</span><span class=\"p\">)</span>   <span class=\"c1\">#数値列の重複を避けるために末尾に識別子を付与\n</span>\n<span class=\"c1\">#学習データ、検証データの生成\n</span><span class=\"nf\">create_images</span><span class=\"p\">(</span><span class=\"n\">mnist_images</span><span class=\"p\">,</span> <span class=\"n\">mnist_labels</span><span class=\"p\">,</span> <span class=\"n\">not_zero_index</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<p data-sourcepos=\"170:1-170:57\">結果、以下のような画像が生成されます。</p>\n<p data-sourcepos=\"172:1-179:123\">790467491_880.png<br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F118c242e-ee59-660d-d48f-84b1b6fb0737.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=10cabbb73c4f26b2de8568323892253f\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F118c242e-ee59-660d-d48f-84b1b6fb0737.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=10cabbb73c4f26b2de8568323892253f\" alt=\"790467491_880.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F118c242e-ee59-660d-d48f-84b1b6fb0737.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=faf174be0528e46908249f7236a9aeb6 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/118c242e-ee59-660d-d48f-84b1b6fb0737.png\" loading=\"lazy\"></a><br>\n1_462.png<br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F3d6f2aee-8c47-1c38-8a81-6ca2f80d3ab8.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=9ebacdd2003704414e73986952412ed6\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F3d6f2aee-8c47-1c38-8a81-6ca2f80d3ab8.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=9ebacdd2003704414e73986952412ed6\" alt=\"1_462.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F3d6f2aee-8c47-1c38-8a81-6ca2f80d3ab8.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=728b9d76c9bfbe71bc497763734585f0 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/3d6f2aee-8c47-1c38-8a81-6ca2f80d3ab8.png\" loading=\"lazy\"></a><br>\n1334361586_422.png<br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F11ead10b-2f13-4786-ae47-500cbbf1c859.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=5fbdba525802704a31d2d85e31446ace\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F11ead10b-2f13-4786-ae47-500cbbf1c859.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=5fbdba525802704a31d2d85e31446ace\" alt=\"1334361586_422.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F11ead10b-2f13-4786-ae47-500cbbf1c859.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=eeedbf5334c059b345dfdcc838330698 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/11ead10b-2f13-4786-ae47-500cbbf1c859.png\" loading=\"lazy\"></a><br>\n7792_97.png<br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F4ee44532-9243-1488-7641-1653ad2a12d1.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=e24381a551f3518ab38e156cc1755c32\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F4ee44532-9243-1488-7641-1653ad2a12d1.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=e24381a551f3518ab38e156cc1755c32\" alt=\"7792_97.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F4ee44532-9243-1488-7641-1653ad2a12d1.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=88fc3f5ee74a6777f59de3945e3471ec 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/4ee44532-9243-1488-7641-1653ad2a12d1.png\" loading=\"lazy\"></a></p>\n<h2 data-sourcepos=\"182:1-182:87\">\n<span id=\"2-シーケンス認識によるai-ocr文字認識モデルを用いた学習\" class=\"fragment\"></span><a href=\"#2-%E3%82%B7%E3%83%BC%E3%82%B1%E3%83%B3%E3%82%B9%E8%AA%8D%E8%AD%98%E3%81%AB%E3%82%88%E3%82%8Bai-ocr%E6%96%87%E5%AD%97%E8%AA%8D%E8%AD%98%E3%83%A2%E3%83%87%E3%83%AB%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E5%AD%A6%E7%BF%92\"><i class=\"fa fa-link\"></i></a>2. シーケンス認識によるAI-OCR（文字認識）モデルを用いた学習</h2>\n<p data-sourcepos=\"183:1-183:235\">このあたりは先程ご紹介した以下のページをほとんど<del>パクり</del>流用しながら、少しだけ変えて利用しました。元のコードから変更した箇所を中心にコメントを付しています。</p>\n<p data-sourcepos=\"185:1-185:45\"><iframe id=\"qiita-embed-content__066decedd45cbb1cf3a8bafb5462f5e4\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__066decedd45cbb1cf3a8bafb5462f5e4\" data-content=\"https%3A%2F%2Fkeras.io%2Fexamples%2Fvision%2Fcaptcha_ocr%2F\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"187:1-409:3\"><div class=\"highlight\"><pre><code><span class=\"c1\">#---------------------------------------------------------\n#データ準備\n#---------------------------------------------------------\n</span>\n<span class=\"c1\"># Get list of all the images\n</span><span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"nf\">sorted</span><span class=\"p\">(</span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"nf\">map</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">data_dir</span><span class=\"p\">.</span><span class=\"nf\">glob</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">*.png</span><span class=\"sh\">\"</span><span class=\"p\">)))))</span>\n\n<span class=\"c1\">#ファイル名の末尾の識別子を除去し、ラベルとして取得\n# 【変更後】\n</span><span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">re</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">_[0-9]*.png</span><span class=\"sh\">\"</span><span class=\"p\">,</span><span class=\"n\">img</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">path</span><span class=\"p\">.</span><span class=\"n\">sep</span><span class=\"p\">)[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">img</span> <span class=\"ow\">in</span> <span class=\"n\">images</span><span class=\"p\">]</span>  \n<span class=\"c1\"># 【変更前】\n# labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n</span>\n<span class=\"c1\"># 空白を表す文字sを右に追加する（サンプルコードは固定文字数だが今回のケースは文字数が変動する）\n</span><span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">label</span><span class=\"p\">.</span><span class=\"nf\">ljust</span><span class=\"p\">(</span><span class=\"n\">str_max_length</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">s</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"n\">labels</span><span class=\"p\">]</span> \n\n<span class=\"n\">characters</span> <span class=\"o\">=</span> <span class=\"nf\">set</span><span class=\"p\">(</span><span class=\"n\">char</span> <span class=\"k\">for</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"n\">labels</span> <span class=\"k\">for</span> <span class=\"n\">char</span> <span class=\"ow\">in</span> <span class=\"n\">label</span><span class=\"p\">)</span>\n<span class=\"n\">characters</span> <span class=\"o\">=</span> <span class=\"nf\">sorted</span><span class=\"p\">(</span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">characters</span><span class=\"p\">))</span>\n\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">Number of images found: </span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">))</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">Number of labels found: </span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">))</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">Number of unique characters: </span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">characters</span><span class=\"p\">))</span>\n<span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">Characters present: </span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">characters</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Batch size for training and validation\n</span><span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n\n<span class=\"c1\"># Desired image dimensions\n#生成した画像データのサイズを指定\n#【変更後】\n</span><span class=\"n\">img_width</span> <span class=\"o\">=</span> <span class=\"n\">mnist_picture_width</span> <span class=\"o\">*</span> <span class=\"mi\">10</span> \n<span class=\"n\">img_height</span> <span class=\"o\">=</span> <span class=\"n\">mnist_picture_height</span>\n<span class=\"c1\">#【変更前】\n#img_width = 200\n#img_height = 50\n</span>\n<span class=\"c1\"># Factor by which the image is going to be downsampled\n# by the convolutional blocks. We will be using two\n# convolution blocks and each block will have\n# a pooling layer which downsample the features by a factor of 2.\n# Hence total downsampling factor would be 4.\n</span><span class=\"n\">downsample_factor</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>\n\n<span class=\"c1\"># Maximum length of any captcha in the dataset\n</span><span class=\"n\">max_length</span> <span class=\"o\">=</span> <span class=\"nf\">max</span><span class=\"p\">([</span><span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"n\">labels</span><span class=\"p\">])</span>\n\n<span class=\"c1\">#---------------------------------------------------------\n#データ加工\n#---------------------------------------------------------\n# Mapping characters to integers\n</span><span class=\"n\">char_to_num</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">StringLookup</span><span class=\"p\">(</span>\n    <span class=\"n\">vocabulary</span><span class=\"o\">=</span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">characters</span><span class=\"p\">),</span> <span class=\"n\">mask_token</span><span class=\"o\">=</span><span class=\"bp\">None</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Mapping integers back to original characters\n</span><span class=\"n\">num_to_char</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">StringLookup</span><span class=\"p\">(</span>\n    <span class=\"n\">vocabulary</span><span class=\"o\">=</span><span class=\"n\">char_to_num</span><span class=\"p\">.</span><span class=\"nf\">get_vocabulary</span><span class=\"p\">(),</span> <span class=\"n\">mask_token</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">invert</span><span class=\"o\">=</span><span class=\"bp\">True</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">split_data</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">train_size</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">):</span>\n    <span class=\"c1\"># 1. Get the total size of the dataset\n</span>    <span class=\"n\">size</span> <span class=\"o\">=</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 2. Make an indices array and shuffle it, if required\n</span>    <span class=\"n\">indices</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nf\">arange</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">shuffle</span><span class=\"p\">:</span>\n        <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">random</span><span class=\"p\">.</span><span class=\"nf\">shuffle</span><span class=\"p\">(</span><span class=\"n\">indices</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 3. Get the size of training samples\n</span>    <span class=\"n\">train_samples</span> <span class=\"o\">=</span> <span class=\"nf\">int</span><span class=\"p\">(</span><span class=\"n\">size</span> <span class=\"o\">*</span> <span class=\"n\">train_size</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 4. Split data into training and validation sets\n</span>    <span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">images</span><span class=\"p\">[</span><span class=\"n\">indices</span><span class=\"p\">[:</span><span class=\"n\">train_samples</span><span class=\"p\">]],</span> <span class=\"n\">labels</span><span class=\"p\">[</span><span class=\"n\">indices</span><span class=\"p\">[:</span><span class=\"n\">train_samples</span><span class=\"p\">]]</span>\n    <span class=\"n\">x_valid</span><span class=\"p\">,</span> <span class=\"n\">y_valid</span> <span class=\"o\">=</span> <span class=\"n\">images</span><span class=\"p\">[</span><span class=\"n\">indices</span><span class=\"p\">[</span><span class=\"n\">train_samples</span><span class=\"p\">:]],</span> <span class=\"n\">labels</span><span class=\"p\">[</span><span class=\"n\">indices</span><span class=\"p\">[</span><span class=\"n\">train_samples</span><span class=\"p\">:]]</span>\n    <span class=\"k\">return</span> <span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_valid</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_valid</span>\n\n<span class=\"c1\"># Splitting data into training and validation sets\n</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">x_valid</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_valid</span> <span class=\"o\">=</span> <span class=\"nf\">split_data</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nf\">array</span><span class=\"p\">(</span><span class=\"n\">images</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nf\">array</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">))</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">encode_single_sample</span><span class=\"p\">(</span><span class=\"n\">img_path</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"p\">):</span>\n    <span class=\"c1\"># 1. Read image\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">io</span><span class=\"p\">.</span><span class=\"nf\">read_file</span><span class=\"p\">(</span><span class=\"n\">img_path</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 2. Decode and convert to grayscale\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">io</span><span class=\"p\">.</span><span class=\"nf\">decode_png</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 3. Convert to float32 in [0, 1] range\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">convert_image_dtype</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 4. Resize to the desired size\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">resize</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"n\">img_height</span><span class=\"p\">,</span> <span class=\"n\">img_width</span><span class=\"p\">])</span>\n    <span class=\"c1\"># 5. Transpose the image because we want the time\n</span>    <span class=\"c1\"># dimension to correspond to the width of the image.\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">transpose</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">perm</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">])</span>\n    <span class=\"c1\"># 6. Map the characters in label to numbers\n</span>    <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"nf\">char_to_num</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">strings</span><span class=\"p\">.</span><span class=\"nf\">unicode_split</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">,</span> <span class=\"n\">input_encoding</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">UTF-8</span><span class=\"sh\">\"</span><span class=\"p\">))</span>\n    <span class=\"c1\"># 7. Return a dict as our model is expecting two inputs\n</span>    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"sh\">\"</span><span class=\"s\">image</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">label</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">label</span><span class=\"p\">}</span>\n\n<span class=\"c1\">#---------------------------------------------------------\n#TensorFlowデータセットを生成\n#---------------------------------------------------------\n</span><span class=\"n\">train_dataset</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">Dataset</span><span class=\"p\">.</span><span class=\"nf\">from_tensor_slices</span><span class=\"p\">((</span><span class=\"n\">x_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">))</span>\n<span class=\"n\">train_dataset</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">train_dataset</span><span class=\"p\">.</span><span class=\"nf\">map</span><span class=\"p\">(</span>\n        <span class=\"n\">encode_single_sample</span><span class=\"p\">,</span> <span class=\"n\">num_parallel_calls</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">AUTOTUNE</span>\n    <span class=\"p\">)</span>\n    <span class=\"p\">.</span><span class=\"nf\">batch</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n    <span class=\"p\">.</span><span class=\"nf\">prefetch</span><span class=\"p\">(</span><span class=\"n\">buffer_size</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">AUTOTUNE</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">validation_dataset</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">Dataset</span><span class=\"p\">.</span><span class=\"nf\">from_tensor_slices</span><span class=\"p\">((</span><span class=\"n\">x_valid</span><span class=\"p\">,</span> <span class=\"n\">y_valid</span><span class=\"p\">))</span>\n<span class=\"n\">validation_dataset</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">validation_dataset</span><span class=\"p\">.</span><span class=\"nf\">map</span><span class=\"p\">(</span>\n        <span class=\"n\">encode_single_sample</span><span class=\"p\">,</span> <span class=\"n\">num_parallel_calls</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">AUTOTUNE</span>\n    <span class=\"p\">)</span>\n    <span class=\"p\">.</span><span class=\"nf\">batch</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n    <span class=\"p\">.</span><span class=\"nf\">prefetch</span><span class=\"p\">(</span><span class=\"n\">buffer_size</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">AUTOTUNE</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\">#---------------------------------------------------------\n#今回使用するモデル\n#---------------------------------------------------------\n</span><span class=\"k\">class</span> <span class=\"nc\">CTCLayer</span><span class=\"p\">(</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"n\">Layer</span><span class=\"p\">):</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"n\">self</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>\n        <span class=\"nf\">super</span><span class=\"p\">().</span><span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">name</span><span class=\"p\">)</span>\n        <span class=\"n\">self</span><span class=\"p\">.</span><span class=\"n\">loss_fn</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">backend</span><span class=\"p\">.</span><span class=\"n\">ctc_batch_cost</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">call</span><span class=\"p\">(</span><span class=\"n\">self</span><span class=\"p\">,</span> <span class=\"n\">y_true</span><span class=\"p\">,</span> <span class=\"n\">y_pred</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Compute the training-time loss value and add it\n</span>        <span class=\"c1\"># to the layer using `self.add_loss()`.\n</span>        <span class=\"n\">batch_len</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">cast</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">shape</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">int64</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"n\">input_length</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">cast</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">shape</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">)[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">int64</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"n\">label_length</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">cast</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">shape</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">)[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">int64</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"n\">input_length</span> <span class=\"o\">=</span> <span class=\"n\">input_length</span> <span class=\"o\">*</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">ones</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">batch_len</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">int64</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"n\">label_length</span> <span class=\"o\">=</span> <span class=\"n\">label_length</span> <span class=\"o\">*</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">ones</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">batch_len</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">int64</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">self</span><span class=\"p\">.</span><span class=\"nf\">loss_fn</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">,</span> <span class=\"n\">y_pred</span><span class=\"p\">,</span> <span class=\"n\">input_length</span><span class=\"p\">,</span> <span class=\"n\">label_length</span><span class=\"p\">)</span>\n        <span class=\"n\">self</span><span class=\"p\">.</span><span class=\"nf\">add_loss</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># At test time, just return the computed predictions\n</span>        <span class=\"k\">return</span> <span class=\"n\">y_pred</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">build_model</span><span class=\"p\">():</span>\n    <span class=\"c1\"># Inputs to the model\n</span>    <span class=\"n\">input_img</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Input</span><span class=\"p\">(</span>\n        <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">img_width</span><span class=\"p\">,</span> <span class=\"n\">img_height</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">image</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">float32</span><span class=\"sh\">\"</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Input</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">label</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">float32</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># First conv block\n</span>    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Conv2D</span><span class=\"p\">(</span>\n        <span class=\"mi\">32</span><span class=\"p\">,</span>\n        <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span>\n        <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">relu</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">kernel_initializer</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">he_normal</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">same</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">Conv1</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n    <span class=\"p\">)(</span><span class=\"n\">input_img</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">MaxPooling2D</span><span class=\"p\">((</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">pool1</span><span class=\"sh\">\"</span><span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Second conv block\n</span>    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Conv2D</span><span class=\"p\">(</span>\n        <span class=\"mi\">64</span><span class=\"p\">,</span>\n        <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span>\n        <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">relu</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">kernel_initializer</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">he_normal</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">same</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">Conv2</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n    <span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">MaxPooling2D</span><span class=\"p\">((</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">pool2</span><span class=\"sh\">\"</span><span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># We have used two max pool with pool size and strides 2.\n</span>    <span class=\"c1\"># Hence, downsampled feature maps are 4x smaller. The number of\n</span>    <span class=\"c1\"># filters in the last layer is 64. Reshape accordingly before\n</span>    <span class=\"c1\"># passing the output to the RNN part of the model\n</span>    <span class=\"n\">new_shape</span> <span class=\"o\">=</span> <span class=\"p\">((</span><span class=\"n\">img_width</span> <span class=\"o\">//</span> <span class=\"mi\">4</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">img_height</span> <span class=\"o\">//</span> <span class=\"mi\">4</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">64</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Reshape</span><span class=\"p\">(</span><span class=\"n\">target_shape</span><span class=\"o\">=</span><span class=\"n\">new_shape</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">reshape</span><span class=\"sh\">\"</span><span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Dense</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">relu</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">dense1</span><span class=\"sh\">\"</span><span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Dropout</span><span class=\"p\">(</span><span class=\"mf\">0.2</span><span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># RNNs\n</span>    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Bidirectional</span><span class=\"p\">(</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">LSTM</span><span class=\"p\">(</span><span class=\"mi\">128</span><span class=\"p\">,</span> <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"mf\">0.25</span><span class=\"p\">))(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Bidirectional</span><span class=\"p\">(</span><span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">LSTM</span><span class=\"p\">(</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">dropout</span><span class=\"o\">=</span><span class=\"mf\">0.25</span><span class=\"p\">))(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Output layer\n</span>    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">Dense</span><span class=\"p\">(</span>\n        <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">char_to_num</span><span class=\"p\">.</span><span class=\"nf\">get_vocabulary</span><span class=\"p\">())</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">softmax</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">dense2</span><span class=\"sh\">\"</span>\n    <span class=\"p\">)(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Add CTC layer for calculating CTC loss at each step\n</span>    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"nc\">CTCLayer</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">ctc_loss</span><span class=\"sh\">\"</span><span class=\"p\">)(</span><span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Define the model\n</span>    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"nc\">Model</span><span class=\"p\">(</span>\n        <span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">input_img</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">],</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">ocr_model_v1</span><span class=\"sh\">\"</span>\n    <span class=\"p\">)</span>\n    <span class=\"c1\"># Optimizer\n</span>    <span class=\"n\">opt</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">optimizers</span><span class=\"p\">.</span><span class=\"nc\">Adam</span><span class=\"p\">()</span>\n    <span class=\"c1\"># Compile the model and return\n</span>    <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nf\">compile</span><span class=\"p\">(</span><span class=\"n\">optimizer</span><span class=\"o\">=</span><span class=\"n\">opt</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">model</span>\n\n<span class=\"c1\"># Get the model\n</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"nf\">build_model</span><span class=\"p\">()</span>\n<span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nf\">summary</span><span class=\"p\">()</span>\n\n<span class=\"c1\">#---------------------------------------------------------\n#モデルの学習\n#---------------------------------------------------------\n</span><span class=\"n\">epochs</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n<span class=\"n\">early_stopping_patience</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"c1\"># Add early stopping\n</span><span class=\"n\">early_stopping</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">callbacks</span><span class=\"p\">.</span><span class=\"nc\">EarlyStopping</span><span class=\"p\">(</span>\n    <span class=\"n\">monitor</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">val_loss</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">patience</span><span class=\"o\">=</span><span class=\"n\">early_stopping_patience</span><span class=\"p\">,</span> <span class=\"n\">restore_best_weights</span><span class=\"o\">=</span><span class=\"bp\">True</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Train the model\n</span><span class=\"n\">history</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nf\">fit</span><span class=\"p\">(</span>\n    <span class=\"n\">train_dataset</span><span class=\"p\">,</span>\n    <span class=\"n\">validation_data</span><span class=\"o\">=</span><span class=\"n\">validation_dataset</span><span class=\"p\">,</span>\n    <span class=\"n\">epochs</span><span class=\"o\">=</span><span class=\"n\">epochs</span><span class=\"p\">,</span>\n    <span class=\"n\">callbacks</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">early_stopping</span><span class=\"p\">],</span>\n<span class=\"p\">)</span>\n</code></pre></div></div>\n<p data-sourcepos=\"412:1-412:69\">後で呼び出せるようにモデルを保存しておきます。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"414:1-421:3\"><div class=\"highlight\"><pre><code><span class=\"c1\">#---------------------------------------------------------\n#モデルの保存\n#---------------------------------------------------------\n</span>\n<span class=\"c1\"># モデル全体を SavedModel として保存\n</span><span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">saved_model/my_model</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<h2 data-sourcepos=\"423:1-423:21\">\n<span id=\"3-精度の検証\" class=\"fragment\"></span><a href=\"#3-%E7%B2%BE%E5%BA%A6%E3%81%AE%E6%A4%9C%E8%A8%BC\"><i class=\"fa fa-link\"></i></a>3. 精度の検証</h2>\n<p data-sourcepos=\"424:1-425:143\">保存したモデルでどの程度の精度が出るかを確認します。<br>\n学習データと検証データは分けずに、最初に生成したMNISTの画像をランダムに取り出して検証してみます。</p>\n<p data-sourcepos=\"427:1-427:72\">※モデル学習時とは別の.pyファイルを作成しています</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"428:1-555:3\"><div class=\"highlight\"><pre><code><span class=\"c1\">#-------------------------------------------\n#作成済みのモデルでテストデータの精度を確認\n#-------------------------------------------\n</span><span class=\"kn\">import</span> <span class=\"n\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"n\">plt</span>\n<span class=\"kn\">import</span> <span class=\"n\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"kn\">from</span> <span class=\"n\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n<span class=\"kn\">import</span> <span class=\"n\">random</span>\n<span class=\"kn\">import</span> <span class=\"n\">cv2</span>\n<span class=\"kn\">import</span> <span class=\"n\">os</span>\n<span class=\"kn\">import</span> <span class=\"n\">re</span>\n<span class=\"kn\">import</span> <span class=\"n\">glob</span>\n\n<span class=\"kn\">from</span> <span class=\"n\">pathlib</span> <span class=\"kn\">import</span> <span class=\"n\">Path</span>\n<span class=\"kn\">from</span> <span class=\"n\">collections</span> <span class=\"kn\">import</span> <span class=\"n\">Counter</span>\n\n<span class=\"kn\">from</span> <span class=\"n\">keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n<span class=\"kn\">import</span> <span class=\"n\">tensorflow</span> <span class=\"k\">as</span> <span class=\"n\">tf</span>\n<span class=\"kn\">from</span> <span class=\"n\">tensorflow</span> <span class=\"kn\">import</span> <span class=\"n\">keras</span>\n<span class=\"kn\">from</span> <span class=\"n\">tensorflow.keras</span> <span class=\"kn\">import</span> <span class=\"n\">layers</span>\n<span class=\"kn\">import</span> <span class=\"n\">pickle</span>\n\n<span class=\"n\">mnist_picture_width</span> <span class=\"o\">=</span> <span class=\"mi\">28</span>\n<span class=\"n\">mnist_picture_height</span> <span class=\"o\">=</span> <span class=\"mi\">28</span>\n\n<span class=\"n\">str_max_length</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n\n<span class=\"n\">img_width</span> <span class=\"o\">=</span> <span class=\"n\">mnist_picture_width</span> <span class=\"o\">*</span> <span class=\"mi\">10</span>\n<span class=\"n\">img_height</span> <span class=\"o\">=</span> <span class=\"n\">mnist_picture_height</span>\n\n<span class=\"c1\"># 保存したモデルをロードする\n</span><span class=\"n\">loaded_model</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"nf\">load_model</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">saved_model/my_model</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#テストデータを取得\n</span><span class=\"n\">test_data_dir</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">./create_images/</span><span class=\"sh\">\"</span>\n<span class=\"n\">data_dir</span> <span class=\"o\">=</span> <span class=\"nc\">Path</span><span class=\"p\">(</span><span class=\"n\">test_data_dir</span><span class=\"p\">)</span>\n\n<span class=\"n\">test_images</span> <span class=\"o\">=</span> <span class=\"nf\">sorted</span><span class=\"p\">(</span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"nf\">map</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">data_dir</span><span class=\"p\">.</span><span class=\"nf\">glob</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">*.png</span><span class=\"sh\">\"</span><span class=\"p\">)))))</span>\n<span class=\"n\">test_labels</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">re</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">_[0-9]*.png</span><span class=\"sh\">\"</span><span class=\"p\">,</span><span class=\"n\">img</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"p\">.</span><span class=\"n\">path</span><span class=\"p\">.</span><span class=\"n\">sep</span><span class=\"p\">)[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">img</span> <span class=\"ow\">in</span> <span class=\"n\">test_images</span><span class=\"p\">]</span>  <span class=\"c1\">#末尾の識別子を除去してラベルとして取得\n</span><span class=\"n\">test_labels</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">label</span><span class=\"p\">.</span><span class=\"nf\">ljust</span><span class=\"p\">(</span><span class=\"n\">str_max_length</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">s</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"n\">test_labels</span><span class=\"p\">]</span> <span class=\"c1\"># 空白を表す文字sを右に追加する\n</span>\n<span class=\"c1\">#検証にあたり必要な変数をセット\n</span>\n<span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">16</span>\n<span class=\"n\">max_length</span> <span class=\"o\">=</span> <span class=\"nf\">max</span><span class=\"p\">([</span><span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"n\">test_labels</span><span class=\"p\">])</span>\n\n<span class=\"n\">test_characters</span> <span class=\"o\">=</span> <span class=\"nf\">set</span><span class=\"p\">(</span><span class=\"n\">char</span> <span class=\"k\">for</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"n\">test_labels</span> <span class=\"k\">for</span> <span class=\"n\">char</span> <span class=\"ow\">in</span> <span class=\"n\">label</span><span class=\"p\">)</span>\n<span class=\"n\">test_characters</span> <span class=\"o\">=</span> <span class=\"nf\">sorted</span><span class=\"p\">(</span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">test_characters</span><span class=\"p\">))</span>\n\n<span class=\"n\">char_to_num</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">StringLookup</span><span class=\"p\">(</span>\n    <span class=\"n\">vocabulary</span><span class=\"o\">=</span><span class=\"nf\">list</span><span class=\"p\">(</span><span class=\"n\">test_characters</span><span class=\"p\">),</span> <span class=\"n\">mask_token</span><span class=\"o\">=</span><span class=\"bp\">None</span>\n<span class=\"p\">)</span>\n<span class=\"n\">num_to_char</span> <span class=\"o\">=</span> <span class=\"n\">layers</span><span class=\"p\">.</span><span class=\"nc\">StringLookup</span><span class=\"p\">(</span>\n    <span class=\"n\">vocabulary</span><span class=\"o\">=</span><span class=\"n\">char_to_num</span><span class=\"p\">.</span><span class=\"nf\">get_vocabulary</span><span class=\"p\">(),</span> <span class=\"n\">mask_token</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">invert</span><span class=\"o\">=</span><span class=\"bp\">True</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">x_test</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nf\">array</span><span class=\"p\">(</span><span class=\"n\">test_images</span><span class=\"p\">)</span>\n<span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nf\">array</span><span class=\"p\">(</span><span class=\"n\">test_labels</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#MNISTデータをモデルが読み込めるように型変更\n</span><span class=\"k\">def</span> <span class=\"nf\">encode_single_sample</span><span class=\"p\">(</span><span class=\"n\">img_path</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"p\">):</span>\n    <span class=\"c1\"># 1. Read image\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">io</span><span class=\"p\">.</span><span class=\"nf\">read_file</span><span class=\"p\">(</span><span class=\"n\">img_path</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 2. Decode and convert to grayscale\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">io</span><span class=\"p\">.</span><span class=\"nf\">decode_png</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 3. Convert to float32 in [0, 1] range\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">convert_image_dtype</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 4. Resize to the desired size\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">resize</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"n\">img_height</span><span class=\"p\">,</span> <span class=\"n\">img_width</span><span class=\"p\">])</span>\n    <span class=\"c1\"># 5. Transpose the image because we want the time\n</span>    <span class=\"c1\"># dimension to correspond to the width of the image.\n</span>    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"nf\">transpose</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">perm</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">])</span>\n    <span class=\"c1\"># 6. Map the characters in label to numbers\n</span>    <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"nf\">char_to_num</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">strings</span><span class=\"p\">.</span><span class=\"nf\">unicode_split</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">,</span> <span class=\"n\">input_encoding</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">UTF-8</span><span class=\"sh\">\"</span><span class=\"p\">))</span>\n    <span class=\"c1\"># 7. Return a dict as our model is expecting two inputs\n</span>    <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"sh\">\"</span><span class=\"s\">image</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">label</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">label</span><span class=\"p\">}</span>\n\n<span class=\"n\">test_dataset</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">Dataset</span><span class=\"p\">.</span><span class=\"nf\">from_tensor_slices</span><span class=\"p\">((</span><span class=\"n\">x_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">))</span>\n<span class=\"n\">test_dataset</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">test_dataset</span><span class=\"p\">.</span><span class=\"nf\">map</span><span class=\"p\">(</span>\n        <span class=\"n\">encode_single_sample</span><span class=\"p\">,</span> <span class=\"n\">num_parallel_calls</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">AUTOTUNE</span>\n    <span class=\"p\">)</span>\n    <span class=\"p\">.</span><span class=\"nf\">batch</span><span class=\"p\">(</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n    <span class=\"p\">.</span><span class=\"nf\">prefetch</span><span class=\"p\">(</span><span class=\"n\">buffer_size</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">data</span><span class=\"p\">.</span><span class=\"n\">AUTOTUNE</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\">#画像を入力として受け取り、出力として文字列を返す予測モデルを生成\n</span><span class=\"n\">prediction_model</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">models</span><span class=\"p\">.</span><span class=\"nc\">Model</span><span class=\"p\">(</span>\n    <span class=\"n\">loaded_model</span><span class=\"p\">.</span><span class=\"nf\">get_layer</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">image</span><span class=\"sh\">\"</span><span class=\"p\">).</span><span class=\"nb\">input</span><span class=\"p\">,</span> <span class=\"n\">loaded_model</span><span class=\"p\">.</span><span class=\"nf\">get_layer</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">dense2</span><span class=\"sh\">\"</span><span class=\"p\">).</span><span class=\"n\">output</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">decode_batch_predictions</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">):</span>\n    <span class=\"n\">input_len</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"p\">.</span><span class=\"nf\">ones</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">pred</span><span class=\"p\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"c1\"># Use greedy search. For complex tasks, you can use beam search\n</span>    <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">keras</span><span class=\"p\">.</span><span class=\"n\">backend</span><span class=\"p\">.</span><span class=\"nf\">ctc_decode</span><span class=\"p\">(</span><span class=\"n\">pred</span><span class=\"p\">,</span> <span class=\"n\">input_length</span><span class=\"o\">=</span><span class=\"n\">input_len</span><span class=\"p\">,</span> <span class=\"n\">greedy</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">][</span>\n        <span class=\"p\">:,</span> <span class=\"p\">:</span><span class=\"n\">max_length</span>\n    <span class=\"p\">]</span>\n    <span class=\"c1\"># Iterate over the results and get back the text\n</span>    <span class=\"n\">output_text</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">res</span> <span class=\"ow\">in</span> <span class=\"n\">results</span><span class=\"p\">:</span>\n        <span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">strings</span><span class=\"p\">.</span><span class=\"nf\">reduce_join</span><span class=\"p\">(</span><span class=\"nf\">num_to_char</span><span class=\"p\">(</span><span class=\"n\">res</span><span class=\"p\">)).</span><span class=\"nf\">numpy</span><span class=\"p\">().</span><span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">utf-8</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"n\">output_text</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">res</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">output_text</span>\n\n<span class=\"c1\">#検証結果の表示\n</span><span class=\"k\">for</span> <span class=\"n\">batch</span> <span class=\"ow\">in</span> <span class=\"n\">test_dataset</span><span class=\"p\">.</span><span class=\"nf\">take</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n    <span class=\"n\">batch_images</span> <span class=\"o\">=</span> <span class=\"n\">batch</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">image</span><span class=\"sh\">\"</span><span class=\"p\">]</span>\n    <span class=\"n\">batch_labels</span> <span class=\"o\">=</span> <span class=\"n\">batch</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">label</span><span class=\"sh\">\"</span><span class=\"p\">]</span>\n\n    <span class=\"n\">preds</span> <span class=\"o\">=</span> <span class=\"n\">prediction_model</span><span class=\"p\">.</span><span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"n\">batch_images</span><span class=\"p\">)</span>\n    <span class=\"n\">pred_texts</span> <span class=\"o\">=</span> <span class=\"nf\">decode_batch_predictions</span><span class=\"p\">(</span><span class=\"n\">preds</span><span class=\"p\">)</span>\n\n    <span class=\"n\">orig_texts</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">label</span> <span class=\"ow\">in</span> <span class=\"n\">batch_labels</span><span class=\"p\">:</span>\n        <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"p\">.</span><span class=\"n\">strings</span><span class=\"p\">.</span><span class=\"nf\">reduce_join</span><span class=\"p\">(</span><span class=\"nf\">num_to_char</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)).</span><span class=\"nf\">numpy</span><span class=\"p\">().</span><span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">utf-8</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"n\">orig_texts</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)</span>\n\n    <span class=\"n\">_</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"nf\">subplots</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">))</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">pred_texts</span><span class=\"p\">)):</span>\n        <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">batch_images</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"p\">:,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"mi\">255</span><span class=\"p\">).</span><span class=\"nf\">numpy</span><span class=\"p\">().</span><span class=\"nf\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"p\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\n        <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">img</span><span class=\"p\">.</span><span class=\"n\">T</span>\n        <span class=\"n\">title</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">Prediction: </span><span class=\"si\">{</span><span class=\"n\">pred_texts</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"si\">}</span><span class=\"sh\">\"</span>\n        <span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">//</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">4</span><span class=\"p\">].</span><span class=\"nf\">imshow</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">gray</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">//</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">4</span><span class=\"p\">].</span><span class=\"nf\">set_title</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"p\">)</span>\n        <span class=\"n\">ax</span><span class=\"p\">[</span><span class=\"n\">i</span> <span class=\"o\">//</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">4</span><span class=\"p\">].</span><span class=\"nf\">axis</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">off</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"nf\">show</span><span class=\"p\">()</span>\n</code></pre></div></div>\n<h2 data-sourcepos=\"557:1-557:12\">\n<span id=\"4-結果\" class=\"fragment\"></span><a href=\"#4-%E7%B5%90%E6%9E%9C\"><i class=\"fa fa-link\"></i></a>4. 結果</h2>\n<p data-sourcepos=\"558:1-559:94\">３回試してみた結果はなんと全問正解。<br>\nたった1000枚程度のテストデータを与えただけなのに、凄い精度です。</p>\n<p data-sourcepos=\"561:1-563:123\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F8750c846-4dfe-f603-de72-b752fe69fd98.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=3a953b85d82b7678cc4a725de583f00d\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F8750c846-4dfe-f603-de72-b752fe69fd98.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=3a953b85d82b7678cc4a725de583f00d\" alt=\"Result1.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F8750c846-4dfe-f603-de72-b752fe69fd98.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=d7b07c4a1f3a153ca7bcc2ba150b11df 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/8750c846-4dfe-f603-de72-b752fe69fd98.png\" loading=\"lazy\"></a><br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Feaf2ea23-1fbe-8ef6-6bc9-f3bced50ed83.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=abff27b025a570501265bff29899ed50\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Feaf2ea23-1fbe-8ef6-6bc9-f3bced50ed83.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=abff27b025a570501265bff29899ed50\" alt=\"Result2.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Feaf2ea23-1fbe-8ef6-6bc9-f3bced50ed83.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=2da46dbdfdaad20241228f54ac44ab18 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/eaf2ea23-1fbe-8ef6-6bc9-f3bced50ed83.png\" loading=\"lazy\"></a><br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F5f8a1fe5-08d1-6c1b-0a7f-a16216069c3e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=b259713364bfaa51912dc1bf24a1abfa\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F5f8a1fe5-08d1-6c1b-0a7f-a16216069c3e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=b259713364bfaa51912dc1bf24a1abfa\" alt=\"Result3.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2F5f8a1fe5-08d1-6c1b-0a7f-a16216069c3e.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=6676c68f5e5b892b17e78727d7d43197 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/5f8a1fe5-08d1-6c1b-0a7f-a16216069c3e.png\" loading=\"lazy\"></a></p>\n<h1 data-sourcepos=\"566:1-566:23\">\n<span id=\"7-次のステップ\" class=\"fragment\"></span><a href=\"#7-%E6%AC%A1%E3%81%AE%E3%82%B9%E3%83%86%E3%83%83%E3%83%97\"><i class=\"fa fa-link\"></i></a>7. 次のステップ</h1>\n<p data-sourcepos=\"568:1-568:264\">本記事の目的は達成したため、ひとまずここまでとさせて頂きますが、実際に私が目標とする「伝票を読み取ってデータを返す」を実現するまでには以下のようなステップが必要と考えています。</p>\n<ul data-sourcepos=\"569:1-572:0\">\n<li data-sourcepos=\"569:1-569:137\">実際に読み取ろうとするデータには桁区切りのカンマがついているため、そこを考慮する必要がある</li>\n<li data-sourcepos=\"570:1-570:193\">読み取りたい伝票は手書きだけでなく、PCで出力したものが多いので、整ったデータに対しても精度が出るようにチューニングする必要がある</li>\n<li data-sourcepos=\"571:1-572:0\">伝票のどの部分にどの数値があるかを指定して読み取らせる</li>\n</ul>\n<h1 data-sourcepos=\"573:1-573:13\">\n<span id=\"8-appendix\" class=\"fragment\"></span><a href=\"#8-appendix\"><i class=\"fa fa-link\"></i></a>8. Appendix</h1>\n<p data-sourcepos=\"574:1-575:138\">上記でも触れていますが、実際に読み取りたい画像データはPCで出力したものを想定しています。<br>\nこの記事の目的とは逸れますが、その画像データを生成するプログラムを付録として置いておきます。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"578:1-630:3\"><div class=\"highlight\"><pre><code><span class=\"kn\">from</span> <span class=\"n\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span><span class=\"p\">,</span> <span class=\"n\">ImageDraw</span><span class=\"p\">,</span> <span class=\"n\">ImageFont</span>\n<span class=\"kn\">import</span> <span class=\"n\">random</span>\n\n<span class=\"c1\">#テストデータの格納先\n</span><span class=\"n\">output_folder_path</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">./test_data/</span><span class=\"sh\">\"</span>\n<span class=\"c1\">#数値列の最大桁数\n</span><span class=\"n\">str_max_length</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"c1\">#MNISTを用いて生成した画像のサイズ\n</span><span class=\"n\">mnist_picture_width</span> <span class=\"o\">=</span> <span class=\"mi\">28</span> <span class=\"o\">*</span> <span class=\"n\">str_max_length</span>\n<span class=\"n\">mnist_picture_height</span> <span class=\"o\">=</span> <span class=\"mi\">28</span>\n<span class=\"c1\">#作成するテストデータ数\n</span><span class=\"n\">test_data_count</span> <span class=\"o\">=</span> <span class=\"mi\">100</span>\n\n<span class=\"c1\"># PCローカルのフォントへのパスと、フォントサイズを指定\n</span><span class=\"n\">font</span> <span class=\"o\">=</span> <span class=\"n\">ImageFont</span><span class=\"p\">.</span><span class=\"nf\">truetype</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">C:/Windows/Fonts/meiryob.ttc</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"mi\">24</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#任意な数値列の長さを取得\n</span><span class=\"k\">def</span> <span class=\"nf\">random_length</span><span class=\"p\">():</span>\n    <span class=\"k\">return</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"nf\">randint</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">str_max_length</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#ランダムに数字を取得\n</span><span class=\"k\">def</span> <span class=\"nf\">random_number</span><span class=\"p\">(</span><span class=\"n\">need_not_zero</span> <span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"nf\">randint</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">need_not_zero</span> <span class=\"k\">else</span> <span class=\"n\">random</span><span class=\"p\">.</span><span class=\"nf\">randint</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">create_random_value</span><span class=\"p\">():</span>\n\n    <span class=\"n\">return_value</span> <span class=\"o\">=</span> <span class=\"sh\">''</span>                    <span class=\"c1\">#（カンマを含む）数字文字列\n</span>    <span class=\"n\">value_length</span> <span class=\"o\">=</span> <span class=\"nf\">random_length</span><span class=\"p\">()</span>      <span class=\"c1\">#数値列の長さを任意に指定\n</span>\n    <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">value_length</span><span class=\"p\">):</span>\n        <span class=\"n\">return_value</span> <span class=\"o\">+=</span> <span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"nf\">random_number</span><span class=\"p\">(</span><span class=\"bp\">True</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">j</span><span class=\"o\">==</span><span class=\"mi\">0</span> <span class=\"k\">else</span> <span class=\"nf\">random_number</span><span class=\"p\">())</span>\n    \n    <span class=\"c1\">#カンマを挿入\n</span>    <span class=\"k\">return</span> <span class=\"n\">return_value</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">{:,}</span><span class=\"sh\">'</span><span class=\"p\">.</span><span class=\"nf\">format</span><span class=\"p\">(</span><span class=\"nf\">int</span><span class=\"p\">(</span><span class=\"n\">return_value</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># 文字描画の初期位置（画像左上からx, yだけ離れた位置）\n</span><span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">test_data_count</span><span class=\"p\">):</span>\n    <span class=\"n\">label</span><span class=\"p\">,</span><span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"nf\">create_random_value</span><span class=\"p\">()</span>    <span class=\"c1\">#ランダムに数値列を生成\n</span>\n    <span class=\"c1\"># RGB, 画像サイズ, 背景色を設定\n</span>    <span class=\"n\">im</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">new</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">RGB</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">mnist_picture_width</span><span class=\"p\">,</span> <span class=\"n\">mnist_picture_height</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">,</span> <span class=\"mi\">256</span><span class=\"p\">))</span>\n\n    <span class=\"n\">draw</span> <span class=\"o\">=</span> <span class=\"n\">ImageDraw</span><span class=\"p\">.</span><span class=\"nc\">Draw</span><span class=\"p\">(</span><span class=\"n\">im</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 描画位置、描画する文字、文字色、フォントを指定\n</span>    <span class=\"n\">draw</span><span class=\"p\">.</span><span class=\"nf\">text</span><span class=\"p\">((</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">),</span> <span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">fill</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">font</span><span class=\"o\">=</span><span class=\"n\">font</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># ファイルに出力\n</span>    <span class=\"n\">im</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">output_folder_path</span> <span class=\"o\">+</span> <span class=\"sh\">\"</span><span class=\"s\">./</span><span class=\"sh\">\"</span> <span class=\"o\">+</span> <span class=\"n\">label</span> <span class=\"o\">+</span><span class=\"sh\">\"</span><span class=\"s\">.png</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<p data-sourcepos=\"632:1-632:9\">出力例</p>\n<p data-sourcepos=\"634:1-635:126\">4114420306.png<br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Fa98a2b64-84df-3622-4c27-d9a9421f76bb.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=a23a48767aee6e25001e4783e92b08e3\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Fa98a2b64-84df-3622-4c27-d9a9421f76bb.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=a23a48767aee6e25001e4783e92b08e3\" alt=\"4114420306.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Fa98a2b64-84df-3622-4c27-d9a9421f76bb.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=2d747a056934f89ee01cf91f8ec68480 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/a98a2b64-84df-3622-4c27-d9a9421f76bb.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"637:1-638:121\">99454.png<br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Fad17887e-71c2-1c6f-d34f-33a91ad79a3c.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=af8a49ac547222167dce644c01f4705d\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Fad17887e-71c2-1c6f-d34f-33a91ad79a3c.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=af8a49ac547222167dce644c01f4705d\" alt=\"99454.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Fad17887e-71c2-1c6f-d34f-33a91ad79a3c.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=6064af218b024075f12f6e62201bf792 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/ad17887e-71c2-1c6f-d34f-33a91ad79a3c.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"640:1-641:119\">491.png<br>\n<a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Fc83b7d0e-568f-3d4b-3a10-f30d02fc65eb.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=8b5aa81c63a6858df354a690d48d6e1c\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Fc83b7d0e-568f-3d4b-3a10-f30d02fc65eb.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=8b5aa81c63a6858df354a690d48d6e1c\" alt=\"491.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F388176%2Fc83b7d0e-568f-3d4b-3a10-f30d02fc65eb.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=9f4923b831bd4fdc160eeb37e3184433 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/c83b7d0e-568f-3d4b-3a10-f30d02fc65eb.png\" loading=\"lazy\"></a></p>\n<p data-sourcepos=\"643:1-645:52\">尚、このコードで生成された画像を上記のモデルで検証させても、精度は全く出ません。<br>\n上記と同じ方法で精度確認を行った所、ほとんど0%に近しい結果になりました。<br>\n1文字のデータですら間違う有様です。</p>\n<p data-sourcepos=\"647:1-648:311\">いわゆる過学習状態なのでしょうか。詳細は不明です。<br>\n現状は単にMNISTの画像を単純に張り合わせているだけですが、ランダム性と規則性を考慮し、回転処理や拡大・縮小処理をランダムに含ませたり、横方向の縮小を適用する事が必要かもしれません（近々検証しようと考えています）</p>\n<p data-sourcepos=\"650:1-651:132\">ちなみに実はこのコード、「0」のデータを生成できない欠陥コードです。<br>\nモデルの仕組みから言って特に問題はないと思いますが、気になる方は修正して使ってください。</p>\n<h1 data-sourcepos=\"653:1-653:17\">\n<span id=\"9-振り返り\" class=\"fragment\"></span><a href=\"#9-%E6%8C%AF%E3%82%8A%E8%BF%94%E3%82%8A\"><i class=\"fa fa-link\"></i></a>9. 振り返り</h1>\n<p data-sourcepos=\"654:1-657:36\">こんな簡単にOCRが実装できるのは凄いの一言です。Pythonは本当に出来る事が多いですね。<br>\n参考にしたkerasのサイトは全て英語だったため、ここが一番苦労しました。英語、大事…<br>\nとはいえChatGPTが全部翻訳してくれるんじゃね？という説もありますけどね。<br>\nいやー、すごい時代です。</p>\n<h1 data-sourcepos=\"659:1-659:18\">\n<span id=\"10-参考文献\" class=\"fragment\"></span><a href=\"#10-%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE\"><i class=\"fa fa-link\"></i></a>10. 参考文献</h1>\n<p data-sourcepos=\"660:1-664:230\"><a href=\"https://keras.io/examples/vision/captcha_ocr/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\">https://keras.io/examples/vision/captcha_ocr/</a><br>\n<a href=\"https://www.netforce.co.jp/techblog/entry/0001.shtml\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\">https://www.netforce.co.jp/techblog/entry/0001.shtml</a><br>\n<a href=\"https://magazine.techacademy.jp/magazine/18981\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\">https://magazine.techacademy.jp/magazine/18981</a><br>\n<a href=\"https://qiita.com/YoshikiIto/items/ace92dc9b05e375b8326\" class=\"autolink\" id=\"reference-f1bc037c2bc41030ced6\">https://qiita.com/YoshikiIto/items/ace92dc9b05e375b8326</a><br>\n<a href=\"https://self-development.info/%E3%80%90%E8%B6%85%E7%B0%A1%E5%8D%98%E3%80%91tensorflow%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BF%9D%E5%AD%98%E3%83%BB%E5%BE%A9%E5%85%83%E3%83%BB%E5%86%8D%E5%88%A9/\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\">https://self-development.info/%E3%80%90%E8%B6%85%E7%B0%A1%E5%8D%98%E3%80%91tensorflow%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BF%9D%E5%AD%98%E3%83%BB%E5%BE%A9%E5%85%83%E3%83%BB%E5%86%8D%E5%88%A9/</a></p>\n",
      "body": "# はじめに\nこんにちは。cosumi77と申します。Qiita初投稿です。\n\n普段はド田舎でSEとしてvb.netの開発に従事しておりますが、PythonやAIについてはつい最近まで「なにそれ？美味しいの？」状態でした。\n本記事は、そんな私がナウい（笑）言語を駆使して課題に取り組みましたので、それを紹介するものとなります。\n\n「こんな~~クソ~~記事をネット上に公開して誰が一体読むのか…？」という気持ちを抑えつつ、アウトプットの練習として公開いたします。\n\n記載内容に誤り等ありましたら、ご指摘いただけると大変ありがたいです。\nどうぞ宜しくお願い致します。\n\n\n# 目次\n[1.実行環境](#1-実行環境)\n[2.本記事の目的](#2-本記事の目的)\n[3.前提となる問題意識](#3-前提となる問題意識)\n[4.モデルの概要](#4-モデルの概要)\n[5.目的に至るまでの道のり](#5-目的に至るまでの道のり)\n[6.やってみた](#6-やってみた)\n[7.次のステップ](#7-次のステップ)\n[8.Appendix](#8-appendix)\n[9.振り返り](#9-振り返り)\n[10.参考文献](#10-参考文献)\n\n# 1. 実行環境\nWindows11 22H2\nVisual Studio Code\n\nPython 3.10.4\n\nmatplotlib                   3.5.2\nnumpy                        1.22.4\nkeras                        2.9.0\ntensorflow                   2.9.1\n\n# 2. 本記事の目的\n任意の長さのの整数（例：123456789、98765、12 等）の画像を「シーケンス認識によるAI-OCRモデル([OCR model for reading Captchas](https://keras.io/examples/vision/captcha_ocr/))」を用いて読み取り、データとして吐き出す。\n\n# 3. 前提となる問題意識\n私は、主に客先などで伝票を見る機会が多いです。\n伝票は紙またはpdf形式で受け渡しがされており、受け取った伝票を見ながら別のシステムへチマチマと転記するようなシーンが多く見られます。\nそこで、「もしこれをAIで読み取り、データとして起こせると~~飯の種~~作業効率化になるよな～…」と思いました。\n\nまずは第一ステップとして、手書きの数値を学習させて、どの程度読み取れるものなのかをやってみたいと思います。\n\n# 4. モデルの概要\n文字データを認識するためのアルゴリズムはいろいろあるようですが、ここではCRNN（CNN + RNN）とCTC損失関数を使ったシーケンス認識（Sequence Recognition）を用いています。\n\n![rcnn_ctc.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/9f050732-2ae6-3b2d-be4f-217ac1a94415.png)\n出典: https://arxiv.org/abs/1507.05717\n\nCNNとは「畳み込みニューラルネットワーク」とも呼ばれ、人間の脳の視覚野と似た構造を持つ 「畳み込み層」 という層を使って特徴抽出を行うニューラルネットワークです。主に画像認識の分野で高い性能を発揮します。\nこのモデルでは、画像内の各数字を識別して画像データ列としてRNNへ渡す役割を担っています。\n\nRNNとは「回帰型ニューラルネットワーク」などとも呼ばれ、数値の時系列データなどのシーケンスデータのパターンを認識するように設計されたニューラルネットワークのモデルです。\nこのモデルでは、CNNから渡された画像データ列から推定される文字列の算出を行います。\n\nCTC損失関数はRNNの出力および正解データで最適化を行います。\n\n# 5. 目的に至るまでの道のり\n1. MNISTから手書き文字を取得し数字のみを抜き取った後でランダムな画像数字列を生成\n2. シーケンス認識によるAI-OCR（文字認識）モデルを用いた学習\n3. テストデータによる検証\n\n<a id=\"anchor4\"></a>\n# 6. やってみた！\n\n## 1. MNISTから手書き文字を取得し、ランダムな画像数字列を生成\n\n### ポイント\n- MNISTから最大10個の画像データをランダムに結合して学習データを作成します\n- 生成する画像のサイズを統一するために適宜余白を埋め込んでいます\n- 最終目的は「伝票に記載された数値を読み取る事」なので、白黒を反転しています\n- 生成した画像のファイル名には、画像に記された数字列をセットします\nただし、ランダムに数値を生成した場合、値が重複するとファイル名も被ってしまうため、これらを識別できるように「(数値)_(識別番号).png」のようなファイル名にしました\n\n```python\n#実行前に実行フォルダ直下にcreate_imagesフォルダを作成してください\n\n#各種インポート\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport random\nimport cv2\nimport os\nimport re\nimport glob\n\nfrom pathlib import Path\nfrom collections import Counter\nfrom PIL import Image\n\nfrom keras.datasets import mnist\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n#作成する画像数\ncreate_data_count = 1000\n\n#MNISTの画像サイズ\nmnist_picture_width = 28\nmnist_picture_height = 28\n\n#数値列の最大桁数\nstr_max_length = 10\n\n#作成した画像の保存先\nsource_path = \"./create_images/\"\ndata_dir = Path(source_path)\n\n#余白画像\nimg_white = np.ones((mnist_picture_width,mnist_picture_height),np.uint8)*255\n\n#################################################################\n#データの準備\n#################################################################\n\n#---------------------------------------------------------\n#MNISTデータの準備、および、加工\n#---------------------------------------------------------\n(mnist_images, mnist_labels)= mnist.load_data()[0]\n\n#学習データの内、0以外のインデックスを取得\nnot_zero_index = (np.where(mnist_labels!=0))[0]\n\n#データセットの色の反転\nmnist_images = 255 - mnist_images\n\n#---------------------------------------------------------\n#MNISTから数字を取得し、最大10桁の画像データを生成\n#---------------------------------------------------------\ndef create_images(mnist_images, mnist_labels, not_zero_index):\n\n    for j in range(0,create_data_count):\n\n        #1から最大桁数の範囲で乱数を生成\n        digit_count = random.randint(1, str_max_length)\n        label_name = ''\n\n        #画像を結合して任意の数値列を作り出す\n        for i in range(0,digit_count):\n\n            if i == 0:\n                #MNISTデータを任意に取り出すための乱数(先頭の数値は0以外で)\n                choise_not_zero = random.randint(0, len(not_zero_index)-1)\n                #初期画像をセット\n                img = mnist_images[not_zero_index[choise_not_zero]]\n                #ラベル情報をセット\n                label_name = str(mnist_labels[not_zero_index[choise_not_zero]])\n            else:\n                #MNISTデータを任意に取り出すための乱数\n                choise_image_index = random.randint(0, mnist_images.shape[0]-1)\n                #画像を横に結合\n                img = cv2.hconcat([img,mnist_images[choise_image_index]])\n                #ラベル情報をセット\n                label_name += str(mnist_labels[choise_image_index])\n        \n        #余白の挿入\n        for k in range(digit_count,str_max_length):\n                img = cv2.hconcat([img,img_white])\n\n        #画像データを保存\n        cv2.imwrite(source_path + label_name + '_' + str(j) +'.png', img)   #数値列の重複を避けるために末尾に識別子を付与\n\n#学習データ、検証データの生成\ncreate_images(mnist_images, mnist_labels, not_zero_index)\n```\n\n結果、以下のような画像が生成されます。\n\n790467491_880.png\n![790467491_880.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/118c242e-ee59-660d-d48f-84b1b6fb0737.png)\n1_462.png\n![1_462.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/3d6f2aee-8c47-1c38-8a81-6ca2f80d3ab8.png)\n1334361586_422.png\n![1334361586_422.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/11ead10b-2f13-4786-ae47-500cbbf1c859.png)\n7792_97.png\n![7792_97.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/4ee44532-9243-1488-7641-1653ad2a12d1.png)\n\n\n## 2. シーケンス認識によるAI-OCR（文字認識）モデルを用いた学習\nこのあたりは先程ご紹介した以下のページをほとんど~~パクり~~流用しながら、少しだけ変えて利用しました。元のコードから変更した箇所を中心にコメントを付しています。\n\nhttps://keras.io/examples/vision/captcha_ocr/\n\n```python\n#---------------------------------------------------------\n#データ準備\n#---------------------------------------------------------\n\n# Get list of all the images\nimages = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\n\n#ファイル名の末尾の識別子を除去し、ラベルとして取得\n# 【変更後】\nlabels = [re.split(\"_[0-9]*.png\",img.split(os.path.sep)[-1])[0] for img in images]  \n# 【変更前】\n# labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in images]\n\n# 空白を表す文字sを右に追加する（サンプルコードは固定文字数だが今回のケースは文字数が変動する）\nlabels = [label.ljust(str_max_length, \"s\") for label in labels] \n\ncharacters = set(char for label in labels for char in label)\ncharacters = sorted(list(characters))\n\nprint(\"Number of images found: \", len(images))\nprint(\"Number of labels found: \", len(labels))\nprint(labels[0])\nprint(\"Number of unique characters: \", len(characters))\nprint(\"Characters present: \", characters)\n\n# Batch size for training and validation\nbatch_size = 16\n\n# Desired image dimensions\n#生成した画像データのサイズを指定\n#【変更後】\nimg_width = mnist_picture_width * 10 \nimg_height = mnist_picture_height\n#【変更前】\n#img_width = 200\n#img_height = 50\n\n# Factor by which the image is going to be downsampled\n# by the convolutional blocks. We will be using two\n# convolution blocks and each block will have\n# a pooling layer which downsample the features by a factor of 2.\n# Hence total downsampling factor would be 4.\ndownsample_factor = 4\n\n# Maximum length of any captcha in the dataset\nmax_length = max([len(label) for label in labels])\n\n#---------------------------------------------------------\n#データ加工\n#---------------------------------------------------------\n# Mapping characters to integers\nchar_to_num = layers.StringLookup(\n    vocabulary=list(characters), mask_token=None\n)\n\n# Mapping integers back to original characters\nnum_to_char = layers.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n)\n\ndef split_data(images, labels, train_size=0.9, shuffle=True):\n    # 1. Get the total size of the dataset\n    size = len(images)\n    # 2. Make an indices array and shuffle it, if required\n    indices = np.arange(size)\n    if shuffle:\n        np.random.shuffle(indices)\n    # 3. Get the size of training samples\n    train_samples = int(size * train_size)\n    # 4. Split data into training and validation sets\n    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n    return x_train, x_valid, y_train, y_valid\n\n# Splitting data into training and validation sets\nx_train, x_valid, y_train, y_valid = split_data(np.array(images), np.array(labels))\n\ndef encode_single_sample(img_path, label):\n    # 1. Read image\n    img = tf.io.read_file(img_path)\n    # 2. Decode and convert to grayscale\n    img = tf.io.decode_png(img, channels=1)\n    # 3. Convert to float32 in [0, 1] range\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # 4. Resize to the desired size\n    img = tf.image.resize(img, [img_height, img_width])\n    # 5. Transpose the image because we want the time\n    # dimension to correspond to the width of the image.\n    img = tf.transpose(img, perm=[1, 0, 2])\n    # 6. Map the characters in label to numbers\n    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n    # 7. Return a dict as our model is expecting two inputs\n    return {\"image\": img, \"label\": label}\n\n#---------------------------------------------------------\n#TensorFlowデータセットを生成\n#---------------------------------------------------------\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntrain_dataset = (\n    train_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\nvalidation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\nvalidation_dataset = (\n    validation_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\n#---------------------------------------------------------\n#今回使用するモデル\n#---------------------------------------------------------\nclass CTCLayer(layers.Layer):\n\n    def __init__(self, name=None):\n        super().__init__(name=name)\n        self.loss_fn = keras.backend.ctc_batch_cost\n\n    def call(self, y_true, y_pred):\n        # Compute the training-time loss value and add it\n        # to the layer using `self.add_loss()`.\n        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n\n        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n\n        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n        self.add_loss(loss)\n\n        # At test time, just return the computed predictions\n        return y_pred\n\ndef build_model():\n    # Inputs to the model\n    input_img = layers.Input(\n        shape=(img_width, img_height, 1), name=\"image\", dtype=\"float32\"\n    )\n    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"float32\")\n\n    # First conv block\n    x = layers.Conv2D(\n        32,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv1\",\n    )(input_img)\n    x = layers.MaxPooling2D((2, 2), name=\"pool1\")(x)\n\n    # Second conv block\n    x = layers.Conv2D(\n        64,\n        (3, 3),\n        activation=\"relu\",\n        kernel_initializer=\"he_normal\",\n        padding=\"same\",\n        name=\"Conv2\",\n    )(x)\n    x = layers.MaxPooling2D((2, 2), name=\"pool2\")(x)\n\n    # We have used two max pool with pool size and strides 2.\n    # Hence, downsampled feature maps are 4x smaller. The number of\n    # filters in the last layer is 64. Reshape accordingly before\n    # passing the output to the RNN part of the model\n    new_shape = ((img_width // 4), (img_height // 4) * 64)\n    x = layers.Reshape(target_shape=new_shape, name=\"reshape\")(x)\n    x = layers.Dense(64, activation=\"relu\", name=\"dense1\")(x)\n    x = layers.Dropout(0.2)(x)\n\n    # RNNs\n    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(x)\n    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n\n    # Output layer\n    x = layers.Dense(\n        len(char_to_num.get_vocabulary()) + 1, activation=\"softmax\", name=\"dense2\"\n    )(x)\n\n    # Add CTC layer for calculating CTC loss at each step\n    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n\n    # Define the model\n    model = keras.models.Model(\n        inputs=[input_img, labels], outputs=output, name=\"ocr_model_v1\"\n    )\n    # Optimizer\n    opt = keras.optimizers.Adam()\n    # Compile the model and return\n    model.compile(optimizer=opt)\n    return model\n\n# Get the model\nmodel = build_model()\nmodel.summary()\n\n#---------------------------------------------------------\n#モデルの学習\n#---------------------------------------------------------\nepochs = 100\nearly_stopping_patience = 10\n# Add early stopping\nearly_stopping = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n)\n\n# Train the model\nhistory = model.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=epochs,\n    callbacks=[early_stopping],\n)\n```\n\n\n後で呼び出せるようにモデルを保存しておきます。\n\n```python\n#---------------------------------------------------------\n#モデルの保存\n#---------------------------------------------------------\n\n# モデル全体を SavedModel として保存\nmodel.save('saved_model/my_model')\n```\n\n## 3. 精度の検証\n保存したモデルでどの程度の精度が出るかを確認します。\n学習データと検証データは分けずに、最初に生成したMNISTの画像をランダムに取り出して検証してみます。\n\n※モデル学習時とは別の.pyファイルを作成しています\n```python\n#-------------------------------------------\n#作成済みのモデルでテストデータの精度を確認\n#-------------------------------------------\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport random\nimport cv2\nimport os\nimport re\nimport glob\n\nfrom pathlib import Path\nfrom collections import Counter\n\nfrom keras.datasets import mnist\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport pickle\n\nmnist_picture_width = 28\nmnist_picture_height = 28\n\nstr_max_length = 10\n\nimg_width = mnist_picture_width * 10\nimg_height = mnist_picture_height\n\n# 保存したモデルをロードする\nloaded_model = tf.keras.models.load_model('saved_model/my_model')\n\n#テストデータを取得\ntest_data_dir = \"./create_images/\"\ndata_dir = Path(test_data_dir)\n\ntest_images = sorted(list(map(str, list(data_dir.glob(\"*.png\")))))\ntest_labels = [re.split(\"_[0-9]*.png\",img.split(os.path.sep)[-1])[0] for img in test_images]  #末尾の識別子を除去してラベルとして取得\ntest_labels = [label.ljust(str_max_length, \"s\") for label in test_labels] # 空白を表す文字sを右に追加する\n\n#検証にあたり必要な変数をセット\n\nbatch_size = 16\nmax_length = max([len(label) for label in test_labels])\n\ntest_characters = set(char for label in test_labels for char in label)\ntest_characters = sorted(list(test_characters))\n\nchar_to_num = layers.StringLookup(\n    vocabulary=list(test_characters), mask_token=None\n)\nnum_to_char = layers.StringLookup(\n    vocabulary=char_to_num.get_vocabulary(), mask_token=None, invert=True\n)\n\nx_test = np.array(test_images)\ny_test = np.array(test_labels)\n\n#MNISTデータをモデルが読み込めるように型変更\ndef encode_single_sample(img_path, label):\n    # 1. Read image\n    img = tf.io.read_file(img_path)\n    # 2. Decode and convert to grayscale\n    img = tf.io.decode_png(img, channels=1)\n    # 3. Convert to float32 in [0, 1] range\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # 4. Resize to the desired size\n    img = tf.image.resize(img, [img_height, img_width])\n    # 5. Transpose the image because we want the time\n    # dimension to correspond to the width of the image.\n    img = tf.transpose(img, perm=[1, 0, 2])\n    # 6. Map the characters in label to numbers\n    label = char_to_num(tf.strings.unicode_split(label, input_encoding=\"UTF-8\"))\n    # 7. Return a dict as our model is expecting two inputs\n    return {\"image\": img, \"label\": label}\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\ntest_dataset = (\n    test_dataset.map(\n        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n    )\n    .batch(batch_size)\n    .prefetch(buffer_size=tf.data.AUTOTUNE)\n)\n\n#画像を入力として受け取り、出力として文字列を返す予測モデルを生成\nprediction_model = keras.models.Model(\n    loaded_model.get_layer(name=\"image\").input, loaded_model.get_layer(name=\"dense2\").output\n)\n\ndef decode_batch_predictions(pred):\n    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n    # Use greedy search. For complex tasks, you can use beam search\n    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n        :, :max_length\n    ]\n    # Iterate over the results and get back the text\n    output_text = []\n    for res in results:\n        res = tf.strings.reduce_join(num_to_char(res)).numpy().decode(\"utf-8\")\n        output_text.append(res)\n    return output_text\n\n#検証結果の表示\nfor batch in test_dataset.take(1):\n    batch_images = batch[\"image\"]\n    batch_labels = batch[\"label\"]\n\n    preds = prediction_model.predict(batch_images)\n    pred_texts = decode_batch_predictions(preds)\n\n    orig_texts = []\n    for label in batch_labels:\n        label = tf.strings.reduce_join(num_to_char(label)).numpy().decode(\"utf-8\")\n        orig_texts.append(label)\n\n    _, ax = plt.subplots(4, 4, figsize=(15, 5))\n    for i in range(len(pred_texts)):\n        img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n        img = img.T\n        title = f\"Prediction: {pred_texts[i]}\"\n        ax[i // 4, i % 4].imshow(img, cmap=\"gray\")\n        ax[i // 4, i % 4].set_title(title)\n        ax[i // 4, i % 4].axis(\"off\")\n\nplt.show()\n```\n\n## 4. 結果\n３回試してみた結果はなんと全問正解。\nたった1000枚程度のテストデータを与えただけなのに、凄い精度です。\n\n![Result1.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/8750c846-4dfe-f603-de72-b752fe69fd98.png)\n![Result2.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/eaf2ea23-1fbe-8ef6-6bc9-f3bced50ed83.png)\n![Result3.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/5f8a1fe5-08d1-6c1b-0a7f-a16216069c3e.png)\n\n\n# 7. 次のステップ\n\n本記事の目的は達成したため、ひとまずここまでとさせて頂きますが、実際に私が目標とする「伝票を読み取ってデータを返す」を実現するまでには以下のようなステップが必要と考えています。\n- 実際に読み取ろうとするデータには桁区切りのカンマがついているため、そこを考慮する必要がある\n- 読み取りたい伝票は手書きだけでなく、PCで出力したものが多いので、整ったデータに対しても精度が出るようにチューニングする必要がある\n- 伝票のどの部分にどの数値があるかを指定して読み取らせる\n\n# 8. Appendix\n上記でも触れていますが、実際に読み取りたい画像データはPCで出力したものを想定しています。\nこの記事の目的とは逸れますが、その画像データを生成するプログラムを付録として置いておきます。\n\n\n```python\nfrom PIL import Image, ImageDraw, ImageFont\nimport random\n\n#テストデータの格納先\noutput_folder_path = \"./test_data/\"\n#数値列の最大桁数\nstr_max_length = 10\n#MNISTを用いて生成した画像のサイズ\nmnist_picture_width = 28 * str_max_length\nmnist_picture_height = 28\n#作成するテストデータ数\ntest_data_count = 100\n\n# PCローカルのフォントへのパスと、フォントサイズを指定\nfont = ImageFont.truetype('C:/Windows/Fonts/meiryob.ttc', 24)\n\n#任意な数値列の長さを取得\ndef random_length():\n    return random.randint(1, str_max_length)\n\n#ランダムに数字を取得\ndef random_number(need_not_zero =False):\n    return random.randint(1, 9) if need_not_zero else random.randint(0, 9)\n\ndef create_random_value():\n\n    return_value = ''                    #（カンマを含む）数字文字列\n    value_length = random_length()      #数値列の長さを任意に指定\n\n    for j in range(0,value_length):\n        return_value += str(random_number(True) if j==0 else random_number())\n    \n    #カンマを挿入\n    return return_value, '{:,}'.format(int(return_value))\n\n# 文字描画の初期位置（画像左上からx, yだけ離れた位置）\nx = 0\ny = 0\n\nfor i in range(0, test_data_count):\n    label,text = create_random_value()    #ランダムに数値列を生成\n\n    # RGB, 画像サイズ, 背景色を設定\n    im = Image.new(\"RGB\", (mnist_picture_width, mnist_picture_height), (256, 256, 256))\n\n    draw = ImageDraw.Draw(im)\n    # 描画位置、描画する文字、文字色、フォントを指定\n    draw.text((x, y), text, fill=(0, 0, 0), font=font)\n\n    # ファイルに出力\n    im.save(output_folder_path + \"./\" + label +\".png\")\n```\n\n出力例\n\n4114420306.png\n![4114420306.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/a98a2b64-84df-3622-4c27-d9a9421f76bb.png)\n\n99454.png\n![99454.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/ad17887e-71c2-1c6f-d34f-33a91ad79a3c.png)\n\n491.png\n![491.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/388176/c83b7d0e-568f-3d4b-3a10-f30d02fc65eb.png)\n\n尚、このコードで生成された画像を上記のモデルで検証させても、精度は全く出ません。\n上記と同じ方法で精度確認を行った所、ほとんど0%に近しい結果になりました。\n1文字のデータですら間違う有様です。\n\nいわゆる過学習状態なのでしょうか。詳細は不明です。\n現状は単にMNISTの画像を単純に張り合わせているだけですが、ランダム性と規則性を考慮し、回転処理や拡大・縮小処理をランダムに含ませたり、横方向の縮小を適用する事が必要かもしれません（近々検証しようと考えています）\n\nちなみに実はこのコード、「0」のデータを生成できない欠陥コードです。\nモデルの仕組みから言って特に問題はないと思いますが、気になる方は修正して使ってください。\n\n# 9. 振り返り\nこんな簡単にOCRが実装できるのは凄いの一言です。Pythonは本当に出来る事が多いですね。\n参考にしたkerasのサイトは全て英語だったため、ここが一番苦労しました。英語、大事…\nとはいえChatGPTが全部翻訳してくれるんじゃね？という説もありますけどね。\nいやー、すごい時代です。\n\n# 10. 参考文献\nhttps://keras.io/examples/vision/captcha_ocr/\nhttps://www.netforce.co.jp/techblog/entry/0001.shtml\nhttps://magazine.techacademy.jp/magazine/18981\nhttps://qiita.com/YoshikiIto/items/ace92dc9b05e375b8326\nhttps://self-development.info/%E3%80%90%E8%B6%85%E7%B0%A1%E5%8D%98%E3%80%91tensorflow%E3%81%AB%E3%81%8A%E3%81%91%E3%82%8B%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BF%9D%E5%AD%98%E3%83%BB%E5%BE%A9%E5%85%83%E3%83%BB%E5%86%8D%E5%88%A9/\n",
      "coediting": false,
      "comments_count": 0,
      "created_at": "2023-02-15T14:57:06+09:00",
      "group": null,
      "id": "83256a5a59e7784bb894",
      "likes_count": 3,
      "private": false,
      "reactions_count": 0,
      "stocks_count": 1,
      "tags": [
        {
          "name": "Python",
          "versions": []
        },
        {
          "name": "AI",
          "versions": []
        },
        {
          "name": "OCR",
          "versions": []
        },
        {
          "name": "MNIST",
          "versions": []
        }
      ],
      "title": "「シーケンス認識によるAI-OCRモデル」を活用しMNISTから生成した数字列を読み取る",
      "updated_at": "2023-02-28T09:57:59+09:00",
      "url": "https://qiita.com/cosumi77/items/83256a5a59e7784bb894",
      "user": {
        "description": null,
        "facebook_id": null,
        "followees_count": 2,
        "followers_count": 1,
        "github_login_name": null,
        "id": "cosumi77",
        "items_count": 2,
        "linkedin_id": null,
        "location": null,
        "name": "",
        "organization": null,
        "permanent_id": 388176,
        "profile_image_url": "https://pbs.twimg.com/profile_images/837261803172196352/4CIi-q-H_bigger.jpg",
        "team_only": false,
        "twitter_screen_name": "cosumi77",
        "website_url": null
      },
      "page_views_count": null,
      "team_membership": null,
      "organization_url_name": null,
      "slide": false,
      "semantic_similarity": 0.8348196744918823,
      "quality_score": 22,
      "python_code_score": 7,
      "python_code_blocks": 5
    },
    {
      "rendered_body": "<p data-sourcepos=\"1:1-1:64\">こんにちは！逆瀬川 ( <a href=\"https://x.com/gyakuse\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\">https://x.com/gyakuse</a> ) です！</p>\n<p data-sourcepos=\"3:1-3:520\">このアドベントカレンダーでは生成AIのアプリケーションを実際に作り、どのように作ればいいのか、ということをわかりやすく書いていければと思います。アプリケーションだけではなく、プロダクト開発に必要なモデルの調査方法、training方法、基礎知識等にも触れていければと思います。12月5日の朝にこれを書いていますが、4日目の記事です。果たして25日まで続くのでしょうか。</p>\n<h2 data-sourcepos=\"5:1-5:30\">\n<span id=\"今回の記事について\" class=\"fragment\"></span><a href=\"#%E4%BB%8A%E5%9B%9E%E3%81%AE%E8%A8%98%E4%BA%8B%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"><i class=\"fa fa-link\"></i></a>今回の記事について</h2>\n<p data-sourcepos=\"7:1-7:544\">本日はレシート等からテキスト情報抽出したときのつらみ、<strong>誤り訂正</strong>についてやっていこうと思います。レシート、請求書、その他諸々、こうしたものはテキスト抽出を用いないとデータベースに効率的に入れることはできず、また各種統計処理等も行えません。そういうときに問題となるのがOCR等での誤認識です。そうした誤り訂正については歴史が長く、幾多もの魔法少女たちが戦ってきました。</p>\n<p data-sourcepos=\"9:1-9:356\">特に最近のLMM (ChatGPT等の大規模マルチモーダルモデル) を用いた情報抽出では、OCR結果のほうだけをひと目見て『間違いだー！』となるようなものは減り、『それっぽい間違い』をするようになりました。校正において見比べる回数が増え、ストレスがすごいです。</p>\n<p data-sourcepos=\"11:1-11:274\"><a href=\"https://prtimes.jp/main/html/rd/p/000000224.000052725.html\" rel=\"nofollow noopener\" target=\"_blank\"><strong>WOZE</strong> という AI OCR を展開されている株式会社ハンモックさんの調査</a>はこの校正処理のつらみを非常によく表しています (めちゃくちゃいい調査だと思います)</p>\n<p data-sourcepos=\"13:1-13:256\">ということで、今回は一定程度自動で誤り訂正し、かつ<strong>確信度</strong>のようなスコアを一緒に表示することで校正者のストレスを軽減するためのアプリケーションを作っていければと思います。</p>\n<p data-sourcepos=\"15:1-15:130\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F27b243d5-d51f-5a57-0132-5503c867e1ad.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=7476351771b9df9ec58783ead33c0821\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F27b243d5-d51f-5a57-0132-5503c867e1ad.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=7476351771b9df9ec58783ead33c0821\" alt=\"kenka_yarouze.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F27b243d5-d51f-5a57-0132-5503c867e1ad.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=e01a7cb07974163f8cdc9a36a9b4b9d4 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/27b243d5-d51f-5a57-0132-5503c867e1ad.png\" loading=\"lazy\"></a></p>\n<h2 data-sourcepos=\"17:1-17:21\">\n<span id=\"想定する読者\" class=\"fragment\"></span><a href=\"#%E6%83%B3%E5%AE%9A%E3%81%99%E3%82%8B%E8%AA%AD%E8%80%85\"><i class=\"fa fa-link\"></i></a>想定する読者</h2>\n<ul data-sourcepos=\"19:1-20:0\">\n<li data-sourcepos=\"19:1-20:0\">泣いてるみんな</li>\n</ul>\n<h2 data-sourcepos=\"21:1-21:33\">\n<span id=\"得られる知識について\" class=\"fragment\"></span><a href=\"#%E5%BE%97%E3%82%89%E3%82%8C%E3%82%8B%E7%9F%A5%E8%AD%98%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"><i class=\"fa fa-link\"></i></a>得られる知識について</h2>\n<ul data-sourcepos=\"23:1-27:0\">\n<li data-sourcepos=\"23:1-23:29\">GoogleでのOCRの使い方</li>\n<li data-sourcepos=\"24:1-24:42\">LMM、LVLMを用いた情報抽出手法</li>\n<li data-sourcepos=\"25:1-25:85\">視覚的文書からの情報抽出手法 (Visual Document Information Extraction)</li>\n<li data-sourcepos=\"26:1-27:0\">LMMを用いたVDIE (VIE) 手法</li>\n</ul>\n<p data-sourcepos=\"28:1-28:42\">それではやっていきましょう。</p>\n<h2 data-sourcepos=\"30:1-30:76\">\n<span id=\"そもそも-現在のaiの日本語テキスト抽出能力について\" class=\"fragment\"></span><a href=\"#%E3%81%9D%E3%82%82%E3%81%9D%E3%82%82-%E7%8F%BE%E5%9C%A8%E3%81%AEai%E3%81%AE%E6%97%A5%E6%9C%AC%E8%AA%9E%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E6%8A%BD%E5%87%BA%E8%83%BD%E5%8A%9B%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"><i class=\"fa fa-link\"></i></a>そもそも: 現在のAIの日本語テキスト抽出能力について</h2>\n<p data-sourcepos=\"32:1-32:165\">ここではとりあえずそもそもどのくらいの精度でテキスト抽出できんの？みたいなのを明らかにしていければと思います。</p>\n<h3 data-sourcepos=\"34:1-34:31\">\n<span id=\"対象モデルについて\" class=\"fragment\"></span><a href=\"#%E5%AF%BE%E8%B1%A1%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"><i class=\"fa fa-link\"></i></a>対象モデルについて</h3>\n<p data-sourcepos=\"36:1-36:66\">以下を対象に、情報抽出のテストをしてみます。</p>\n<ul data-sourcepos=\"38:1-41:0\">\n<li data-sourcepos=\"38:1-38:20\">Google Document AI</li>\n<li data-sourcepos=\"39:1-39:8\">GPT-4o</li>\n<li data-sourcepos=\"40:1-41:0\">Gemini 1.5 Pro 002</li>\n</ul>\n<h3 data-sourcepos=\"42:1-42:16\">\n<span id=\"評価手法\" class=\"fragment\"></span><a href=\"#%E8%A9%95%E4%BE%A1%E6%89%8B%E6%B3%95\"><i class=\"fa fa-link\"></i></a>評価手法</h3>\n<p data-sourcepos=\"44:1-44:89\">サンプル5画像に対してそれぞれ文字を抽出し、F値を計算します。</p>\n<p data-sourcepos=\"46:1-46:492\">F値は以下のように計算します。F値は[0, 1]の範囲で示される実数値であり、1に近いほど認識精度が高いです。<a href=\"https://lab.ndl.go.jp/data_set/r4_koten/\" rel=\"nofollow noopener\" target=\"_blank\">NDLラボによる古典籍資料のOCRテキスト化実験（令和4年度～）</a> で使用されているものと同じような評価手法ですが、ここでは文字ではなく単語の多重集合としています(古典籍資料とは異なり容易に分かち書きができるため)。</p>\n<p data-sourcepos=\"48:1-50:2\">$$<br>\ny_{\\text{true}} = {\\text{正解データの単語の多重集合}}<br>\n$$</p>\n<p data-sourcepos=\"52:1-54:2\">$$<br>\ny_{\\text{pred}} = {\\text{予測データの単語の多重集合}}<br>\n$$</p>\n<p data-sourcepos=\"56:1-58:2\">$$<br>\n\\text{Precision} = \\frac{|y_{\\text{true}} \\cap y_{\\text{pred}}|}{|y_{\\text{pred}}|}<br>\n$$</p>\n<p data-sourcepos=\"60:1-62:2\">$$<br>\n\\text{Recall} = \\frac{|y_{\\text{true}} \\cap y_{\\text{pred}}|}{|y_{\\text{true}}|}<br>\n$$</p>\n<p data-sourcepos=\"64:1-66:2\">$$<br>\nF_{\\text{measure}} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}<br>\n$$</p>\n<h3 data-sourcepos=\"68:1-68:25\">\n<span id=\"利用するデータ\" class=\"fragment\"></span><a href=\"#%E5%88%A9%E7%94%A8%E3%81%99%E3%82%8B%E3%83%87%E3%83%BC%E3%82%BF\"><i class=\"fa fa-link\"></i></a>利用するデータ</h3>\n<p data-sourcepos=\"70:1-71:108\">手元にあった以下のデータを利用します。<br>\nもとデータにテキスト情報が含まれていない2-5は自分で書き起こしておきます。</p>\n<p data-sourcepos=\"73:1-73:124\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F2a9ae083-d0f9-da78-7926-fc3a97b52d46.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=903c2d31fa73c4f5bc61ca6ba540c469\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F2a9ae083-d0f9-da78-7926-fc3a97b52d46.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=903c2d31fa73c4f5bc61ca6ba540c469\" alt=\"for_ocr.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F2a9ae083-d0f9-da78-7926-fc3a97b52d46.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=d18fc33180afa4752dd27f8ae08cd116 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/2a9ae083-d0f9-da78-7926-fc3a97b52d46.png\" loading=\"lazy\"></a></p>\n<h3 data-sourcepos=\"75:1-75:10\">\n<span id=\"実装\" class=\"fragment\"></span><a href=\"#%E5%AE%9F%E8%A3%85\"><i class=\"fa fa-link\"></i></a>実装</h3>\n<p data-sourcepos=\"77:1-77:14\">Appendix参照</p>\n<h3 data-sourcepos=\"79:1-79:16\">\n<span id=\"検証結果\" class=\"fragment\"></span><a href=\"#%E6%A4%9C%E8%A8%BC%E7%B5%90%E6%9E%9C\"><i class=\"fa fa-link\"></i></a>検証結果</h3>\n<p data-sourcepos=\"81:1-81:103\">サンプルが5つなのであまり参考になりませんが、以下のようになりました。</p>\n<table data-sourcepos=\"83:1-99:44\">\n<thead>\n<tr data-sourcepos=\"83:1-83:69\">\n<th data-sourcepos=\"83:2-83:8\">Image</th>\n<th data-sourcepos=\"83:10-83:17\">Metric</th>\n<th data-sourcepos=\"83:19-83:38\">Google Document AI</th>\n<th data-sourcepos=\"83:40-83:47\">GPT-4o</th>\n<th data-sourcepos=\"83:49-83:68\">Gemini-1.5-pro-002</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"85:1-85:44\">\n<td data-sourcepos=\"85:2-85:4\">1</td>\n<td data-sourcepos=\"85:6-85:16\">Precision</td>\n<td data-sourcepos=\"85:18-85:25\">0.8960</td>\n<td data-sourcepos=\"85:27-85:34\">0.9680</td>\n<td data-sourcepos=\"85:36-85:43\">0.9675</td>\n</tr>\n<tr data-sourcepos=\"86:1-86:44\">\n<td data-sourcepos=\"86:2-86:4\"></td>\n<td data-sourcepos=\"86:6-86:16\">Recall</td>\n<td data-sourcepos=\"86:18-86:25\">0.8960</td>\n<td data-sourcepos=\"86:27-86:34\">0.9680</td>\n<td data-sourcepos=\"86:36-86:43\">0.9520</td>\n</tr>\n<tr data-sourcepos=\"87:1-87:44\">\n<td data-sourcepos=\"87:2-87:4\"></td>\n<td data-sourcepos=\"87:6-87:16\">F1</td>\n<td data-sourcepos=\"87:18-87:25\">0.8960</td>\n<td data-sourcepos=\"87:27-87:34\">0.9680</td>\n<td data-sourcepos=\"87:36-87:43\">0.9597</td>\n</tr>\n<tr data-sourcepos=\"88:1-88:44\">\n<td data-sourcepos=\"88:2-88:4\">2</td>\n<td data-sourcepos=\"88:6-88:16\">Precision</td>\n<td data-sourcepos=\"88:18-88:25\">1.0000</td>\n<td data-sourcepos=\"88:27-88:34\">0.8800</td>\n<td data-sourcepos=\"88:36-88:43\">0.9804</td>\n</tr>\n<tr data-sourcepos=\"89:1-89:44\">\n<td data-sourcepos=\"89:2-89:4\"></td>\n<td data-sourcepos=\"89:6-89:16\">Recall</td>\n<td data-sourcepos=\"89:18-89:25\">1.0000</td>\n<td data-sourcepos=\"89:27-89:34\">0.8800</td>\n<td data-sourcepos=\"89:36-89:43\">1.0000</td>\n</tr>\n<tr data-sourcepos=\"90:1-90:44\">\n<td data-sourcepos=\"90:2-90:4\"></td>\n<td data-sourcepos=\"90:6-90:16\">F1</td>\n<td data-sourcepos=\"90:18-90:25\">1.0000</td>\n<td data-sourcepos=\"90:27-90:34\">0.8800</td>\n<td data-sourcepos=\"90:36-90:43\">0.9901</td>\n</tr>\n<tr data-sourcepos=\"91:1-91:44\">\n<td data-sourcepos=\"91:2-91:4\">3</td>\n<td data-sourcepos=\"91:6-91:16\">Precision</td>\n<td data-sourcepos=\"91:18-91:25\">0.7105</td>\n<td data-sourcepos=\"91:27-91:34\">0.2596</td>\n<td data-sourcepos=\"91:36-91:43\">0.7586</td>\n</tr>\n<tr data-sourcepos=\"92:1-92:44\">\n<td data-sourcepos=\"92:2-92:4\"></td>\n<td data-sourcepos=\"92:6-92:16\">Recall</td>\n<td data-sourcepos=\"92:18-92:25\">0.7200</td>\n<td data-sourcepos=\"92:27-92:34\">0.3600</td>\n<td data-sourcepos=\"92:36-92:43\">0.8800</td>\n</tr>\n<tr data-sourcepos=\"93:1-93:44\">\n<td data-sourcepos=\"93:2-93:4\"></td>\n<td data-sourcepos=\"93:6-93:16\">F1</td>\n<td data-sourcepos=\"93:18-93:25\">0.7152</td>\n<td data-sourcepos=\"93:27-93:34\">0.3017</td>\n<td data-sourcepos=\"93:36-93:43\">0.8148</td>\n</tr>\n<tr data-sourcepos=\"94:1-94:44\">\n<td data-sourcepos=\"94:2-94:4\">4</td>\n<td data-sourcepos=\"94:6-94:16\">Precision</td>\n<td data-sourcepos=\"94:18-94:25\">0.8667</td>\n<td data-sourcepos=\"94:27-94:34\">0.7597</td>\n<td data-sourcepos=\"94:36-94:43\">0.8551</td>\n</tr>\n<tr data-sourcepos=\"95:1-95:44\">\n<td data-sourcepos=\"95:2-95:4\"></td>\n<td data-sourcepos=\"95:6-95:16\">Recall</td>\n<td data-sourcepos=\"95:18-95:25\">0.8083</td>\n<td data-sourcepos=\"95:27-95:34\">0.5078</td>\n<td data-sourcepos=\"95:36-95:43\">0.6114</td>\n</tr>\n<tr data-sourcepos=\"96:1-96:44\">\n<td data-sourcepos=\"96:2-96:4\"></td>\n<td data-sourcepos=\"96:6-96:16\">F1</td>\n<td data-sourcepos=\"96:18-96:25\">0.8365</td>\n<td data-sourcepos=\"96:27-96:34\">0.6087</td>\n<td data-sourcepos=\"96:36-96:43\">0.7130</td>\n</tr>\n<tr data-sourcepos=\"97:1-97:44\">\n<td data-sourcepos=\"97:2-97:4\">5</td>\n<td data-sourcepos=\"97:6-97:16\">Precision</td>\n<td data-sourcepos=\"97:18-97:25\">0.8812</td>\n<td data-sourcepos=\"97:27-97:34\">0.8901</td>\n<td data-sourcepos=\"97:36-97:43\">0.8586</td>\n</tr>\n<tr data-sourcepos=\"98:1-98:44\">\n<td data-sourcepos=\"98:2-98:4\"></td>\n<td data-sourcepos=\"98:6-98:16\">Recall</td>\n<td data-sourcepos=\"98:18-98:25\">0.8318</td>\n<td data-sourcepos=\"98:27-98:34\">0.7570</td>\n<td data-sourcepos=\"98:36-98:43\">0.7944</td>\n</tr>\n<tr data-sourcepos=\"99:1-99:44\">\n<td data-sourcepos=\"99:2-99:4\"></td>\n<td data-sourcepos=\"99:6-99:16\">F1</td>\n<td data-sourcepos=\"99:18-99:25\">0.8558</td>\n<td data-sourcepos=\"99:27-99:34\">0.8182</td>\n<td data-sourcepos=\"99:36-99:43\">0.8252</td>\n</tr>\n</tbody>\n</table>\n<table data-sourcepos=\"102:1-106:49\">\n<thead>\n<tr data-sourcepos=\"102:1-102:35\">\n<th data-sourcepos=\"102:2-102:8\">Model</th>\n<th data-sourcepos=\"102:10-102:20\">Precision</th>\n<th data-sourcepos=\"102:22-102:29\">Recall</th>\n<th data-sourcepos=\"102:31-102:34\">F1</th>\n</tr>\n</thead>\n<tbody>\n<tr data-sourcepos=\"104:1-104:49\">\n<td data-sourcepos=\"104:2-104:21\">Google Document AI</td>\n<td data-sourcepos=\"104:23-104:30\">0.8709</td>\n<td data-sourcepos=\"104:32-104:39\">0.8512</td>\n<td data-sourcepos=\"104:41-104:48\">0.8607</td>\n</tr>\n<tr data-sourcepos=\"105:1-105:37\">\n<td data-sourcepos=\"105:2-105:9\">GPT-4o</td>\n<td data-sourcepos=\"105:11-105:18\">0.7515</td>\n<td data-sourcepos=\"105:20-105:27\">0.6946</td>\n<td data-sourcepos=\"105:29-105:36\">0.7153</td>\n</tr>\n<tr data-sourcepos=\"106:1-106:49\">\n<td data-sourcepos=\"106:2-106:21\">Gemini-1.5-pro-002</td>\n<td data-sourcepos=\"106:23-106:30\">0.8840</td>\n<td data-sourcepos=\"106:32-106:39\">0.8476</td>\n<td data-sourcepos=\"106:41-106:48\">0.8606</td>\n</tr>\n</tbody>\n</table>\n<p data-sourcepos=\"108:1-108:49\">サンプル数が少なすぎる雑プロット:</p>\n<p data-sourcepos=\"110:1-110:122\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F13e51fa5-c742-3ad7-4fc9-6263566c57d0.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=0c0c3037c69466a2adebc0015cc5883f\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F13e51fa5-c742-3ad7-4fc9-6263566c57d0.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=0c0c3037c69466a2adebc0015cc5883f\" alt=\"zatsu.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F13e51fa5-c742-3ad7-4fc9-6263566c57d0.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=a36a3bb49a0842a6b1ea84b257f31af7 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/13e51fa5-c742-3ad7-4fc9-6263566c57d0.png\" loading=\"lazy\"></a></p>\n<ul data-sourcepos=\"112:1-118:0\">\n<li data-sourcepos=\"112:1-112:60\">F1値だとGemini 1.5 Pro 002とDocument AIが同率一位</li>\n<li data-sourcepos=\"113:1-113:68\">GPT-4oは特にRecallの低さが顕著で取りこぼしが多い</li>\n<li data-sourcepos=\"114:1-114:165\">タスク別に見ていくと文字埋め込みのないPDFやスキャンされたデータのようなきれいなテキスト抽出はLMMのほうが優秀 (1)</li>\n<li data-sourcepos=\"115:1-115:54\">横書きの活字抽出はどれも同程度 (2, 5)</li>\n<li data-sourcepos=\"116:1-116:63\">縦書きの活字抽出はGPT-4oが異様に低くなる (3)</li>\n<li data-sourcepos=\"117:1-118:0\">横書きの手書き文字抽出はDocument AIが優秀, LMMは取りこぼしが多くなる (4)</li>\n</ul>\n<h2 data-sourcepos=\"119:1-119:66\">\n<span id=\"視覚的文書からの情報抽出処理の流れについて\" class=\"fragment\"></span><a href=\"#%E8%A6%96%E8%A6%9A%E7%9A%84%E6%96%87%E6%9B%B8%E3%81%8B%E3%82%89%E3%81%AE%E6%83%85%E5%A0%B1%E6%8A%BD%E5%87%BA%E5%87%A6%E7%90%86%E3%81%AE%E6%B5%81%E3%82%8C%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"><i class=\"fa fa-link\"></i></a>視覚的文書からの情報抽出処理の流れについて</h2>\n<p data-sourcepos=\"121:1-121:120\">次に、認識ミスを考える前にどうやって画像からの情報抽出を行うかを明らかにします。</p>\n<p data-sourcepos=\"123:1-123:182\">この情報抽出においては、単純に画像からテキスト抽出しているだけではなく、情報抽出タスク(構造化データの抽出)も絡んできます。</p>\n<h3 data-sourcepos=\"125:1-125:28\">\n<span id=\"わかりやすい事例\" class=\"fragment\"></span><a href=\"#%E3%82%8F%E3%81%8B%E3%82%8A%E3%82%84%E3%81%99%E3%81%84%E4%BA%8B%E4%BE%8B\"><i class=\"fa fa-link\"></i></a>わかりやすい事例</h3>\n<p data-sourcepos=\"127:1-127:60\"><iframe id=\"qiita-embed-content__9585ff3a3b94d4addd26040fd8ce8084\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__9585ff3a3b94d4addd26040fd8ce8084\" data-content=\"https%3A%2F%2Fgihyo.jp%2Farticle%2F2023%2F07%2Fprogramming-with-chatgpt-04\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<p data-sourcepos=\"129:1-129:126\">以前、こちらの記事では、レシート等の画像から自動的に情報抽出する手法を紹介しました。</p>\n<p data-sourcepos=\"131:1-131:36\">手法を簡単に説明すると、</p>\n<ul data-sourcepos=\"133:1-135:0\">\n<li data-sourcepos=\"133:1-133:47\">OCRでテキスト情報を画像から抽出</li>\n<li data-sourcepos=\"134:1-135:0\">ChatGPTで抽出したいデータ構造を定義し、OCRで得られた情報をもとに抽出</li>\n</ul>\n<p data-sourcepos=\"136:1-136:252\">という流れです。単純にOCRしただけだと座標とテキストしか得られませんが、情報抽出処理によって構造化することでデータベース等に入れたりするなどの活用ができるようになります。</p>\n<p data-sourcepos=\"138:1-138:139\">現在はChatGPTやGeminiが画像等のマルチモーダルに対応したため、別の手法も検討できるようになりました</p>\n<h3 data-sourcepos=\"140:1-140:34\">\n<span id=\"事前処理-傾き補正等\" class=\"fragment\"></span><a href=\"#%E4%BA%8B%E5%89%8D%E5%87%A6%E7%90%86-%E5%82%BE%E3%81%8D%E8%A3%9C%E6%AD%A3%E7%AD%89\"><i class=\"fa fa-link\"></i></a>事前処理 (傾き補正等)</h3>\n<p data-sourcepos=\"142:1-142:226\">OCRのプリプロセスとしては、傾き、歪み、ノイズ等を補正し、文字をシャープ化したり画像を二値化したり (大津やNickの二値化手法が有名です)、いろいろ頑張ります。</p>\n<p data-sourcepos=\"144:1-144:63\">これはLMMでも同様で、鮮明なほうがモテます。</p>\n<p data-sourcepos=\"146:1-146:323\">ぼやけた文字とかもよくある (許せん) ので、いい感じのシーンテキスト超解像 (STISR) も大事です。Toyotaさんも<a href=\"https://github.com/ToyotaInfoTech/stisr-tcdm\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/ToyotaInfoTech/stisr-tcdm</a> というのを公開していたりします。<a href=\"https://github.com/yfaqh/Awesome-Scene-Text-Image-Super-Resolution\" class=\"autolink\" rel=\"nofollow noopener\" target=\"_blank\">https://github.com/yfaqh/Awesome-Scene-Text-Image-Super-Resolution</a> とか便利。</p>\n<p data-sourcepos=\"148:1-148:51\">Sansanさんのこの資料等も楽しいです。</p>\n<p data-sourcepos=\"150:1-150:78\"><iframe id=\"qiita-embed-content__98dcd72b4106051c33dec62b57abeff2\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__98dcd72b4106051c33dec62b57abeff2\" data-content=\"https%3A%2F%2Fspeakerdeck.com%2Fsansandsoc%2Frecent-topics-on-character-super-resolution\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<h3 data-sourcepos=\"152:1-152:28\">\n<span id=\"モデルによる処理\" class=\"fragment\"></span><a href=\"#%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AB%E3%82%88%E3%82%8B%E5%87%A6%E7%90%86\"><i class=\"fa fa-link\"></i></a>モデルによる処理</h3>\n<p data-sourcepos=\"154:1-154:54\">次にモデルのほうで、抽出処理をします</p>\n<p data-sourcepos=\"156:1-156:74\"><strong>OCR Onlyの場合 (わたしのgihyo記事の例がこれにあたる)</strong></p>\n<ul data-sourcepos=\"158:1-160:0\">\n<li data-sourcepos=\"158:1-158:26\">テキスト抽出処理</li>\n<li data-sourcepos=\"159:1-160:0\">情報抽出処理 (例: LLMを利用)</li>\n</ul>\n<p data-sourcepos=\"161:1-161:16\"><strong>LMMの場合</strong></p>\n<ul data-sourcepos=\"163:1-164:0\">\n<li data-sourcepos=\"163:1-164:0\">画像からそのまま情報抽出処理をする</li>\n</ul>\n<p data-sourcepos=\"165:1-165:28\"><strong>組み合わせる場合</strong></p>\n<ul data-sourcepos=\"167:1-169:0\">\n<li data-sourcepos=\"167:1-167:29\">OCRでのテキスト抽出</li>\n<li data-sourcepos=\"168:1-169:0\">LMMでの画像・OCR結果を両方考慮した情報抽出</li>\n</ul>\n<p data-sourcepos=\"170:1-170:76\">こうしてJSONだったりする構造化データが手に入ります。</p>\n<h2 data-sourcepos=\"172:1-172:57\">\n<span id=\"テキスト情報抽出での認識ミスについて\" class=\"fragment\"></span><a href=\"#%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E6%83%85%E5%A0%B1%E6%8A%BD%E5%87%BA%E3%81%A7%E3%81%AE%E8%AA%8D%E8%AD%98%E3%83%9F%E3%82%B9%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"><i class=\"fa fa-link\"></i></a>テキスト情報抽出での認識ミスについて</h2>\n<p data-sourcepos=\"174:1-174:348\">それでは、悲しいやつを考えていきましょう。OCRやLMMはさきほど見たように、完全な精度で文字を抽出することができません。ということは、構造化データを出すテキスト情報抽出でも、その精度が引き継がれるというわけです。より悪くなったりもします。</p>\n<p data-sourcepos=\"176:1-176:123\">ではどういうエラーがあるか考えてみましょう。大きく分けて以下の種類が検討できます。</p>\n<ul data-sourcepos=\"178:1-180:0\">\n<li data-sourcepos=\"178:1-178:35\">テキスト自体の認識ミス</li>\n<li data-sourcepos=\"179:1-180:0\">レイアウト構造の理解の失敗 (情報抽出の問題)</li>\n</ul>\n<p data-sourcepos=\"181:1-181:295\">後者に関しては、たとえば請求書で合計の値を取りたかったのに、小計の数字が取れてしまうなどがあります。今回はレイアウト構造の理解の失敗についてはあまり考え (たく) ないので前者について深堀りしましょう。</p>\n<p data-sourcepos=\"183:1-183:69\">ざっくり以下のようなパターンがあると思います。</p>\n<ul data-sourcepos=\"185:1-203:0\">\n<li data-sourcepos=\"185:1-194:17\">光学的類似性によるもの\n<ul data-sourcepos=\"186:5-194:17\">\n<li data-sourcepos=\"186:5-186:111\">光学的には似ている文字列を誤って検出してしまうパターン、OCRでよく起きる</li>\n<li data-sourcepos=\"187:5-187:93\">特徴として、文脈的に不自然な単語やフレーズである場合が多い</li>\n<li data-sourcepos=\"188:5-194:17\">例\n<ul data-sourcepos=\"189:9-194:17\">\n<li data-sourcepos=\"189:9-189:99\">八丁目 → ハ丁目 (漢数字の八ではなくカタカナのハになっている)</li>\n<li data-sourcepos=\"190:9-190:93\">一丁目 → ー丁目 (漢数字の一ではなく長音記号になっている)</li>\n<li data-sourcepos=\"191:9-191:21\">日 → 目</li>\n<li data-sourcepos=\"192:9-192:21\">見 → 貝</li>\n<li data-sourcepos=\"193:9-193:21\">1 → l (L)</li>\n<li data-sourcepos=\"194:9-194:17\">O → 0</li>\n</ul>\n</li>\n</ul>\n</li>\n<li data-sourcepos=\"195:1-198:57\">文脈的妥当性によるもの\n<ul data-sourcepos=\"196:5-198:57\">\n<li data-sourcepos=\"196:5-196:112\">文脈に沿っているように見えるため、訂正が困難なパターン、LVLMでよく起きる</li>\n<li data-sourcepos=\"197:5-197:38\">「1,000円」→「10,000円」</li>\n<li data-sourcepos=\"198:5-198:57\">「株式会社OpenAI」→「有限会社OpenAI」</li>\n</ul>\n</li>\n<li data-sourcepos=\"199:1-203:0\">光学的にも文脈的にもおかしい異常ケース\n<ul data-sourcepos=\"200:5-203:0\">\n<li data-sourcepos=\"200:5-200:100\">明らかに不自然な文字列が入るパターン、OCRでもLVLMでもたまに起きる</li>\n<li data-sourcepos=\"201:5-201:83\">「本日ハ晴天ナリ」→「本日ハ食事ナリ」 (晴天 → 食事)</li>\n<li data-sourcepos=\"202:5-203:0\">「本日はとても」→「本日はw%@とても」 (<code>w%@</code> が入ってしまった)</li>\n</ul>\n</li>\n</ul>\n<h2 data-sourcepos=\"204:1-204:33\">\n<span id=\"校正支援方法について\" class=\"fragment\"></span><a href=\"#%E6%A0%A1%E6%AD%A3%E6%94%AF%E6%8F%B4%E6%96%B9%E6%B3%95%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6\"><i class=\"fa fa-link\"></i></a>校正支援方法について</h2>\n<p data-sourcepos=\"206:1-206:78\">こうした認識ミスが発生してしまうのは避けられません。</p>\n<p data-sourcepos=\"208:1-208:195\">ほとんどミスをしないモデルを構築できても一定割合はミスをするので、そのミスを許容できるシステムでない限り校正処理が必要になります。</p>\n<p data-sourcepos=\"210:1-210:117\">校正処理を支援するシステムには以下のようなものが表示されると嬉しさがあります。</p>\n<ul data-sourcepos=\"212:1-215:0\">\n<li data-sourcepos=\"212:1-212:26\">確信度を表示する</li>\n<li data-sourcepos=\"213:1-213:38\">根拠となる領域を明示する</li>\n<li data-sourcepos=\"214:1-215:0\">修正候補を提案する</li>\n</ul>\n<p data-sourcepos=\"216:1-216:177\">もちろん、自動で誤り訂正ができれば嬉しいです。基本的にポストプロセスに誤り訂正を入れたあと、人手による校正が入ります。</p>\n<h3 data-sourcepos=\"218:1-218:16\">\n<span id=\"誤り訂正\" class=\"fragment\"></span><a href=\"#%E8%AA%A4%E3%82%8A%E8%A8%82%E6%AD%A3\"><i class=\"fa fa-link\"></i></a>誤り訂正</h3>\n<p data-sourcepos=\"220:1-220:640\">基本的には文脈に適しているかを判断するアプローチと文字ごとに視覚的な形状のマッチングを行うアプローチがあります。seq2seqベースのアプローチもありましたが、LMMで生ずるハルシネーションには効果があまり期待できません。<a href=\"https://arxiv.org/abs/2308.15262\" rel=\"nofollow noopener\" target=\"_blank\">arxiv:2308.15262v1</a> では、CharBERTとGlyph Embeddingを活用したモデルが、単語や文レベルでの誤り訂正において高い効果を示しています。字体に注目した視覚的特徴と文脈の両方を組み合わせるアプローチがわりとよさげです。</p>\n<h2 data-sourcepos=\"222:1-222:87\">\n<span id=\"複数のモデルからの出力を用いて校正の負担を軽減させる方法\" class=\"fragment\"></span><a href=\"#%E8%A4%87%E6%95%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%8B%E3%82%89%E3%81%AE%E5%87%BA%E5%8A%9B%E3%82%92%E7%94%A8%E3%81%84%E3%81%A6%E6%A0%A1%E6%AD%A3%E3%81%AE%E8%B2%A0%E6%8B%85%E3%82%92%E8%BB%BD%E6%B8%9B%E3%81%95%E3%81%9B%E3%82%8B%E6%96%B9%E6%B3%95\"><i class=\"fa fa-link\"></i></a>複数のモデルからの出力を用いて校正の負担を軽減させる方法</h2>\n<p data-sourcepos=\"224:1-225:210\">今回検討するのがこちらです。アンサンブル的な手法を用いてスコアを出します。<br>\n仮にGPT-4o、Gemini 1.5 Pro 002、Google Document AIの抽出結果を要素ごとにうまくマッチングできれば、あとはレーベンシュタイン距離を見ればうまくできそうです。</p>\n<p data-sourcepos=\"227:1-228:167\">ただし、マッチングどうする問題が難しい〜〜。<br>\n<a href=\"https://huggingface.co/microsoft/Florence-2-large\" rel=\"nofollow noopener\" target=\"_blank\">Florence2</a>のように座標を吐いてくれるわけではないので、うまくやる必要があります。</p>\n<h3 data-sourcepos=\"230:1-230:31\">\n<span id=\"座標出せませんか\" class=\"fragment\"></span><a href=\"#%E5%BA%A7%E6%A8%99%E5%87%BA%E3%81%9B%E3%81%BE%E3%81%9B%E3%82%93%E3%81%8B\"><i class=\"fa fa-link\"></i></a>座標出せませんか？</h3>\n<p data-sourcepos=\"232:1-232:132\">ということで、LMMに座標を出してほしいので以下のような構造を定義して吐き出させてみました。</p>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"234:1-250:3\"><div class=\"highlight\"><pre><code><span class=\"k\">class</span> <span class=\"nc\">ExtractedText</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"nc\">Field</span><span class=\"p\">(...,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">The content of the extracted text.</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n    <span class=\"n\">coordinates</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nc\">Field</span><span class=\"p\">(</span>\n        <span class=\"p\">...,</span>\n        <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"p\">(</span>\n            <span class=\"sh\">\"</span><span class=\"s\">Bounding box coordinates represented as a list of two points [x1, y1, x2, y2].</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n            <span class=\"sh\">\"</span><span class=\"s\">- x1, y1: The top-left corner of the bounding box.</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span>\n            <span class=\"sh\">\"</span><span class=\"s\">- x2, y2: The bottom-right corner of the bounding box.</span><span class=\"sh\">\"</span>\n        <span class=\"p\">)</span>\n    <span class=\"p\">)</span>\n\n\n\n<span class=\"k\">class</span> <span class=\"nc\">OCRResultWithCoordinates</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">extracted_texts</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">ExtractedText</span><span class=\"p\">]</span>\n</code></pre></div></div>\n<p data-sourcepos=\"252:1-252:285\">結果がこちら。解像度を与えると若干改善するものの、やっぱり難しいです。てか、ここまである程度うまくいくのやばいな。要素数が少なければあとはうまくやれそうですが、大量にあるときには難しいです。</p>\n<p data-sourcepos=\"254:1-254:125\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F2c5ea18a-cadf-9e3f-1d9c-4ea5979c39fc.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=ac4873d6064fbf5115bfd1c38b7254fb\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F2c5ea18a-cadf-9e3f-1d9c-4ea5979c39fc.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=ac4873d6064fbf5115bfd1c38b7254fb\" alt=\"kanashii.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F2c5ea18a-cadf-9e3f-1d9c-4ea5979c39fc.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=4614f0b9ac30bcc1306cb956105001f2 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/2c5ea18a-cadf-9e3f-1d9c-4ea5979c39fc.png\" loading=\"lazy\"></a></p>\n<h3 data-sourcepos=\"256:1-256:43\">\n<span id=\"食べログさんのやつはどうか\" class=\"fragment\"></span><a href=\"#%E9%A3%9F%E3%81%B9%E3%83%AD%E3%82%B0%E3%81%95%E3%82%93%E3%81%AE%E3%82%84%E3%81%A4%E3%81%AF%E3%81%A9%E3%81%86%E3%81%8B\"><i class=\"fa fa-link\"></i></a>食べログさんのやつはどうか</h3>\n<p data-sourcepos=\"258:1-258:47\"><iframe id=\"qiita-embed-content__39b73dd3ec07a6a0f55b4ad1bfd8a192\" src=\"https://qiita.com/embed-contents/link-card#qiita-embed-content__39b73dd3ec07a6a0f55b4ad1bfd8a192\" data-content=\"https%3A%2F%2Ftech-blog.tabelog.com%2Fentry%2Fai-menu-ocr\" frameborder=\"0\" scrolling=\"no\" loading=\"lazy\" style=\"width:100%;\" height=\"29\">\n</iframe>\n</p>\n<p data-sourcepos=\"260:1-262:87\">こっちはめちゃくちゃ情報抽出にはいいやり方です。<br>\n画像 + OCRで抽出された情報 (要素ごとにIDを振り、抽出された文字列と座標をセットにする) 渡して情報抽出タスクを解いています。<br>\nちなみにLLMを用いた情報抽出処理はいろんなやり方があります。</p>\n<h3 data-sourcepos=\"264:1-264:39\">\n<span id=\"いい感じの情報抽出手法\" class=\"fragment\"></span><a href=\"#%E3%81%84%E3%81%84%E6%84%9F%E3%81%98%E3%81%AE%E6%83%85%E5%A0%B1%E6%8A%BD%E5%87%BA%E6%89%8B%E6%B3%95\"><i class=\"fa fa-link\"></i></a>(いい感じの情報抽出手法)</h3>\n<ul data-sourcepos=\"266:1-279:0\">\n<li data-sourcepos=\"266:1-267:51\">OCRテキストのみを渡して情報抽出 (画像は渡さない)\n<ul data-sourcepos=\"267:5-267:51\">\n<li data-sourcepos=\"267:5-267:51\">テキストの精度はOCRの能力に依存</li>\n</ul>\n</li>\n<li data-sourcepos=\"268:1-269:33\">画像のみを渡して情報抽出\n<ul data-sourcepos=\"269:5-269:33\">\n<li data-sourcepos=\"269:5-269:33\">LMMの能力のみに依存</li>\n</ul>\n</li>\n<li data-sourcepos=\"270:1-272:54\">画像 + OCRテキスト\n<ul data-sourcepos=\"271:5-272:54\">\n<li data-sourcepos=\"271:5-271:96\">OCRの抽出したテキストの情報を使える (精度が上がる可能性もある)</li>\n<li data-sourcepos=\"272:5-272:54\">OCRの認識能力に引っ張られる可能性</li>\n</ul>\n</li>\n<li data-sourcepos=\"273:1-276:84\">画像 + OCRの抽出要素に採番 + 頂点の座標\n<ul data-sourcepos=\"274:5-276:84\">\n<li data-sourcepos=\"274:5-274:30\">ブログの提案手法</li>\n<li data-sourcepos=\"275:5-275:72\">LMMに正しく座標の意味が伝わらない可能性がある</li>\n<li data-sourcepos=\"276:5-276:84\">どこの領域のデータか対応関係を表示できる嬉しさがある</li>\n</ul>\n</li>\n<li data-sourcepos=\"277:1-279:0\">画像 (OCRの要素に矩形 + 採番を描画) + OCRの抽出要素に採番\n<ul data-sourcepos=\"278:5-279:0\">\n<li data-sourcepos=\"278:5-279:0\">画像を上書きするので、正しくLMMが番号を読み取れなかったり、番号を描画する位置の決定が難しい</li>\n</ul>\n</li>\n</ul>\n<h3 data-sourcepos=\"280:1-280:31\">\n<span id=\"ゴリ押しマッチング\" class=\"fragment\"></span><a href=\"#%E3%82%B4%E3%83%AA%E6%8A%BC%E3%81%97%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0\"><i class=\"fa fa-link\"></i></a>ゴリ押しマッチング</h3>\n<p data-sourcepos=\"282:1-282:456\">ここまで書いて寝て朝になりました。とりあえず <code>画像 (OCRの要素に矩形 + 採番を描画)</code> でやります。今回はLMMの独立したテキスト抽出能力が必要であるため、OCRから抽出されたテキストのデータは渡しません。なお、この手法は要素が混雑している画像には使えません (単純に採番が文字に被るので. 薄く描画したらワンチャンいけるかも)</p>\n<h3 data-sourcepos=\"284:1-284:19\">\n<span id=\"処理の流れ\" class=\"fragment\"></span><a href=\"#%E5%87%A6%E7%90%86%E3%81%AE%E6%B5%81%E3%82%8C\"><i class=\"fa fa-link\"></i></a>処理の流れ</h3>\n<ul data-sourcepos=\"286:1-291:0\">\n<li data-sourcepos=\"286:1-286:20\">OCRで要素抽出</li>\n<li data-sourcepos=\"287:1-287:97\">座標データをもとに画像にバウンディングボックスとID番号を書き込み</li>\n<li data-sourcepos=\"288:1-288:59\">それぞれ抽出して要素ごとの候補群が出る</li>\n<li data-sourcepos=\"289:1-289:20\">スコアを算出</li>\n<li data-sourcepos=\"290:1-291:0\">cropされた画像、スコア、候補が表示される</li>\n</ul>\n<p data-sourcepos=\"292:1-292:131\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F43704e60-7646-9bda-1818-3b5d85500d1f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=6d0c315bd4fd2469ac21c20d29f9bca6\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F43704e60-7646-9bda-1818-3b5d85500d1f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=6d0c315bd4fd2469ac21c20d29f9bca6\" alt=\"temp_annotated.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F43704e60-7646-9bda-1818-3b5d85500d1f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=7f5b51c9d43489bd58419c9dce18ae1f 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/43704e60-7646-9bda-1818-3b5d85500d1f.png\" loading=\"lazy\"></a></p>\n<h3 data-sourcepos=\"295:1-295:10\">\n<span id=\"実装-1\" class=\"fragment\"></span><a href=\"#%E5%AE%9F%E8%A3%85-1\"><i class=\"fa fa-link\"></i></a>実装</h3>\n<p data-sourcepos=\"297:1-297:17\">Appendixを参照</p>\n<h3 data-sourcepos=\"299:1-299:13\">\n<span id=\"結果\" class=\"fragment\"></span><a href=\"#%E7%B5%90%E6%9E%9C\"><i class=\"fa fa-link\"></i></a>結果！</h3>\n<p data-sourcepos=\"301:1-301:126\">いい感じにマッチングした！ (ちょっとまだ精度あれだけど、プロンプト次第で良くなりそう)</p>\n<ul data-sourcepos=\"303:1-304:0\">\n<li data-sourcepos=\"303:1-304:0\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2Fd50b6413-7253-0eca-304c-83ec9951ccc9.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=52315c61fd732a2148dba68462f42da7\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2Fd50b6413-7253-0eca-304c-83ec9951ccc9.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=52315c61fd732a2148dba68462f42da7\" alt=\"kakkohagomi.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2Fd50b6413-7253-0eca-304c-83ec9951ccc9.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=36444e8848887ffb98eefdc99ba2cc41 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/d50b6413-7253-0eca-304c-83ec9951ccc9.png\" loading=\"lazy\"></a></li>\n</ul>\n<p data-sourcepos=\"305:1-305:93\">こんな感じで、意見が割れているところが濃くハイライトされます。</p>\n<p data-sourcepos=\"307:1-307:130\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F27b243d5-d51f-5a57-0132-5503c867e1ad.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=7476351771b9df9ec58783ead33c0821\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F27b243d5-d51f-5a57-0132-5503c867e1ad.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=7476351771b9df9ec58783ead33c0821\" alt=\"kenka_yarouze.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F27b243d5-d51f-5a57-0132-5503c867e1ad.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=e01a7cb07974163f8cdc9a36a9b4b9d4 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/27b243d5-d51f-5a57-0132-5503c867e1ad.png\" loading=\"lazy\"></a></p>\n<h2 data-sourcepos=\"309:1-309:12\">\n<span id=\"まとめ\" class=\"fragment\"></span><a href=\"#%E3%81%BE%E3%81%A8%E3%82%81\"><i class=\"fa fa-link\"></i></a>まとめ</h2>\n<ul data-sourcepos=\"311:1-313:0\">\n<li data-sourcepos=\"311:1-311:101\">校正に時間が結構かかるのであればかなりありなアプローチかもしれない</li>\n<li data-sourcepos=\"312:1-313:0\">OCRがんばっている人はえらい！</li>\n</ul>\n<h2 data-sourcepos=\"314:1-314:11\">\n<span id=\"appendix\" class=\"fragment\"></span><a href=\"#appendix\"><i class=\"fa fa-link\"></i></a>Appendix</h2>\n<h3 data-sourcepos=\"316:1-316:22\">\n<span id=\"評価用データ\" class=\"fragment\"></span><a href=\"#%E8%A9%95%E4%BE%A1%E7%94%A8%E3%83%87%E3%83%BC%E3%82%BF\"><i class=\"fa fa-link\"></i></a>評価用データ</h3>\n<p data-sourcepos=\"318:1-318:72\">今回作った以下の評価データは自由にお使い下さい。</p>\n<p data-sourcepos=\"320:1-320:123\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F0a57fc75-c3c7-77ff-05f1-2358223d1e70.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=169d8cd0e8cafe7c507a801a213e1ae4\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F0a57fc75-c3c7-77ff-05f1-2358223d1e70.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=169d8cd0e8cafe7c507a801a213e1ae4\" alt=\"image1.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F0a57fc75-c3c7-77ff-05f1-2358223d1e70.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=6fa91bfffeb3826d7b489021fcbda4d5 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/0a57fc75-c3c7-77ff-05f1-2358223d1e70.png\" loading=\"lazy\"></a></p>\n<blockquote data-sourcepos=\"321:1-321:546\">\n<p data-sourcepos=\"321:3-321:546\">請求書 株式会社 Dark AI 御中 請求No. 2024110103 請求日 2024/11/01 件名： 社会奉仕活動 渋谷太郎 下記の通り、ご請求申し上げます。 〒150-0002 東京都渋谷区渋谷1-1-1 渋谷ヒルズ1001 TEL： 090-1919-1919 合計金額 ¥16,500 （税込） お支払期限： 2024/11/30 No. 摘要 数量 単価 金額 社会奉仕活動 50 時間 300 ¥15,000 小計 ¥15,000 8%消費税 ¥0 10%消費税 ¥1,500 合計 ¥16,500 お振込先 最高銀行 渋谷支店 普通 1129917 シブヤ　タロウ 備考</p>\n</blockquote>\n<p data-sourcepos=\"323:1-323:123\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F97111993-5a41-9d21-de8b-0863ed4de827.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=eddb3dffdeb7cbc4870edd4ef5f9ace4\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F97111993-5a41-9d21-de8b-0863ed4de827.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=eddb3dffdeb7cbc4870edd4ef5f9ace4\" alt=\"image2.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F97111993-5a41-9d21-de8b-0863ed4de827.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=cea50a7577e85bc6bc2538241c95b749 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/97111993-5a41-9d21-de8b-0863ed4de827.png\" loading=\"lazy\"></a></p>\n<blockquote data-sourcepos=\"324:1-324:216\">\n<p data-sourcepos=\"324:3-324:216\">栄養成分表示(100ml当たり)/エネルギー0kcalたんぱく質・脂質・炭水化物0g、ナトリウム1.1mg(食塩相当量0.003g)、カルシウム0.72mg、カリウム0.09mg、マグネシウム0.23mg</p>\n</blockquote>\n<p data-sourcepos=\"326:1-326:123\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F22e40c63-b469-8098-a7dd-f68b888b973b.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=059ce7257563edf6ec74a722660af428\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F22e40c63-b469-8098-a7dd-f68b888b973b.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=059ce7257563edf6ec74a722660af428\" alt=\"image3.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F22e40c63-b469-8098-a7dd-f68b888b973b.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=6fd2a5a7788f8d24893f68f8b50efd8b 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/22e40c63-b469-8098-a7dd-f68b888b973b.png\" loading=\"lazy\"></a></p>\n<blockquote data-sourcepos=\"327:1-327:487\">\n<p data-sourcepos=\"327:3-327:487\">Jupyterでトークナイズ データセット作りたい→何分かかる？ PDFデータなら「分かっている. 対象 PDF(テキスト入り) スキャンデータ 写真 傾き 手書き フォントへの頑健性 文字の明瞭化 誤り訂正アプローチ seq-to-seqベース レイアウト←どう定義するか 文字 レシート OCR template Matching LLMによる訂正→文脈として、大量に渡す 対象の画像に入ってそうな文字列を渡す</p>\n</blockquote>\n<p data-sourcepos=\"329:1-329:123\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F5e252c2f-ea9b-b759-d600-cb371266c6ae.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=f7cdc1ecd28db93de8286619bfa6e3d9\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F5e252c2f-ea9b-b759-d600-cb371266c6ae.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=f7cdc1ecd28db93de8286619bfa6e3d9\" alt=\"image4.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2F5e252c2f-ea9b-b759-d600-cb371266c6ae.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=5e8029ce441bad1c4c9ac62952fee910 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/5e252c2f-ea9b-b759-d600-cb371266c6ae.png\" loading=\"lazy\"></a></p>\n<blockquote data-sourcepos=\"330:1-330:1044\">\n<p data-sourcepos=\"330:3-330:1044\">音羽山清水寺 第九十六 大吉 にわとりほうをおうておなじくとぶ 鶏逐鳳同飛 レ こうりんうぎをとゝのう 高林整羽儀 二 一 ふねにさおさしてすべからくきしにわたるべし 棹船須湾岸 レ レ レ ほうかふねにみちてかえらん 宝貸満船帰 レ ◯このみくじにあう人は、人を見くだし、軽んずることなく、万事つゝましやかにして、たとえ地位低き人たりとも大切にして、仕事に勉強すれば、家内の仕合せきわめてよろし◯望み事叶う◯病人次第に本ぷくすべし◯あらそいごと勝ちなり◯失せ物出ること遅し◯転居、ふしん、旅行ゆる〱してよし◯えんだん吉◯売買吉◯職業は金、土、木、紙に縁あるものよし◯あきないもよし◯子に賢く出世するものあり、大切に育つべし くゝりつけないでお持ち帰り下さい。\", # くの字点 (ゆる〱の『〱』)、平仮名繰返し記号 (ゝ) が含まれる</p>\n</blockquote>\n<p data-sourcepos=\"332:1-332:123\"><a href=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2Fa01502ce-adba-3385-8e8a-88c8ada2e44f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=85d638efcaafef130b5415830f77a16a\" target=\"_blank\" rel=\"nofollow noopener\"><img src=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2Fa01502ce-adba-3385-8e8a-88c8ada2e44f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;s=85d638efcaafef130b5415830f77a16a\" alt=\"image5.png\" srcset=\"https://qiita-user-contents.imgix.net/https%3A%2F%2Fqiita-image-store.s3.ap-northeast-1.amazonaws.com%2F0%2F2958603%2Fa01502ce-adba-3385-8e8a-88c8ada2e44f.png?ixlib=rb-4.0.0&amp;auto=format&amp;gif-q=60&amp;q=75&amp;w=1400&amp;fit=max&amp;s=900c8229c2ba619bb2b4c9d773ed832a 1x\" data-canonical-src=\"https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/a01502ce-adba-3385-8e8a-88c8ada2e44f.png\" loading=\"lazy\"></a></p>\n<blockquote data-sourcepos=\"333:1-333:523\">\n<p data-sourcepos=\"333:3-333:523\">ぼっち・ざ・ろっく！ BOCCHI THE ROCK! 2023 5.24 WED. RELEASE (私+君)-時間÷ギター= BOCCHI THE ROCK! 光の中へ 結束バンド NEW SINGLE 結束バンド 光の中へ 初回仕様 限定盤 ¥1,320(税込) svwc-70620 収録曲 1 光の中へ 2 青い春と西の空 3 光の中へ -instrumental- 4 青い春と西の空 -instrumental- 封入特典 結束バンドLIVE -恒星- バックステージパス風ステッカー 先着購入特典 ジャケットイラスト &amp; ロゴステッカーシート</p>\n</blockquote>\n<h3 data-sourcepos=\"335:1-335:19\">\n<span id=\"評価用実装\" class=\"fragment\"></span><a href=\"#%E8%A9%95%E4%BE%A1%E7%94%A8%E5%AE%9F%E8%A3%85\"><i class=\"fa fa-link\"></i></a>評価用実装</h3>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"337:1-572:3\"><div class=\"highlight\"><pre><code><span class=\"kn\">from</span> <span class=\"n\">pydantic</span> <span class=\"kn\">import</span> <span class=\"n\">BaseModel</span>\n<span class=\"kn\">from</span> <span class=\"n\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">List</span>\n<span class=\"kn\">import</span> <span class=\"n\">base64</span>\n<span class=\"kn\">from</span> <span class=\"n\">collections</span> <span class=\"kn\">import</span> <span class=\"n\">Counter</span>\n<span class=\"kn\">import</span> <span class=\"n\">re</span>\n<span class=\"kn\">from</span> <span class=\"n\">google.api_core.client_options</span> <span class=\"kn\">import</span> <span class=\"n\">ClientOptions</span>\n<span class=\"kn\">from</span> <span class=\"n\">google.cloud</span> <span class=\"kn\">import</span> <span class=\"n\">documentai</span>\n<span class=\"kn\">import</span> <span class=\"n\">openai</span>\n<span class=\"kn\">import</span> <span class=\"n\">MeCab</span>\n<span class=\"kn\">from</span> <span class=\"n\">pdf2image</span> <span class=\"kn\">import</span> <span class=\"n\">convert_from_path</span>\n<span class=\"kn\">import</span> <span class=\"n\">google.generativeai</span> <span class=\"k\">as</span> <span class=\"n\">genai</span>\n<span class=\"kn\">import</span> <span class=\"n\">json</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">OCRResult</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">extracted_texts</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span>\n\n<span class=\"c1\"># サンプル画像と対応する正解テキスト\n</span><span class=\"n\">sample_images</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">data/image1.png</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">data/image2.png</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">data/image3.png</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">data/image4.png</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">data/image5.png</span><span class=\"sh\">'</span><span class=\"p\">]</span>\n<span class=\"n\">ground_truth_texts</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"sh\">\"</span><span class=\"s\">請求書 株式会社 Dark AI 御中 請求No. 2024110103 請求日 2024/11/01 件名： 社会奉仕活動 渋谷太郎 下記の通り、ご請求申し上げます。 〒150-0002 東京都渋谷区渋谷1-1-1 渋谷ヒルズ1001 TEL： 090-1919-1919 合計金額 ¥16,500 （税込） お支払期限： 2024/11/30 No. 摘要 数量 単価 金額 社会奉仕活動 50 時間 300 ¥15,000 小計 ¥15,000 8%消費税 ¥0 10%消費税 ¥1,500 合計 ¥16,500 お振込先 最高銀行 渋谷支店 普通 1129917 シブヤ　タロウ 備考</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n    <span class=\"sh\">\"</span><span class=\"s\">栄養成分表示(100ml当たり)/エネルギー0kcalたんぱく質・脂質・炭水化物0g、ナトリウム1.1mg(食塩相当量0.003g)、カルシウム0.72mg、カリウム0.09mg、マグネシウム0.23mg</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n    <span class=\"sh\">\"</span><span class=\"s\">Jupyterでトークナイズ データセット作りたい→何分かかる？ PDFデータなら「分かっている. 対象 PDF(テキスト入り) スキャンデータ 写真 傾き 手書き フォントへの頑健性 文字の明瞭化 誤り訂正アプローチ seq-to-seqベース レイアウト←どう定義するか 文字 レシート OCR template Matching LLMによる訂正→文脈として、大量に渡す 対象の画像に入ってそうな文字列を渡す</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n    <span class=\"sh\">\"</span><span class=\"s\">音羽山清水寺 第九十六 大吉 にわとりほうをおうておなじくとぶ 鶏逐鳳同飛 レ こうりんうぎをとゝのう 高林整羽儀 二 一 ふねにさおさしてすべからくきしにわたるべし 棹船須湾岸 レ レ レ ほうかふねにみちてかえらん 宝貸満船帰 レ ◯このみくじにあう人は、人を見くだし、軽んずることなく、万事つゝましやかにして、たとえ地位低き人たりとも大切にして、仕事に勉強すれば、家内の仕合せきわめてよろし◯望み事叶う◯病人次第に本ぷくすべし◯あらそいごと勝ちなり◯失せ物出ること遅し◯転居、ふしん、旅行ゆる〱してよし◯えんだん吉◯売買吉◯職業は金、土、木、紙に縁あるものよし◯あきないもよし◯子に賢く出世するものあり、大切に育つべし くゝりつけないでお持ち帰り下さい。</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"c1\"># くの字点 (ゆる〱の『〱』)、平仮名繰返し記号 (ゝ) が含まれる\n</span>    <span class=\"sh\">\"</span><span class=\"s\">ぼっち・ざ・ろっく！ BOCCHI THE ROCK! 2023 5.24 WED. RELEASE (私+君)-時間÷ギター= BOCCHI THE ROCK! 光の中へ 結束バンド NEW SINGLE 結束バンド 光の中へ 初回仕様 限定盤 ¥1,320(税込) svwc-70620 収録曲 1 光の中へ 2 青い春と西の空 3 光の中へ -instrumental- 4 青い春と西の空 -instrumental- 封入特典 結束バンドLIVE -恒星- バックステージパス風ステッカー 先着購入特典 ジャケットイラスト &amp; ロゴステッカーシート</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n<span class=\"p\">]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">encode_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    画像ファイルをBase64エンコードする関数\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"k\">with</span> <span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">rb</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">image_file</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">base64</span><span class=\"p\">.</span><span class=\"nf\">b64encode</span><span class=\"p\">(</span><span class=\"n\">image_file</span><span class=\"p\">.</span><span class=\"nf\">read</span><span class=\"p\">()).</span><span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">utf-8</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">tokenize_japanese</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">list</span><span class=\"p\">:</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    日本語のテキストを分かち書きし、単語のみをリストで返す関数\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"c1\"># タブ, 改行, スペース, 全角スペースを削除\n</span>    <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">re</span><span class=\"p\">.</span><span class=\"nf\">sub</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"sh\">\"</span><span class=\"s\">[ \\t\\n\\r\\u3000]+</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"\"</span><span class=\"p\">,</span> <span class=\"n\">text</span><span class=\"p\">)</span>\n    <span class=\"n\">tagger</span> <span class=\"o\">=</span> <span class=\"n\">MeCab</span><span class=\"p\">.</span><span class=\"nc\">Tagger</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"sh\">'</span><span class=\"s\">-r /opt/homebrew/etc/mecabrc -d /opt/homebrew/lib/mecab/dic/ipadic</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n    <span class=\"n\">parsed</span> <span class=\"o\">=</span> <span class=\"n\">tagger</span><span class=\"p\">.</span><span class=\"nf\">parse</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n    <span class=\"n\">tokens</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">parsed</span><span class=\"p\">.</span><span class=\"nf\">splitlines</span><span class=\"p\">():</span>\n        <span class=\"k\">if</span> <span class=\"n\">line</span> <span class=\"o\">==</span> <span class=\"sh\">\"</span><span class=\"s\">EOS</span><span class=\"sh\">\"</span> <span class=\"ow\">or</span> <span class=\"n\">line</span> <span class=\"o\">==</span> <span class=\"sh\">\"\"</span><span class=\"p\">:</span>\n            <span class=\"k\">continue</span>\n        <span class=\"n\">surface</span> <span class=\"o\">=</span> <span class=\"n\">line</span><span class=\"p\">.</span><span class=\"nf\">split</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"se\">\\t</span><span class=\"sh\">\"</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n        <span class=\"n\">tokens</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">surface</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">tokens</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">compute_f1_score</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">,</span> <span class=\"n\">y_pred</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">tuple</span><span class=\"p\">:</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Precision、Recall、F1スコアを計算する関数\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"n\">y_true_counter</span> <span class=\"o\">=</span> <span class=\"nc\">Counter</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">)</span>\n    <span class=\"n\">y_pred_counter</span> <span class=\"o\">=</span> <span class=\"nc\">Counter</span><span class=\"p\">(</span><span class=\"n\">y_pred</span><span class=\"p\">)</span>\n\n    <span class=\"n\">intersection</span> <span class=\"o\">=</span> <span class=\"n\">y_true_counter</span> <span class=\"o\">&amp;</span> <span class=\"n\">y_pred_counter</span>\n    <span class=\"n\">true_positives</span> <span class=\"o\">=</span> <span class=\"nf\">sum</span><span class=\"p\">(</span><span class=\"n\">intersection</span><span class=\"p\">.</span><span class=\"nf\">values</span><span class=\"p\">())</span>\n\n    <span class=\"n\">precision</span> <span class=\"o\">=</span> <span class=\"n\">true_positives</span> <span class=\"o\">/</span> <span class=\"nf\">sum</span><span class=\"p\">(</span><span class=\"n\">y_pred_counter</span><span class=\"p\">.</span><span class=\"nf\">values</span><span class=\"p\">())</span> <span class=\"k\">if</span> <span class=\"n\">y_pred_counter</span> <span class=\"k\">else</span> <span class=\"mi\">0</span>\n    <span class=\"n\">recall</span> <span class=\"o\">=</span> <span class=\"n\">true_positives</span> <span class=\"o\">/</span> <span class=\"nf\">sum</span><span class=\"p\">(</span><span class=\"n\">y_true_counter</span><span class=\"p\">.</span><span class=\"nf\">values</span><span class=\"p\">())</span> <span class=\"k\">if</span> <span class=\"n\">y_true_counter</span> <span class=\"k\">else</span> <span class=\"mi\">0</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">precision</span> <span class=\"o\">+</span> <span class=\"n\">recall</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">f1</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">f1</span> <span class=\"o\">=</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">precision</span> <span class=\"o\">*</span> <span class=\"n\">recall</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">precision</span> <span class=\"o\">+</span> <span class=\"n\">recall</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">precision</span><span class=\"p\">,</span> <span class=\"n\">recall</span><span class=\"p\">,</span> <span class=\"n\">f1</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">extract_text_with_documentai</span><span class=\"p\">(</span><span class=\"n\">project_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n                                 <span class=\"n\">processor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n                                 <span class=\"n\">mime_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    Google Document AIを使用してテキストを抽出する関数\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"n\">docai_client</span> <span class=\"o\">=</span> <span class=\"n\">documentai</span><span class=\"p\">.</span><span class=\"nc\">DocumentProcessorServiceClient</span><span class=\"p\">(</span>\n        <span class=\"n\">client_options</span><span class=\"o\">=</span><span class=\"nc\">ClientOptions</span><span class=\"p\">(</span><span class=\"n\">api_endpoint</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"si\">{</span><span class=\"n\">location</span><span class=\"si\">}</span><span class=\"s\">-documentai.googleapis.com</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">credentials_file</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">./key.json</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">resource_name</span> <span class=\"o\">=</span> <span class=\"n\">docai_client</span><span class=\"p\">.</span><span class=\"nf\">processor_path</span><span class=\"p\">(</span><span class=\"n\">project_id</span><span class=\"p\">,</span> <span class=\"n\">location</span><span class=\"p\">,</span> <span class=\"n\">processor_id</span><span class=\"p\">)</span>\n\n    <span class=\"k\">with</span> <span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">file_path</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">rb</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">image</span><span class=\"p\">:</span>\n        <span class=\"n\">image_content</span> <span class=\"o\">=</span> <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">read</span><span class=\"p\">()</span>\n    <span class=\"n\">raw_document</span> <span class=\"o\">=</span> <span class=\"n\">documentai</span><span class=\"p\">.</span><span class=\"nc\">RawDocument</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"n\">image_content</span><span class=\"p\">,</span> <span class=\"n\">mime_type</span><span class=\"o\">=</span><span class=\"n\">mime_type</span><span class=\"p\">)</span>\n    <span class=\"n\">request</span> <span class=\"o\">=</span> <span class=\"n\">documentai</span><span class=\"p\">.</span><span class=\"nc\">ProcessRequest</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">resource_name</span><span class=\"p\">,</span> <span class=\"n\">raw_document</span><span class=\"o\">=</span><span class=\"n\">raw_document</span><span class=\"p\">)</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">docai_client</span><span class=\"p\">.</span><span class=\"nf\">process_document</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"o\">=</span><span class=\"n\">request</span><span class=\"p\">)</span>\n    <span class=\"n\">document_object</span> <span class=\"o\">=</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">document</span>\n    <span class=\"k\">return</span> <span class=\"n\">document_object</span><span class=\"p\">.</span><span class=\"n\">text</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">extract_text_with_gpt4o</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">openai_api_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    gpt-4oを使用してテキストを抽出する関数\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"n\">openai_client</span> <span class=\"o\">=</span> <span class=\"n\">openai</span><span class=\"p\">.</span><span class=\"nc\">Client</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"n\">openai_api_key</span><span class=\"p\">)</span>\n    <span class=\"n\">base64_image</span> <span class=\"o\">=</span> <span class=\"nf\">encode_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)</span>\n    <span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"sh\">\"</span><span class=\"s\">role</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">system</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"sh\">\"</span><span class=\"s\">content</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">あなたは非常に優秀なOCRです。画像から**すべて**の文字列を抽出してください。</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"sh\">\"</span><span class=\"s\">role</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">user</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"sh\">\"</span><span class=\"s\">content</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">type</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">image_url</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">image_url</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"sh\">\"</span><span class=\"s\">url</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">data:image/jpeg;base64,</span><span class=\"si\">{</span><span class=\"n\">base64_image</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n                    <span class=\"p\">},</span>\n                <span class=\"p\">},</span>\n            <span class=\"p\">],</span>\n        <span class=\"p\">},</span>\n    <span class=\"p\">]</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">openai_client</span><span class=\"p\">.</span><span class=\"n\">beta</span><span class=\"p\">.</span><span class=\"n\">chat</span><span class=\"p\">.</span><span class=\"n\">completions</span><span class=\"p\">.</span><span class=\"nf\">parse</span><span class=\"p\">(</span>\n        <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">gpt-4o</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"n\">messages</span><span class=\"p\">,</span>\n        <span class=\"n\">response_format</span><span class=\"o\">=</span><span class=\"n\">OCRResult</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n    <span class=\"n\">ocr_result</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"p\">.</span><span class=\"nf\">loads</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">choices</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">message</span><span class=\"p\">.</span><span class=\"n\">content</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"sh\">\"\"</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">(</span><span class=\"n\">ocr_result</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">extracted_texts</span><span class=\"sh\">'</span><span class=\"p\">])</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">extract_text_with_gemini</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">gemini_api_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    gemini-1.5-pro-002を使用してテキストを抽出する関数\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"nf\">configure</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"n\">gemini_api_key</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"nc\">GenerativeModel</span><span class=\"p\">(</span><span class=\"n\">model_name</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">gemini-1.5-pro-002</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n    <span class=\"n\">prompt</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">あなたは非常に優秀なOCRです。画像から**すべて**の文字列を抽出してください。</span><span class=\"sh\">\"</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nf\">generate_content</span><span class=\"p\">(</span>\n        <span class=\"p\">[{</span><span class=\"sh\">'</span><span class=\"s\">mime_type</span><span class=\"sh\">'</span><span class=\"p\">:</span><span class=\"sh\">'</span><span class=\"s\">image/png</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">data</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"nf\">encode_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)},</span> <span class=\"n\">prompt</span><span class=\"p\">],</span>\n        <span class=\"n\">generation_config</span><span class=\"o\">=</span><span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"nc\">GenerationConfig</span><span class=\"p\">(</span>\n            <span class=\"n\">response_mime_type</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">application/json</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"n\">response_schema</span> <span class=\"o\">=</span> <span class=\"n\">OCRResult</span><span class=\"p\">,</span>\n        <span class=\"p\">),</span>\n        <span class=\"n\">request_options</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"sh\">\"</span><span class=\"s\">timeout</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"mi\">600</span><span class=\"p\">},</span>  <span class=\"c1\"># タイムアウト\n</span>    <span class=\"p\">)</span>\n\n    <span class=\"n\">ocr_result</span> <span class=\"o\">=</span> <span class=\"n\">json</span><span class=\"p\">.</span><span class=\"nf\">loads</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"sh\">\"\"</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">(</span><span class=\"n\">ocr_result</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">extracted_texts</span><span class=\"sh\">'</span><span class=\"p\">])</span>\n\n<span class=\"kn\">from</span> <span class=\"n\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">display_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"n\">title</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">):</span>\n    <span class=\"sh\">\"\"\"</span><span class=\"s\">\n    指定した画像を表示する関数\n    </span><span class=\"sh\">\"\"\"</span>\n    <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"nf\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"nf\">imshow</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"nf\">axis</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">off</span><span class=\"sh\">'</span><span class=\"p\">)</span>  <span class=\"c1\"># 軸を非表示\n</span>    <span class=\"k\">if</span> <span class=\"n\">title</span><span class=\"p\">:</span>\n        <span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"nf\">title</span><span class=\"p\">(</span><span class=\"n\">title</span><span class=\"p\">,</span> <span class=\"n\">fontsize</span><span class=\"o\">=</span><span class=\"mi\">16</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"p\">.</span><span class=\"nf\">show</span><span class=\"p\">()</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">main_process</span><span class=\"p\">():</span>\n    <span class=\"n\">project_id</span> <span class=\"o\">=</span> <span class=\"sh\">\"\"</span>\n    <span class=\"n\">location</span> <span class=\"o\">=</span> <span class=\"sh\">\"\"</span>\n    <span class=\"n\">processor_id</span> <span class=\"o\">=</span> <span class=\"sh\">\"\"</span>\n\n    <span class=\"n\">openai_api_key</span> <span class=\"o\">=</span> <span class=\"sh\">\"\"</span>\n    <span class=\"n\">gemini_api_key</span> <span class=\"o\">=</span> <span class=\"sh\">\"\"</span>\n\n    <span class=\"c1\"># 評価結果の格納\n</span>    <span class=\"n\">evaluation_results</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n        <span class=\"sh\">'</span><span class=\"s\">Google Document AI</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"p\">[],</span>\n        <span class=\"sh\">'</span><span class=\"s\">gpt-4o</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"p\">[],</span>\n        <span class=\"sh\">'</span><span class=\"s\">gemini-1.5-pro-002</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"p\">[]</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"c1\"># 各画像に対する処理\n</span>    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"n\">ground_truth_text</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nf\">enumerate</span><span class=\"p\">(</span><span class=\"nf\">zip</span><span class=\"p\">(</span><span class=\"n\">sample_images</span><span class=\"p\">,</span> <span class=\"n\">ground_truth_texts</span><span class=\"p\">)):</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">Processing Image </span><span class=\"si\">{</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">: </span><span class=\"si\">{</span><span class=\"n\">image_path</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"c1\"># plot image\n</span>        <span class=\"nf\">display_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"n\">title</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">Image </span><span class=\"si\">{</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># 正解テキストのトークン化\n</span>        <span class=\"n\">y_true</span> <span class=\"o\">=</span> <span class=\"nf\">tokenize_japanese</span><span class=\"p\">(</span><span class=\"n\">ground_truth_text</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Google Document AIでテキスト抽出\n</span>        <span class=\"n\">mime_type</span> <span class=\"o\">=</span> <span class=\"sh\">'</span><span class=\"s\">image/png</span><span class=\"sh\">'</span>\n        <span class=\"n\">document_text</span> <span class=\"o\">=</span> <span class=\"nf\">extract_text_with_documentai</span><span class=\"p\">(</span>\n            <span class=\"n\">project_id</span><span class=\"o\">=</span><span class=\"n\">project_id</span><span class=\"p\">,</span>\n            <span class=\"n\">location</span><span class=\"o\">=</span><span class=\"n\">location</span><span class=\"p\">,</span>\n            <span class=\"n\">processor_id</span><span class=\"o\">=</span><span class=\"n\">processor_id</span><span class=\"p\">,</span>\n            <span class=\"n\">file_path</span><span class=\"o\">=</span><span class=\"n\">image_path</span><span class=\"p\">,</span>\n            <span class=\"n\">mime_type</span><span class=\"o\">=</span><span class=\"n\">mime_type</span>\n        <span class=\"p\">)</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">document_text</span><span class=\"p\">.</span><span class=\"nf\">replace</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"se\">\\n</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"\"</span><span class=\"p\">))</span>\n        <span class=\"n\">y_pred_docai</span> <span class=\"o\">=</span> <span class=\"nf\">tokenize_japanese</span><span class=\"p\">(</span><span class=\"n\">document_text</span><span class=\"p\">)</span>\n        <span class=\"n\">precision_docai</span><span class=\"p\">,</span> <span class=\"n\">recall_docai</span><span class=\"p\">,</span> <span class=\"n\">f1_docai</span> <span class=\"o\">=</span> <span class=\"nf\">compute_f1_score</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">,</span> <span class=\"n\">y_pred_docai</span><span class=\"p\">)</span>\n        <span class=\"n\">evaluation_results</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">Google Document AI</span><span class=\"sh\">'</span><span class=\"p\">].</span><span class=\"nf\">append</span><span class=\"p\">({</span>\n            <span class=\"sh\">'</span><span class=\"s\">precision</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">precision_docai</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">recall</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">recall_docai</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">f1</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">f1_docai</span>\n        <span class=\"p\">})</span>\n\n        <span class=\"c1\"># gpt-4oでテキスト抽出\n</span>        <span class=\"n\">ocr_text_gpt4o</span> <span class=\"o\">=</span> <span class=\"nf\">extract_text_with_gpt4o</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"n\">openai_api_key</span><span class=\"p\">)</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">ocr_text_gpt4o</span><span class=\"p\">)</span>\n        <span class=\"n\">y_pred_gpt4o</span> <span class=\"o\">=</span> <span class=\"nf\">tokenize_japanese</span><span class=\"p\">(</span><span class=\"n\">ocr_text_gpt4o</span><span class=\"p\">)</span>\n        <span class=\"n\">precision_gpt4o</span><span class=\"p\">,</span> <span class=\"n\">recall_gpt4o</span><span class=\"p\">,</span> <span class=\"n\">f1_gpt4o</span> <span class=\"o\">=</span> <span class=\"nf\">compute_f1_score</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">,</span> <span class=\"n\">y_pred_gpt4o</span><span class=\"p\">)</span>\n        <span class=\"n\">evaluation_results</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">gpt-4o</span><span class=\"sh\">'</span><span class=\"p\">].</span><span class=\"nf\">append</span><span class=\"p\">({</span>\n            <span class=\"sh\">'</span><span class=\"s\">precision</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">precision_gpt4o</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">recall</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">recall_gpt4o</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">f1</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">f1_gpt4o</span>\n        <span class=\"p\">})</span>\n\n        <span class=\"c1\"># gemini-1.5-pro-002でテキスト抽出\n</span>        <span class=\"n\">ocr_text_gemini</span> <span class=\"o\">=</span> <span class=\"nf\">extract_text_with_gemini</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"n\">gemini_api_key</span><span class=\"p\">)</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"n\">ocr_text_gemini</span><span class=\"p\">)</span>\n        <span class=\"n\">y_pred_gemini</span> <span class=\"o\">=</span> <span class=\"nf\">tokenize_japanese</span><span class=\"p\">(</span><span class=\"n\">ocr_text_gemini</span><span class=\"p\">)</span>\n        <span class=\"n\">precision_gemini</span><span class=\"p\">,</span> <span class=\"n\">recall_gemini</span><span class=\"p\">,</span> <span class=\"n\">f1_gemini</span> <span class=\"o\">=</span> <span class=\"nf\">compute_f1_score</span><span class=\"p\">(</span><span class=\"n\">y_true</span><span class=\"p\">,</span> <span class=\"n\">y_pred_gemini</span><span class=\"p\">)</span>\n        <span class=\"n\">evaluation_results</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">gemini-1.5-pro-002</span><span class=\"sh\">'</span><span class=\"p\">].</span><span class=\"nf\">append</span><span class=\"p\">({</span>\n            <span class=\"sh\">'</span><span class=\"s\">precision</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">precision_gemini</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">recall</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">recall_gemini</span><span class=\"p\">,</span>\n            <span class=\"sh\">'</span><span class=\"s\">f1</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"n\">f1_gemini</span>\n        <span class=\"p\">})</span>\n    <span class=\"k\">return</span> <span class=\"n\">evaluation_results</span>\n\n    <span class=\"c1\"># 結果の出力\n</span>    <span class=\"k\">for</span> <span class=\"n\">model_name</span><span class=\"p\">,</span> <span class=\"n\">results</span> <span class=\"ow\">in</span> <span class=\"n\">evaluation_results</span><span class=\"p\">.</span><span class=\"nf\">items</span><span class=\"p\">():</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"se\">\\n</span><span class=\"s\">Results for </span><span class=\"si\">{</span><span class=\"n\">model_name</span><span class=\"si\">}</span><span class=\"s\">:</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n        <span class=\"n\">total_precision</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"n\">total_recall</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"n\">total_f1</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">res</span> <span class=\"ow\">in</span> <span class=\"nf\">enumerate</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">):</span>\n            <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">Image </span><span class=\"si\">{</span><span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"si\">}</span><span class=\"s\">: Precision=</span><span class=\"si\">{</span><span class=\"n\">res</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">precision</span><span class=\"sh\">'</span><span class=\"p\">]</span><span class=\"si\">:</span><span class=\"p\">.</span><span class=\"mi\">4</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">, Recall=</span><span class=\"si\">{</span><span class=\"n\">res</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">recall</span><span class=\"sh\">'</span><span class=\"p\">]</span><span class=\"si\">:</span><span class=\"p\">.</span><span class=\"mi\">4</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">, F1=</span><span class=\"si\">{</span><span class=\"n\">res</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">f1</span><span class=\"sh\">'</span><span class=\"p\">]</span><span class=\"si\">:</span><span class=\"p\">.</span><span class=\"mi\">4</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n            <span class=\"n\">total_precision</span> <span class=\"o\">+=</span> <span class=\"n\">res</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">precision</span><span class=\"sh\">'</span><span class=\"p\">]</span>\n            <span class=\"n\">total_recall</span> <span class=\"o\">+=</span> <span class=\"n\">res</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">recall</span><span class=\"sh\">'</span><span class=\"p\">]</span>\n            <span class=\"n\">total_f1</span> <span class=\"o\">+=</span> <span class=\"n\">res</span><span class=\"p\">[</span><span class=\"sh\">'</span><span class=\"s\">f1</span><span class=\"sh\">'</span><span class=\"p\">]</span>\n        <span class=\"n\">avg_precision</span> <span class=\"o\">=</span> <span class=\"n\">total_precision</span> <span class=\"o\">/</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n        <span class=\"n\">avg_recall</span> <span class=\"o\">=</span> <span class=\"n\">total_recall</span> <span class=\"o\">/</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n        <span class=\"n\">avg_f1</span> <span class=\"o\">=</span> <span class=\"n\">total_f1</span> <span class=\"o\">/</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">)</span>\n        <span class=\"nf\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">Average: Precision=</span><span class=\"si\">{</span><span class=\"n\">avg_precision</span><span class=\"si\">:</span><span class=\"p\">.</span><span class=\"mi\">4</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">, Recall=</span><span class=\"si\">{</span><span class=\"n\">avg_recall</span><span class=\"si\">:</span><span class=\"p\">.</span><span class=\"mi\">4</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">, F1=</span><span class=\"si\">{</span><span class=\"n\">avg_f1</span><span class=\"si\">:</span><span class=\"p\">.</span><span class=\"mi\">4</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n</code></pre></div></div>\n<h3 data-sourcepos=\"574:1-574:25\">\n<span id=\"検証用gradio実装\" class=\"fragment\"></span><a href=\"#%E6%A4%9C%E8%A8%BC%E7%94%A8gradio%E5%AE%9F%E8%A3%85\"><i class=\"fa fa-link\"></i></a>検証用Gradio実装</h3>\n<div class=\"code-frame\" data-lang=\"python\" data-sourcepos=\"576:1-878:3\"><div class=\"highlight\"><pre><code><span class=\"kn\">from</span> <span class=\"n\">typing</span> <span class=\"kn\">import</span> <span class=\"n\">List</span><span class=\"p\">,</span> <span class=\"n\">Dict</span>\n<span class=\"kn\">from</span> <span class=\"n\">pydantic</span> <span class=\"kn\">import</span> <span class=\"n\">BaseModel</span><span class=\"p\">,</span> <span class=\"n\">Field</span>\n<span class=\"kn\">import</span> <span class=\"n\">gradio</span> <span class=\"k\">as</span> <span class=\"n\">gr</span>\n<span class=\"kn\">import</span> <span class=\"n\">base64</span>\n<span class=\"kn\">import</span> <span class=\"n\">cv2</span>\n<span class=\"kn\">import</span> <span class=\"n\">numpy</span> <span class=\"k\">as</span> <span class=\"n\">np</span>\n<span class=\"kn\">from</span> <span class=\"n\">google.cloud</span> <span class=\"kn\">import</span> <span class=\"n\">documentai</span>\n<span class=\"kn\">from</span> <span class=\"n\">google.api_core.client_options</span> <span class=\"kn\">import</span> <span class=\"n\">ClientOptions</span>\n<span class=\"kn\">import</span> <span class=\"n\">openai</span>\n<span class=\"kn\">import</span> <span class=\"n\">google.generativeai</span> <span class=\"k\">as</span> <span class=\"n\">genai</span>\n<span class=\"kn\">import</span> <span class=\"n\">json</span>\n<span class=\"kn\">from</span> <span class=\"n\">PIL</span> <span class=\"kn\">import</span> <span class=\"n\">Image</span><span class=\"p\">,</span> <span class=\"n\">ImageDraw</span><span class=\"p\">,</span> <span class=\"n\">ImageFont</span>\n<span class=\"kn\">import</span> <span class=\"n\">Levenshtein</span>\n<span class=\"kn\">from</span> <span class=\"n\">dataclasses</span> <span class=\"kn\">import</span> <span class=\"n\">dataclass</span><span class=\"p\">,</span> <span class=\"n\">field</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">ExtractedElement</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"nc\">Field</span><span class=\"p\">(...,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">ID number of the extracted element</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n    <span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"nc\">Field</span><span class=\"p\">(...,</span> <span class=\"n\">description</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">The extracted text content</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">OCRResult</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">extracted_elements</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">ExtractedElement</span><span class=\"p\">]</span>\n\n<span class=\"nd\">@dataclass</span>\n<span class=\"k\">class</span> <span class=\"nc\">DocumentElement</span><span class=\"p\">:</span>\n    <span class=\"n\">text</span><span class=\"p\">:</span> <span class=\"nb\">str</span>\n    <span class=\"n\">bbox</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]]</span>  <span class=\"c1\"># [{\"x\": x1, \"y\": y1}, ..., {\"x\": x4, \"y\": y4}]\n</span>    <span class=\"nb\">id</span><span class=\"p\">:</span> <span class=\"nb\">int</span>\n    <span class=\"n\">candidates</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nf\">field</span><span class=\"p\">(</span><span class=\"n\">default_factory</span><span class=\"o\">=</span><span class=\"nb\">list</span><span class=\"p\">)</span>\n    <span class=\"n\">confidence_score</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">encode_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"k\">with</span> <span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">rb</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">image_file</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">base64</span><span class=\"p\">.</span><span class=\"nf\">b64encode</span><span class=\"p\">(</span><span class=\"n\">image_file</span><span class=\"p\">.</span><span class=\"nf\">read</span><span class=\"p\">()).</span><span class=\"nf\">decode</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">utf-8</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">extract_text_with_documentai</span><span class=\"p\">(</span><span class=\"n\">project_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">processor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n                                 <span class=\"n\">file_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">mime_type</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">DocumentElement</span><span class=\"p\">]:</span>\n\n    <span class=\"n\">docai_client</span> <span class=\"o\">=</span> <span class=\"n\">documentai</span><span class=\"p\">.</span><span class=\"nc\">DocumentProcessorServiceClient</span><span class=\"p\">(</span>\n        <span class=\"n\">client_options</span><span class=\"o\">=</span><span class=\"nc\">ClientOptions</span><span class=\"p\">(</span><span class=\"n\">api_endpoint</span><span class=\"o\">=</span><span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"si\">{</span><span class=\"n\">location</span><span class=\"si\">}</span><span class=\"s\">-documentai.googleapis.com</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">credentials_file</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">./key.json</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"n\">resource_name</span> <span class=\"o\">=</span> <span class=\"n\">docai_client</span><span class=\"p\">.</span><span class=\"nf\">processor_path</span><span class=\"p\">(</span><span class=\"n\">project_id</span><span class=\"p\">,</span> <span class=\"n\">location</span><span class=\"p\">,</span> <span class=\"n\">processor_id</span><span class=\"p\">)</span>\n\n    <span class=\"k\">with</span> <span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">file_path</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">rb</span><span class=\"sh\">\"</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">image</span><span class=\"p\">:</span>\n        <span class=\"n\">image_content</span> <span class=\"o\">=</span> <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">read</span><span class=\"p\">()</span>\n\n    <span class=\"n\">raw_document</span> <span class=\"o\">=</span> <span class=\"n\">documentai</span><span class=\"p\">.</span><span class=\"nc\">RawDocument</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"n\">image_content</span><span class=\"p\">,</span> <span class=\"n\">mime_type</span><span class=\"o\">=</span><span class=\"n\">mime_type</span><span class=\"p\">)</span>\n\n    <span class=\"n\">request</span> <span class=\"o\">=</span> <span class=\"n\">documentai</span><span class=\"p\">.</span><span class=\"nc\">ProcessRequest</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">resource_name</span><span class=\"p\">,</span> <span class=\"n\">raw_document</span><span class=\"o\">=</span><span class=\"n\">raw_document</span><span class=\"p\">)</span>\n\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">docai_client</span><span class=\"p\">.</span><span class=\"nf\">process_document</span><span class=\"p\">(</span><span class=\"n\">request</span><span class=\"o\">=</span><span class=\"n\">request</span><span class=\"p\">)</span>\n\n    <span class=\"n\">document</span> <span class=\"o\">=</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">document</span>\n    <span class=\"n\">elements</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">page</span> <span class=\"o\">=</span> <span class=\"n\">document</span><span class=\"p\">.</span><span class=\"n\">pages</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">doc_text</span> <span class=\"o\">=</span> <span class=\"n\">document</span><span class=\"p\">.</span><span class=\"n\">text</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">token</span> <span class=\"ow\">in</span> <span class=\"nf\">enumerate</span><span class=\"p\">(</span><span class=\"n\">page</span><span class=\"p\">.</span><span class=\"n\">paragraphs</span><span class=\"p\">):</span>  <span class=\"c1\"># token or paragraph\n</span>        <span class=\"n\">layout</span> <span class=\"o\">=</span> <span class=\"n\">token</span><span class=\"p\">.</span><span class=\"n\">layout</span>\n\n        <span class=\"c1\"># テキストを取得\n</span>        <span class=\"k\">if</span> <span class=\"n\">i</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">doc_text</span><span class=\"p\">[:</span><span class=\"n\">token</span><span class=\"p\">.</span><span class=\"n\">layout</span><span class=\"p\">.</span><span class=\"n\">text_anchor</span><span class=\"p\">.</span><span class=\"n\">text_segments</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">end_index</span><span class=\"p\">]</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">segment</span> <span class=\"o\">=</span> <span class=\"n\">token</span><span class=\"p\">.</span><span class=\"n\">layout</span><span class=\"p\">.</span><span class=\"n\">text_anchor</span><span class=\"p\">.</span><span class=\"n\">text_segments</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n            <span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">doc_text</span><span class=\"p\">[</span><span class=\"n\">segment</span><span class=\"p\">.</span><span class=\"n\">start_index</span><span class=\"p\">:</span><span class=\"n\">segment</span><span class=\"p\">.</span><span class=\"n\">end_index</span><span class=\"p\">]</span>\n\n        <span class=\"n\">bbox</span> <span class=\"o\">=</span> <span class=\"p\">[{</span><span class=\"sh\">\"</span><span class=\"s\">x</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">vertex</span><span class=\"p\">.</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">y</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"n\">vertex</span><span class=\"p\">.</span><span class=\"n\">y</span><span class=\"p\">}</span> <span class=\"k\">for</span> <span class=\"n\">vertex</span> <span class=\"ow\">in</span> <span class=\"n\">layout</span><span class=\"p\">.</span><span class=\"n\">bounding_poly</span><span class=\"p\">.</span><span class=\"n\">vertices</span><span class=\"p\">]</span>\n\n        <span class=\"n\">elements</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"nc\">DocumentElement</span><span class=\"p\">(</span>\n            <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"n\">text</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">(),</span>\n            <span class=\"n\">bbox</span><span class=\"o\">=</span><span class=\"n\">bbox</span><span class=\"p\">,</span>\n            <span class=\"nb\">id</span><span class=\"o\">=</span><span class=\"n\">i</span><span class=\"p\">,</span>\n            <span class=\"n\">candidates</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">text</span><span class=\"p\">.</span><span class=\"nf\">strip</span><span class=\"p\">()]</span>\n        <span class=\"p\">))</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">elements</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">draw_boxes_and_ids</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">elements</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">DocumentElement</span><span class=\"p\">])</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)</span>\n    <span class=\"n\">draw</span> <span class=\"o\">=</span> <span class=\"n\">ImageDraw</span><span class=\"p\">.</span><span class=\"nc\">Draw</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">)</span>\n\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">font</span> <span class=\"o\">=</span> <span class=\"n\">ImageFont</span><span class=\"p\">.</span><span class=\"nf\">truetype</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">fonts/ipagp.ttf</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">)</span>\n    <span class=\"k\">except</span><span class=\"p\">:</span>\n        <span class=\"n\">font</span> <span class=\"o\">=</span> <span class=\"n\">ImageFont</span><span class=\"p\">.</span><span class=\"nf\">load_default</span><span class=\"p\">()</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">element</span> <span class=\"ow\">in</span> <span class=\"n\">elements</span><span class=\"p\">:</span>\n        <span class=\"n\">polygon</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"n\">vertex</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">x</span><span class=\"sh\">\"</span><span class=\"p\">],</span> <span class=\"n\">vertex</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">y</span><span class=\"sh\">\"</span><span class=\"p\">])</span> <span class=\"k\">for</span> <span class=\"n\">vertex</span> <span class=\"ow\">in</span> <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">bbox</span><span class=\"p\">]</span>\n        <span class=\"n\">draw</span><span class=\"p\">.</span><span class=\"nf\">polygon</span><span class=\"p\">(</span><span class=\"n\">polygon</span><span class=\"p\">,</span> <span class=\"n\">outline</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">red</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">width</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n        <span class=\"n\">x1</span><span class=\"p\">,</span> <span class=\"n\">y1</span> <span class=\"o\">=</span> <span class=\"n\">polygon</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n        <span class=\"n\">draw</span><span class=\"p\">.</span><span class=\"nf\">text</span><span class=\"p\">((</span><span class=\"n\">x1</span> <span class=\"o\">-</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">y1</span> <span class=\"o\">-</span> <span class=\"mi\">20</span><span class=\"p\">),</span> <span class=\"nf\">str</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">.</span><span class=\"nb\">id</span><span class=\"p\">),</span> <span class=\"n\">fill</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">red</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">font</span><span class=\"o\">=</span><span class=\"n\">font</span><span class=\"p\">)</span>\n\n    <span class=\"n\">temp_path</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">temp_annotated.png</span><span class=\"sh\">\"</span>\n    <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">temp_path</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">temp_path</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">highlight_elements_by_score</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">elements</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">DocumentElement</span><span class=\"p\">])</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">str</span><span class=\"p\">:</span>\n    <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">).</span><span class=\"nf\">convert</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">RGBA</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n    <span class=\"n\">overlay</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">new</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">RGBA</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"n\">size</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">draw</span> <span class=\"o\">=</span> <span class=\"n\">ImageDraw</span><span class=\"p\">.</span><span class=\"nc\">Draw</span><span class=\"p\">(</span><span class=\"n\">overlay</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">element</span> <span class=\"ow\">in</span> <span class=\"n\">elements</span><span class=\"p\">:</span>\n        <span class=\"n\">score</span> <span class=\"o\">=</span> <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">confidence_score</span>\n        <span class=\"n\">color_intensity</span> <span class=\"o\">=</span> <span class=\"nf\">int</span><span class=\"p\">((</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">score</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">255</span><span class=\"p\">)</span>\n        <span class=\"n\">fill_color</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"mi\">255</span><span class=\"p\">,</span> <span class=\"mi\">255</span> <span class=\"o\">-</span> <span class=\"n\">color_intensity</span><span class=\"p\">,</span> <span class=\"mi\">255</span> <span class=\"o\">-</span> <span class=\"n\">color_intensity</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">)</span>\n\n        <span class=\"n\">polygon</span> <span class=\"o\">=</span> <span class=\"p\">[(</span><span class=\"n\">vertex</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">x</span><span class=\"sh\">\"</span><span class=\"p\">],</span> <span class=\"n\">vertex</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">y</span><span class=\"sh\">\"</span><span class=\"p\">])</span> <span class=\"k\">for</span> <span class=\"n\">vertex</span> <span class=\"ow\">in</span> <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">bbox</span><span class=\"p\">]</span>\n        <span class=\"n\">draw</span><span class=\"p\">.</span><span class=\"nf\">polygon</span><span class=\"p\">(</span><span class=\"n\">polygon</span><span class=\"p\">,</span> <span class=\"n\">fill</span><span class=\"o\">=</span><span class=\"n\">fill_color</span><span class=\"p\">)</span>\n\n    <span class=\"n\">combined</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">alpha_composite</span><span class=\"p\">(</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"n\">overlay</span><span class=\"p\">)</span>\n    <span class=\"n\">temp_path</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">temp_highlighted.png</span><span class=\"sh\">\"</span>\n    <span class=\"n\">combined</span><span class=\"p\">.</span><span class=\"nf\">convert</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">RGB</span><span class=\"sh\">\"</span><span class=\"p\">).</span><span class=\"nf\">save</span><span class=\"p\">(</span><span class=\"n\">temp_path</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">temp_path</span>\n\n\n\n<span class=\"k\">def</span> <span class=\"nf\">extract_text_with_gpt4o</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">openai_api_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">ExtractedElement</span><span class=\"p\">]:</span>\n    <span class=\"n\">client</span> <span class=\"o\">=</span> <span class=\"n\">openai</span><span class=\"p\">.</span><span class=\"nc\">Client</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"n\">openai_api_key</span><span class=\"p\">)</span>\n    <span class=\"n\">base64_image</span> <span class=\"o\">=</span> <span class=\"nf\">encode_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)</span>\n\n    <span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n        <span class=\"p\">{</span>\n            <span class=\"sh\">\"</span><span class=\"s\">role</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">system</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"sh\">\"</span><span class=\"s\">content</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">画像から要素を抽出し、各要素のIDと文字列を抽出してください。</span><span class=\"sh\">\"</span>\n        <span class=\"p\">},</span>\n        <span class=\"p\">{</span>\n            <span class=\"sh\">\"</span><span class=\"s\">role</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">user</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"sh\">\"</span><span class=\"s\">content</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"p\">{</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">type</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sh\">\"</span><span class=\"s\">image_url</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n                    <span class=\"sh\">\"</span><span class=\"s\">image_url</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"p\">{</span>\n                        <span class=\"sh\">\"</span><span class=\"s\">url</span><span class=\"sh\">\"</span><span class=\"p\">:</span> <span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">data:image/png;base64,</span><span class=\"si\">{</span><span class=\"n\">base64_image</span><span class=\"si\">}</span><span class=\"sh\">\"</span>\n                    <span class=\"p\">}</span>\n                <span class=\"p\">}</span>\n            <span class=\"p\">]</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">]</span>\n\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"p\">.</span><span class=\"n\">beta</span><span class=\"p\">.</span><span class=\"n\">chat</span><span class=\"p\">.</span><span class=\"n\">completions</span><span class=\"p\">.</span><span class=\"nf\">parse</span><span class=\"p\">(</span>\n        <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">gpt-4o</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n        <span class=\"n\">messages</span><span class=\"o\">=</span><span class=\"n\">messages</span><span class=\"p\">,</span>\n        <span class=\"n\">response_format</span><span class=\"o\">=</span><span class=\"n\">OCRResult</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">OCRResult</span><span class=\"p\">.</span><span class=\"nf\">parse_raw</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">choices</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">].</span><span class=\"n\">message</span><span class=\"p\">.</span><span class=\"n\">content</span><span class=\"p\">)</span> <span class=\"c1\"># 一旦model_validateは使わず\n</span>    <span class=\"k\">return</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">extracted_elements</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">extract_text_with_gemini</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">gemini_api_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">)</span> <span class=\"o\">-&gt;</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">ExtractedElement</span><span class=\"p\">]:</span>\n    <span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"nf\">configure</span><span class=\"p\">(</span><span class=\"n\">api_key</span><span class=\"o\">=</span><span class=\"n\">gemini_api_key</span><span class=\"p\">)</span>\n    <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"nc\">GenerativeModel</span><span class=\"p\">(</span><span class=\"sh\">'</span><span class=\"s\">gemini-1.5-pro-002</span><span class=\"sh\">'</span><span class=\"p\">)</span>\n\n    <span class=\"n\">prompt</span> <span class=\"o\">=</span> <span class=\"sh\">\"</span><span class=\"s\">画像から要素を抽出し、各要素のIDと文字列を抽出してください。</span><span class=\"sh\">\"</span>\n\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"p\">.</span><span class=\"nf\">generate_content</span><span class=\"p\">(</span>\n        <span class=\"p\">[</span>\n            <span class=\"p\">{</span><span class=\"sh\">'</span><span class=\"s\">mime_type</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"sh\">'</span><span class=\"s\">image/png</span><span class=\"sh\">'</span><span class=\"p\">,</span> <span class=\"sh\">'</span><span class=\"s\">data</span><span class=\"sh\">'</span><span class=\"p\">:</span> <span class=\"nf\">encode_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)},</span>\n            <span class=\"n\">prompt</span>\n        <span class=\"p\">],</span>\n        <span class=\"n\">generation_config</span><span class=\"o\">=</span><span class=\"n\">genai</span><span class=\"p\">.</span><span class=\"nc\">GenerationConfig</span><span class=\"p\">(</span>\n            <span class=\"n\">response_mime_type</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">application/json</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"n\">response_schema</span><span class=\"o\">=</span><span class=\"n\">OCRResult</span>\n        <span class=\"p\">)</span>\n    <span class=\"p\">)</span>\n\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">OCRResult</span><span class=\"p\">.</span><span class=\"nf\">parse_raw</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">result</span><span class=\"p\">.</span><span class=\"n\">extracted_elements</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">calculate_confidence_score</span><span class=\"p\">(</span><span class=\"n\">candidates</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">])</span> <span class=\"o\">-&gt;</span> <span class=\"nb\">float</span><span class=\"p\">:</span>\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">candidates</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"mf\">0.0</span>\n\n    <span class=\"n\">total_distance</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"n\">comparisons</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">candidates</span><span class=\"p\">)):</span>\n        <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nf\">range</span><span class=\"p\">(</span><span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">candidates</span><span class=\"p\">)):</span>\n            <span class=\"n\">distance</span> <span class=\"o\">=</span> <span class=\"n\">Levenshtein</span><span class=\"p\">.</span><span class=\"nf\">distance</span><span class=\"p\">(</span><span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">])</span>\n            <span class=\"n\">max_length</span> <span class=\"o\">=</span> <span class=\"nf\">max</span><span class=\"p\">(</span><span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]),</span> <span class=\"nf\">len</span><span class=\"p\">(</span><span class=\"n\">candidates</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]))</span>\n            <span class=\"n\">normalized_distance</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">distance</span> <span class=\"o\">/</span> <span class=\"n\">max_length</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">max_length</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span> <span class=\"k\">else</span> <span class=\"mi\">0</span>\n            <span class=\"n\">total_distance</span> <span class=\"o\">+=</span> <span class=\"n\">normalized_distance</span>\n            <span class=\"n\">comparisons</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">total_distance</span> <span class=\"o\">/</span> <span class=\"n\">comparisons</span> <span class=\"k\">if</span> <span class=\"n\">comparisons</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span> <span class=\"k\">else</span> <span class=\"mf\">0.0</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">crop_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">bbox</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"n\">Dict</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"nb\">int</span><span class=\"p\">]])</span> <span class=\"o\">-&gt;</span> <span class=\"n\">Image</span><span class=\"p\">:</span>\n    <span class=\"n\">image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">)</span>\n    <span class=\"n\">xs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">vertex</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">x</span><span class=\"sh\">\"</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">vertex</span> <span class=\"ow\">in</span> <span class=\"n\">bbox</span><span class=\"p\">]</span>\n    <span class=\"n\">ys</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">vertex</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">y</span><span class=\"sh\">\"</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">vertex</span> <span class=\"ow\">in</span> <span class=\"n\">bbox</span><span class=\"p\">]</span>\n    <span class=\"n\">min_x</span><span class=\"p\">,</span> <span class=\"n\">max_x</span> <span class=\"o\">=</span> <span class=\"nf\">min</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">),</span> <span class=\"nf\">max</span><span class=\"p\">(</span><span class=\"n\">xs</span><span class=\"p\">)</span>\n    <span class=\"n\">min_y</span><span class=\"p\">,</span> <span class=\"n\">max_y</span> <span class=\"o\">=</span> <span class=\"nf\">min</span><span class=\"p\">(</span><span class=\"n\">ys</span><span class=\"p\">),</span> <span class=\"nf\">max</span><span class=\"p\">(</span><span class=\"n\">ys</span><span class=\"p\">)</span>\n    <span class=\"n\">cropped</span> <span class=\"o\">=</span> <span class=\"n\">image</span><span class=\"p\">.</span><span class=\"nf\">crop</span><span class=\"p\">((</span><span class=\"n\">min_x</span><span class=\"p\">,</span> <span class=\"n\">min_y</span><span class=\"p\">,</span> <span class=\"n\">max_x</span><span class=\"p\">,</span> <span class=\"n\">max_y</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">cropped</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">process_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">project_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">location</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">processor_id</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span>\n                  <span class=\"n\">openai_api_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">gemini_api_key</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">):</span>\n    <span class=\"c1\"># OCR\n</span>    <span class=\"n\">elements</span> <span class=\"o\">=</span> <span class=\"nf\">extract_text_with_documentai</span><span class=\"p\">(</span>\n        <span class=\"n\">project_id</span><span class=\"p\">,</span> <span class=\"n\">location</span><span class=\"p\">,</span> <span class=\"n\">processor_id</span><span class=\"p\">,</span> <span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">image/png</span><span class=\"sh\">\"</span>\n    <span class=\"p\">)</span>\n    <span class=\"c1\"># バウンディングボックスとIDを描画\n</span>    <span class=\"n\">annotated_image_path</span> <span class=\"o\">=</span> <span class=\"nf\">draw_boxes_and_ids</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"n\">elements</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># gpt-4oとgeminiでもテキスト抽出\n</span>    <span class=\"n\">gpt4_elements</span> <span class=\"o\">=</span> <span class=\"nf\">extract_text_with_gpt4o</span><span class=\"p\">(</span><span class=\"n\">annotated_image_path</span><span class=\"p\">,</span> <span class=\"n\">openai_api_key</span><span class=\"p\">)</span>\n    <span class=\"n\">gemini_elements</span> <span class=\"o\">=</span> <span class=\"nf\">extract_text_with_gemini</span><span class=\"p\">(</span><span class=\"n\">annotated_image_path</span><span class=\"p\">,</span> <span class=\"n\">gemini_api_key</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">element</span> <span class=\"ow\">in</span> <span class=\"n\">elements</span><span class=\"p\">:</span>\n        <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">candidates</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">]</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">gpt4_elem</span> <span class=\"ow\">in</span> <span class=\"n\">gpt4_elements</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">gpt4_elem</span><span class=\"p\">.</span><span class=\"nb\">id</span> <span class=\"o\">==</span> <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"nb\">id</span><span class=\"p\">:</span>\n                <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">candidates</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">gpt4_elem</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n                <span class=\"k\">break</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">gemini_elem</span> <span class=\"ow\">in</span> <span class=\"n\">gemini_elements</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">gemini_elem</span><span class=\"p\">.</span><span class=\"nb\">id</span> <span class=\"o\">==</span> <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"nb\">id</span><span class=\"p\">:</span>\n                <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">candidates</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">(</span><span class=\"n\">gemini_elem</span><span class=\"p\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n                <span class=\"k\">break</span>\n\n        <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">confidence_score</span> <span class=\"o\">=</span> <span class=\"nf\">calculate_confidence_score</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">candidates</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># ハイライト画像を作成\n</span>    <span class=\"n\">highlighted_image_path</span> <span class=\"o\">=</span> <span class=\"nf\">highlight_elements_by_score</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"n\">elements</span><span class=\"p\">)</span>\n\n    <span class=\"n\">output_elements</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n\n    <span class=\"c1\"># 言語モデルに入力したバウンディングボックス付き画像を追加\n</span>    <span class=\"n\">annotated_image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">annotated_image_path</span><span class=\"p\">)</span>\n    <span class=\"n\">output_elements</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">([</span><span class=\"n\">annotated_image</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">バウンディングボックス付き画像</span><span class=\"sh\">\"</span><span class=\"p\">])</span>\n\n    <span class=\"c1\"># スコアでハイライトされた画像を追加\n</span>    <span class=\"n\">highlighted_image</span> <span class=\"o\">=</span> <span class=\"n\">Image</span><span class=\"p\">.</span><span class=\"nf\">open</span><span class=\"p\">(</span><span class=\"n\">highlighted_image_path</span><span class=\"p\">)</span>\n    <span class=\"n\">output_elements</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">([</span><span class=\"n\">highlighted_image</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">スコアでハイライトされた画像</span><span class=\"sh\">\"</span><span class=\"p\">])</span>\n\n    <span class=\"c1\"># 各要素のクロップ画像とキャプションを追加\n</span>    <span class=\"k\">for</span> <span class=\"n\">element</span> <span class=\"ow\">in</span> <span class=\"n\">elements</span><span class=\"p\">:</span>\n        <span class=\"n\">cropped_image</span> <span class=\"o\">=</span> <span class=\"nf\">crop_image</span><span class=\"p\">(</span><span class=\"n\">image_path</span><span class=\"p\">,</span> <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">bbox</span><span class=\"p\">)</span>\n        <span class=\"n\">caption</span> <span class=\"o\">=</span> <span class=\"sa\">f</span><span class=\"sh\">\"</span><span class=\"s\">ID: </span><span class=\"si\">{</span><span class=\"n\">element</span><span class=\"p\">.</span><span class=\"nb\">id</span><span class=\"si\">}</span><span class=\"s\">, スコア: </span><span class=\"si\">{</span><span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">confidence_score</span><span class=\"si\">:</span><span class=\"p\">.</span><span class=\"mi\">2</span><span class=\"n\">f</span><span class=\"si\">}</span><span class=\"s\">, 候補: </span><span class=\"sh\">\"</span> <span class=\"o\">+</span> <span class=\"sh\">\"</span><span class=\"s\">, </span><span class=\"sh\">\"</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">candidates</span><span class=\"p\">)</span>\n        <span class=\"n\">output_elements</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">([</span><span class=\"n\">cropped_image</span><span class=\"p\">,</span> <span class=\"n\">caption</span><span class=\"p\">])</span>\n\n    <span class=\"c1\"># テーブルデータを作成\n</span>    <span class=\"n\">table_data</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">element</span> <span class=\"ow\">in</span> <span class=\"n\">elements</span><span class=\"p\">:</span>\n        <span class=\"n\">table_data</span><span class=\"p\">.</span><span class=\"nf\">append</span><span class=\"p\">([</span>\n            <span class=\"n\">element</span><span class=\"p\">.</span><span class=\"nb\">id</span><span class=\"p\">,</span>\n            <span class=\"nf\">round</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">confidence_score</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span>\n            <span class=\"sh\">\"</span><span class=\"s\">, </span><span class=\"sh\">\"</span><span class=\"p\">.</span><span class=\"nf\">join</span><span class=\"p\">(</span><span class=\"n\">element</span><span class=\"p\">.</span><span class=\"n\">candidates</span><span class=\"p\">)</span>\n        <span class=\"p\">])</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">output_elements</span><span class=\"p\">,</span> <span class=\"n\">table_data</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">create_gradio_interface</span><span class=\"p\">():</span>\n    <span class=\"k\">with</span> <span class=\"n\">gr</span><span class=\"p\">.</span><span class=\"nc\">Blocks</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">interface</span><span class=\"p\">:</span>\n        <span class=\"n\">gr</span><span class=\"p\">.</span><span class=\"nc\">Markdown</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">## 確信度つきOCR</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"k\">with</span> <span class=\"n\">gr</span><span class=\"p\">.</span><span class=\"nc\">Row</span><span class=\"p\">():</span>\n            <span class=\"n\">input_image</span> <span class=\"o\">=</span> <span class=\"n\">gr</span><span class=\"p\">.</span><span class=\"nc\">Image</span><span class=\"p\">(</span><span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">filepath</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">Input Image</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"k\">with</span> <span class=\"n\">gr</span><span class=\"p\">.</span><span class=\"nc\">Row</span><span class=\"p\">():</span>\n            <span class=\"n\">submit_btn</span> <span class=\"o\">=</span> <span class=\"n\">gr</span><span class=\"p\">.</span><span class=\"nc\">Button</span><span class=\"p\">(</span><span class=\"sh\">\"</span><span class=\"s\">画像を処理</span><span class=\"sh\">\"</span><span class=\"p\">)</span>\n\n        <span class=\"n\">output_gallery</span> <span class=\"o\">=</span> <span class=\"n\">gr</span><span class=\"p\">.</span><span class=\"nc\">Gallery</span><span class=\"p\">(</span>\n            <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">結果</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"n\">show_label</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n            <span class=\"n\">elem_id</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">gallery</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"n\">columns</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span>\n            <span class=\"n\">height</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">auto</span><span class=\"sh\">\"</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"n\">output_table</span> <span class=\"o\">=</span> <span class=\"n\">gr</span><span class=\"p\">.</span><span class=\"nc\">Dataframe</span><span class=\"p\">(</span>\n            <span class=\"n\">headers</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">ID</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">スコア</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">候補</span><span class=\"sh\">\"</span><span class=\"p\">],</span>\n            <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"sh\">\"</span><span class=\"s\">ID、スコア、候補の一覧</span><span class=\"sh\">\"</span><span class=\"p\">,</span>\n            <span class=\"n\">datatype</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"sh\">\"</span><span class=\"s\">number</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">number</span><span class=\"sh\">\"</span><span class=\"p\">,</span> <span class=\"sh\">\"</span><span class=\"s\">str</span><span class=\"sh\">\"</span><span class=\"p\">],</span>\n            <span class=\"n\">wrap</span><span class=\"o\">=</span><span class=\"bp\">True</span>\n        <span class=\"p\">)</span>\n\n        <span class=\"k\">def</span> <span class=\"nf\">process_and_return</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">):</span>\n            <span class=\"n\">outputs</span><span class=\"p\">,</span> <span class=\"n\">table</span> <span class=\"o\">=</span> <span class=\"nf\">process_image</span><span class=\"p\">(</span>\n                <span class=\"n\">img</span><span class=\"p\">,</span>\n                <span class=\"n\">project_id</span><span class=\"o\">=</span><span class=\"sh\">\"\"</span><span class=\"p\">,</span>\n                <span class=\"n\">location</span><span class=\"o\">=</span><span class=\"sh\">\"\"</span><span class=\"p\">,</span>\n                <span class=\"n\">processor_id</span><span class=\"o\">=</span><span class=\"sh\">\"\"</span><span class=\"p\">,</span>\n                <span class=\"n\">openai_api_key</span><span class=\"o\">=</span><span class=\"sh\">\"\"</span><span class=\"p\">,</span>\n                <span class=\"n\">gemini_api_key</span><span class=\"o\">=</span><span class=\"sh\">\"\"</span>\n            <span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"n\">outputs</span><span class=\"p\">,</span> <span class=\"n\">table</span>\n\n        <span class=\"n\">submit_btn</span><span class=\"p\">.</span><span class=\"nf\">click</span><span class=\"p\">(</span>\n            <span class=\"n\">fn</span><span class=\"o\">=</span><span class=\"n\">process_and_return</span><span class=\"p\">,</span>\n            <span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">input_image</span><span class=\"p\">],</span>\n            <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">output_gallery</span><span class=\"p\">,</span> <span class=\"n\">output_table</span><span class=\"p\">]</span>\n        <span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">interface</span>\n\n<span class=\"k\">if</span> <span class=\"n\">__name__</span> <span class=\"o\">==</span> <span class=\"sh\">\"</span><span class=\"s\">__main__</span><span class=\"sh\">\"</span><span class=\"p\">:</span>\n    <span class=\"n\">interface</span> <span class=\"o\">=</span> <span class=\"nf\">create_gradio_interface</span><span class=\"p\">()</span>\n    <span class=\"n\">interface</span><span class=\"p\">.</span><span class=\"nf\">launch</span><span class=\"p\">()</span>\n\n</code></pre></div></div>\n",
      "body": "こんにちは！逆瀬川 ( https://x.com/gyakuse ) です！\n\nこのアドベントカレンダーでは生成AIのアプリケーションを実際に作り、どのように作ればいいのか、ということをわかりやすく書いていければと思います。アプリケーションだけではなく、プロダクト開発に必要なモデルの調査方法、training方法、基礎知識等にも触れていければと思います。12月5日の朝にこれを書いていますが、4日目の記事です。果たして25日まで続くのでしょうか。\n\n## 今回の記事について\n\n本日はレシート等からテキスト情報抽出したときのつらみ、**誤り訂正**についてやっていこうと思います。レシート、請求書、その他諸々、こうしたものはテキスト抽出を用いないとデータベースに効率的に入れることはできず、また各種統計処理等も行えません。そういうときに問題となるのがOCR等での誤認識です。そうした誤り訂正については歴史が長く、幾多もの魔法少女たちが戦ってきました。\n\n特に最近のLMM (ChatGPT等の大規模マルチモーダルモデル) を用いた情報抽出では、OCR結果のほうだけをひと目見て『間違いだー！』となるようなものは減り、『それっぽい間違い』をするようになりました。校正において見比べる回数が増え、ストレスがすごいです。\n\n[**WOZE** という AI OCR を展開されている株式会社ハンモックさんの調査](https://prtimes.jp/main/html/rd/p/000000224.000052725.html)はこの校正処理のつらみを非常によく表しています (めちゃくちゃいい調査だと思います)\n\nということで、今回は一定程度自動で誤り訂正し、かつ**確信度**のようなスコアを一緒に表示することで校正者のストレスを軽減するためのアプリケーションを作っていければと思います。\n\n![kenka_yarouze.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/27b243d5-d51f-5a57-0132-5503c867e1ad.png)\n\n## 想定する読者\n\n- 泣いてるみんな\n\n## 得られる知識について\n\n- GoogleでのOCRの使い方\n- LMM、LVLMを用いた情報抽出手法\n- 視覚的文書からの情報抽出手法 (Visual Document Information Extraction)\n- LMMを用いたVDIE (VIE) 手法\n\nそれではやっていきましょう。\n\n## そもそも: 現在のAIの日本語テキスト抽出能力について\n\nここではとりあえずそもそもどのくらいの精度でテキスト抽出できんの？みたいなのを明らかにしていければと思います。\n\n### 対象モデルについて\n\n以下を対象に、情報抽出のテストをしてみます。\n\n- Google Document AI\n- GPT-4o\n- Gemini 1.5 Pro 002\n\n### 評価手法\n\nサンプル5画像に対してそれぞれ文字を抽出し、F値を計算します。\n\nF値は以下のように計算します。F値は[0, 1]の範囲で示される実数値であり、1に近いほど認識精度が高いです。[NDLラボによる古典籍資料のOCRテキスト化実験（令和4年度～）](https://lab.ndl.go.jp/data_set/r4_koten/) で使用されているものと同じような評価手法ですが、ここでは文字ではなく単語の多重集合としています(古典籍資料とは異なり容易に分かち書きができるため)。\n\n$$\ny_{\\text{true}} = \\{\\text{正解データの単語の多重集合}\\}\n$$\n\n$$\ny_{\\text{pred}} = \\{\\text{予測データの単語の多重集合}\\}\n$$\n\n$$\n\\text{Precision} = \\frac{|y_{\\text{true}} \\cap y_{\\text{pred}}|}{|y_{\\text{pred}}|}\n$$\n\n$$\n\\text{Recall} = \\frac{|y_{\\text{true}} \\cap y_{\\text{pred}}|}{|y_{\\text{true}}|}\n$$\n\n$$\nF_{\\text{measure}} = \\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n$$\n\n### 利用するデータ\n\n手元にあった以下のデータを利用します。\nもとデータにテキスト情報が含まれていない2-5は自分で書き起こしておきます。\n\n![for_ocr.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/2a9ae083-d0f9-da78-7926-fc3a97b52d46.png)\n\n### 実装\n\nAppendix参照\n\n### 検証結果\n\nサンプルが5つなのであまり参考になりませんが、以下のようになりました。\n\n| Image | Metric | Google Document AI | GPT-4o | Gemini-1.5-pro-002 |\n|-------|---------|-------------------|--------|-------------------|\n| 1 | Precision | 0.8960 | 0.9680 | 0.9675 |\n|   | Recall    | 0.8960 | 0.9680 | 0.9520 |\n|   | F1        | 0.8960 | 0.9680 | 0.9597 |\n| 2 | Precision | 1.0000 | 0.8800 | 0.9804 |\n|   | Recall    | 1.0000 | 0.8800 | 1.0000 |\n|   | F1        | 1.0000 | 0.8800 | 0.9901 |\n| 3 | Precision | 0.7105 | 0.2596 | 0.7586 |\n|   | Recall    | 0.7200 | 0.3600 | 0.8800 |\n|   | F1        | 0.7152 | 0.3017 | 0.8148 |\n| 4 | Precision | 0.8667 | 0.7597 | 0.8551 |\n|   | Recall    | 0.8083 | 0.5078 | 0.6114 |\n|   | F1        | 0.8365 | 0.6087 | 0.7130 |\n| 5 | Precision | 0.8812 | 0.8901 | 0.8586 |\n|   | Recall    | 0.8318 | 0.7570 | 0.7944 |\n|   | F1        | 0.8558 | 0.8182 | 0.8252 |\n\n\n| Model | Precision | Recall | F1 |\n|-------|-----------|--------|------|\n| Google Document AI | 0.8709 | 0.8512 | 0.8607 |\n| GPT-4o | 0.7515 | 0.6946 | 0.7153 |\n| Gemini-1.5-pro-002 | 0.8840 | 0.8476 | 0.8606 |\n\nサンプル数が少なすぎる雑プロット:\n\n![zatsu.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/13e51fa5-c742-3ad7-4fc9-6263566c57d0.png)\n\n- F1値だとGemini 1.5 Pro 002とDocument AIが同率一位\n- GPT-4oは特にRecallの低さが顕著で取りこぼしが多い\n- タスク別に見ていくと文字埋め込みのないPDFやスキャンされたデータのようなきれいなテキスト抽出はLMMのほうが優秀 (1)\n- 横書きの活字抽出はどれも同程度 (2, 5)\n- 縦書きの活字抽出はGPT-4oが異様に低くなる (3)\n- 横書きの手書き文字抽出はDocument AIが優秀, LMMは取りこぼしが多くなる (4)\n\n## 視覚的文書からの情報抽出処理の流れについて\n\n次に、認識ミスを考える前にどうやって画像からの情報抽出を行うかを明らかにします。\n\nこの情報抽出においては、単純に画像からテキスト抽出しているだけではなく、情報抽出タスク(構造化データの抽出)も絡んできます。\n\n### わかりやすい事例\n\nhttps://gihyo.jp/article/2023/07/programming-with-chatgpt-04\n\n以前、こちらの記事では、レシート等の画像から自動的に情報抽出する手法を紹介しました。\n\n手法を簡単に説明すると、\n\n- OCRでテキスト情報を画像から抽出\n- ChatGPTで抽出したいデータ構造を定義し、OCRで得られた情報をもとに抽出\n\nという流れです。単純にOCRしただけだと座標とテキストしか得られませんが、情報抽出処理によって構造化することでデータベース等に入れたりするなどの活用ができるようになります。\n\n現在はChatGPTやGeminiが画像等のマルチモーダルに対応したため、別の手法も検討できるようになりました\n\n### 事前処理 (傾き補正等)\n\nOCRのプリプロセスとしては、傾き、歪み、ノイズ等を補正し、文字をシャープ化したり画像を二値化したり (大津やNickの二値化手法が有名です)、いろいろ頑張ります。\n\nこれはLMMでも同様で、鮮明なほうがモテます。\n\nぼやけた文字とかもよくある (許せん) ので、いい感じのシーンテキスト超解像 (STISR) も大事です。Toyotaさんもhttps://github.com/ToyotaInfoTech/stisr-tcdm というのを公開していたりします。https://github.com/yfaqh/Awesome-Scene-Text-Image-Super-Resolution とか便利。\n\nSansanさんのこの資料等も楽しいです。\n\nhttps://speakerdeck.com/sansandsoc/recent-topics-on-character-super-resolution\n\n### モデルによる処理\n\n次にモデルのほうで、抽出処理をします\n\n**OCR Onlyの場合 (わたしのgihyo記事の例がこれにあたる)**\n\n- テキスト抽出処理\n- 情報抽出処理 (例: LLMを利用)\n\n**LMMの場合**\n\n- 画像からそのまま情報抽出処理をする\n\n**組み合わせる場合**\n\n- OCRでのテキスト抽出\n- LMMでの画像・OCR結果を両方考慮した情報抽出\n\nこうしてJSONだったりする構造化データが手に入ります。\n\n## テキスト情報抽出での認識ミスについて\n\nそれでは、悲しいやつを考えていきましょう。OCRやLMMはさきほど見たように、完全な精度で文字を抽出することができません。ということは、構造化データを出すテキスト情報抽出でも、その精度が引き継がれるというわけです。より悪くなったりもします。\n\nではどういうエラーがあるか考えてみましょう。大きく分けて以下の種類が検討できます。\n\n- テキスト自体の認識ミス\n- レイアウト構造の理解の失敗 (情報抽出の問題)\n\n後者に関しては、たとえば請求書で合計の値を取りたかったのに、小計の数字が取れてしまうなどがあります。今回はレイアウト構造の理解の失敗についてはあまり考え (たく) ないので前者について深堀りしましょう。\n\nざっくり以下のようなパターンがあると思います。\n\n- 光学的類似性によるもの\n    - 光学的には似ている文字列を誤って検出してしまうパターン、OCRでよく起きる\n    - 特徴として、文脈的に不自然な単語やフレーズである場合が多い\n    - 例\n        - 八丁目 → ハ丁目 (漢数字の八ではなくカタカナのハになっている)\n        - 一丁目 → ー丁目 (漢数字の一ではなく長音記号になっている)\n        - 日 → 目\n        - 見 → 貝\n        - 1 → l (L)\n        - O → 0\n- 文脈的妥当性によるもの\n    - 文脈に沿っているように見えるため、訂正が困難なパターン、LVLMでよく起きる\n    - 「1,000円」→「10,000円」\n    - 「株式会社OpenAI」→「有限会社OpenAI」\n- 光学的にも文脈的にもおかしい異常ケース\n    - 明らかに不自然な文字列が入るパターン、OCRでもLVLMでもたまに起きる\n    - 「本日ハ晴天ナリ」→「本日ハ食事ナリ」 (晴天 → 食事)\n    - 「本日はとても」→「本日はw%@とても」 (`w%@` が入ってしまった)\n\n## 校正支援方法について\n\nこうした認識ミスが発生してしまうのは避けられません。\n\nほとんどミスをしないモデルを構築できても一定割合はミスをするので、そのミスを許容できるシステムでない限り校正処理が必要になります。\n\n校正処理を支援するシステムには以下のようなものが表示されると嬉しさがあります。\n\n- 確信度を表示する\n- 根拠となる領域を明示する\n- 修正候補を提案する\n\nもちろん、自動で誤り訂正ができれば嬉しいです。基本的にポストプロセスに誤り訂正を入れたあと、人手による校正が入ります。\n\n### 誤り訂正\n\n基本的には文脈に適しているかを判断するアプローチと文字ごとに視覚的な形状のマッチングを行うアプローチがあります。seq2seqベースのアプローチもありましたが、LMMで生ずるハルシネーションには効果があまり期待できません。[arxiv:2308.15262v1](https://arxiv.org/abs/2308.15262) では、CharBERTとGlyph Embeddingを活用したモデルが、単語や文レベルでの誤り訂正において高い効果を示しています。字体に注目した視覚的特徴と文脈の両方を組み合わせるアプローチがわりとよさげです。\n\n## 複数のモデルからの出力を用いて校正の負担を軽減させる方法\n\n今回検討するのがこちらです。アンサンブル的な手法を用いてスコアを出します。\n仮にGPT-4o、Gemini 1.5 Pro 002、Google Document AIの抽出結果を要素ごとにうまくマッチングできれば、あとはレーベンシュタイン距離を見ればうまくできそうです。\n\nただし、マッチングどうする問題が難しい〜〜。\n[Florence2](https://huggingface.co/microsoft/Florence-2-large)のように座標を吐いてくれるわけではないので、うまくやる必要があります。\n\n### 座標出せませんか？\n\nということで、LMMに座標を出してほしいので以下のような構造を定義して吐き出させてみました。\n\n```python\nclass ExtractedText(BaseModel):\n    text: str = Field(..., description=\"The content of the extracted text.\")\n    coordinates: List[int] = Field(\n        ...,\n        description=(\n            \"Bounding box coordinates represented as a list of two points [x1, y1, x2, y2].\\n\"\n            \"- x1, y1: The top-left corner of the bounding box.\\n\"\n            \"- x2, y2: The bottom-right corner of the bounding box.\"\n        )\n    )\n\n\n\nclass OCRResultWithCoordinates(BaseModel):\n    extracted_texts: List[ExtractedText]\n```\n\n結果がこちら。解像度を与えると若干改善するものの、やっぱり難しいです。てか、ここまである程度うまくいくのやばいな。要素数が少なければあとはうまくやれそうですが、大量にあるときには難しいです。\n\n![kanashii.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/2c5ea18a-cadf-9e3f-1d9c-4ea5979c39fc.png)\n\n### 食べログさんのやつはどうか\n\nhttps://tech-blog.tabelog.com/entry/ai-menu-ocr\n\nこっちはめちゃくちゃ情報抽出にはいいやり方です。\n画像 + OCRで抽出された情報 (要素ごとにIDを振り、抽出された文字列と座標をセットにする) 渡して情報抽出タスクを解いています。\nちなみにLLMを用いた情報抽出処理はいろんなやり方があります。\n\n### (いい感じの情報抽出手法)\n\n- OCRテキストのみを渡して情報抽出 (画像は渡さない)\n    - テキストの精度はOCRの能力に依存\n- 画像のみを渡して情報抽出\n    - LMMの能力のみに依存\n- 画像 + OCRテキスト\n    - OCRの抽出したテキストの情報を使える (精度が上がる可能性もある)\n    - OCRの認識能力に引っ張られる可能性\n- 画像 + OCRの抽出要素に採番 + 頂点の座標\n    - ブログの提案手法\n    - LMMに正しく座標の意味が伝わらない可能性がある\n    - どこの領域のデータか対応関係を表示できる嬉しさがある\n- 画像 (OCRの要素に矩形 + 採番を描画) + OCRの抽出要素に採番\n    - 画像を上書きするので、正しくLMMが番号を読み取れなかったり、番号を描画する位置の決定が難しい\n\n### ゴリ押しマッチング\n\nここまで書いて寝て朝になりました。とりあえず `画像 (OCRの要素に矩形 + 採番を描画)` でやります。今回はLMMの独立したテキスト抽出能力が必要であるため、OCRから抽出されたテキストのデータは渡しません。なお、この手法は要素が混雑している画像には使えません (単純に採番が文字に被るので. 薄く描画したらワンチャンいけるかも)\n\n### 処理の流れ\n\n- OCRで要素抽出\n- 座標データをもとに画像にバウンディングボックスとID番号を書き込み\n- それぞれ抽出して要素ごとの候補群が出る\n- スコアを算出\n- cropされた画像、スコア、候補が表示される\n\n![temp_annotated.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/43704e60-7646-9bda-1818-3b5d85500d1f.png)\n\n\n### 実装\n\nAppendixを参照\n\n### 結果！\n\nいい感じにマッチングした！ (ちょっとまだ精度あれだけど、プロンプト次第で良くなりそう)\n\n- ![kakkohagomi.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/d50b6413-7253-0eca-304c-83ec9951ccc9.png)\n\nこんな感じで、意見が割れているところが濃くハイライトされます。\n\n![kenka_yarouze.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/27b243d5-d51f-5a57-0132-5503c867e1ad.png)\n\n## まとめ\n\n- 校正に時間が結構かかるのであればかなりありなアプローチかもしれない\n- OCRがんばっている人はえらい！\n\n## Appendix\n\n### 評価用データ\n\n今回作った以下の評価データは自由にお使い下さい。\n\n![image1.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/0a57fc75-c3c7-77ff-05f1-2358223d1e70.png)\n> 請求書 株式会社 Dark AI 御中 請求No. 2024110103 請求日 2024/11/01 件名： 社会奉仕活動 渋谷太郎 下記の通り、ご請求申し上げます。 〒150-0002 東京都渋谷区渋谷1-1-1 渋谷ヒルズ1001 TEL： 090-1919-1919 合計金額 ¥16,500 （税込） お支払期限： 2024/11/30 No. 摘要 数量 単価 金額 社会奉仕活動 50 時間 300 ¥15,000 小計 ¥15,000 8%消費税 ¥0 10%消費税 ¥1,500 合計 ¥16,500 お振込先 最高銀行 渋谷支店 普通 1129917 シブヤ　タロウ 備考\n\n![image2.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/97111993-5a41-9d21-de8b-0863ed4de827.png)\n> 栄養成分表示(100ml当たり)/エネルギー0kcalたんぱく質・脂質・炭水化物0g、ナトリウム1.1mg(食塩相当量0.003g)、カルシウム0.72mg、カリウム0.09mg、マグネシウム0.23mg\n\n![image3.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/22e40c63-b469-8098-a7dd-f68b888b973b.png)\n> Jupyterでトークナイズ データセット作りたい→何分かかる？ PDFデータなら「分かっている. 対象 PDF(テキスト入り) スキャンデータ 写真 傾き 手書き フォントへの頑健性 文字の明瞭化 誤り訂正アプローチ seq-to-seqベース レイアウト←どう定義するか 文字 レシート OCR template Matching LLMによる訂正→文脈として、大量に渡す 対象の画像に入ってそうな文字列を渡す\n\n![image4.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/5e252c2f-ea9b-b759-d600-cb371266c6ae.png)\n> 音羽山清水寺 第九十六 大吉 にわとりほうをおうておなじくとぶ 鶏逐鳳同飛 レ こうりんうぎをとゝのう 高林整羽儀 二 一 ふねにさおさしてすべからくきしにわたるべし 棹船須湾岸 レ レ レ ほうかふねにみちてかえらん 宝貸満船帰 レ ◯このみくじにあう人は、人を見くだし、軽んずることなく、万事つゝましやかにして、たとえ地位低き人たりとも大切にして、仕事に勉強すれば、家内の仕合せきわめてよろし◯望み事叶う◯病人次第に本ぷくすべし◯あらそいごと勝ちなり◯失せ物出ること遅し◯転居、ふしん、旅行ゆる〱してよし◯えんだん吉◯売買吉◯職業は金、土、木、紙に縁あるものよし◯あきないもよし◯子に賢く出世するものあり、大切に育つべし くゝりつけないでお持ち帰り下さい。\", # くの字点 (ゆる〱の『〱』)、平仮名繰返し記号 (ゝ) が含まれる\n\n![image5.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/2958603/a01502ce-adba-3385-8e8a-88c8ada2e44f.png)\n> ぼっち・ざ・ろっく！ BOCCHI THE ROCK! 2023 5.24 WED. RELEASE (私+君)-時間÷ギター= BOCCHI THE ROCK! 光の中へ 結束バンド NEW SINGLE 結束バンド 光の中へ 初回仕様 限定盤 ¥1,320(税込) svwc-70620 収録曲 1 光の中へ 2 青い春と西の空 3 光の中へ -instrumental- 4 青い春と西の空 -instrumental- 封入特典 結束バンドLIVE -恒星- バックステージパス風ステッカー 先着購入特典 ジャケットイラスト & ロゴステッカーシート\n\n### 評価用実装\n\n```python\nfrom pydantic import BaseModel\nfrom typing import List\nimport base64\nfrom collections import Counter\nimport re\nfrom google.api_core.client_options import ClientOptions\nfrom google.cloud import documentai\nimport openai\nimport MeCab\nfrom pdf2image import convert_from_path\nimport google.generativeai as genai\nimport json\n\n\nclass OCRResult(BaseModel):\n    extracted_texts: List[str]\n\n# サンプル画像と対応する正解テキスト\nsample_images = ['data/image1.png', 'data/image2.png', 'data/image3.png', 'data/image4.png', 'data/image5.png']\nground_truth_texts = [\n    \"請求書 株式会社 Dark AI 御中 請求No. 2024110103 請求日 2024/11/01 件名： 社会奉仕活動 渋谷太郎 下記の通り、ご請求申し上げます。 〒150-0002 東京都渋谷区渋谷1-1-1 渋谷ヒルズ1001 TEL： 090-1919-1919 合計金額 ¥16,500 （税込） お支払期限： 2024/11/30 No. 摘要 数量 単価 金額 社会奉仕活動 50 時間 300 ¥15,000 小計 ¥15,000 8%消費税 ¥0 10%消費税 ¥1,500 合計 ¥16,500 お振込先 最高銀行 渋谷支店 普通 1129917 シブヤ　タロウ 備考\",\n    \"栄養成分表示(100ml当たり)/エネルギー0kcalたんぱく質・脂質・炭水化物0g、ナトリウム1.1mg(食塩相当量0.003g)、カルシウム0.72mg、カリウム0.09mg、マグネシウム0.23mg\",\n    \"Jupyterでトークナイズ データセット作りたい→何分かかる？ PDFデータなら「分かっている. 対象 PDF(テキスト入り) スキャンデータ 写真 傾き 手書き フォントへの頑健性 文字の明瞭化 誤り訂正アプローチ seq-to-seqベース レイアウト←どう定義するか 文字 レシート OCR template Matching LLMによる訂正→文脈として、大量に渡す 対象の画像に入ってそうな文字列を渡す\",\n    \"音羽山清水寺 第九十六 大吉 にわとりほうをおうておなじくとぶ 鶏逐鳳同飛 レ こうりんうぎをとゝのう 高林整羽儀 二 一 ふねにさおさしてすべからくきしにわたるべし 棹船須湾岸 レ レ レ ほうかふねにみちてかえらん 宝貸満船帰 レ ◯このみくじにあう人は、人を見くだし、軽んずることなく、万事つゝましやかにして、たとえ地位低き人たりとも大切にして、仕事に勉強すれば、家内の仕合せきわめてよろし◯望み事叶う◯病人次第に本ぷくすべし◯あらそいごと勝ちなり◯失せ物出ること遅し◯転居、ふしん、旅行ゆる〱してよし◯えんだん吉◯売買吉◯職業は金、土、木、紙に縁あるものよし◯あきないもよし◯子に賢く出世するものあり、大切に育つべし くゝりつけないでお持ち帰り下さい。\", # くの字点 (ゆる〱の『〱』)、平仮名繰返し記号 (ゝ) が含まれる\n    \"ぼっち・ざ・ろっく！ BOCCHI THE ROCK! 2023 5.24 WED. RELEASE (私+君)-時間÷ギター= BOCCHI THE ROCK! 光の中へ 結束バンド NEW SINGLE 結束バンド 光の中へ 初回仕様 限定盤 ¥1,320(税込) svwc-70620 収録曲 1 光の中へ 2 青い春と西の空 3 光の中へ -instrumental- 4 青い春と西の空 -instrumental- 封入特典 結束バンドLIVE -恒星- バックステージパス風ステッカー 先着購入特典 ジャケットイラスト & ロゴステッカーシート\",\n]\n\ndef encode_image(image_path):\n    \"\"\"\n    画像ファイルをBase64エンコードする関数\n    \"\"\"\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\ndef tokenize_japanese(text: str) -> list:\n    \"\"\"\n    日本語のテキストを分かち書きし、単語のみをリストで返す関数\n    \"\"\"\n    # タブ, 改行, スペース, 全角スペースを削除\n    text = re.sub(r\"[ \\t\\n\\r\\u3000]+\", \"\", text)\n    tagger = MeCab.Tagger(r'-r /opt/homebrew/etc/mecabrc -d /opt/homebrew/lib/mecab/dic/ipadic')\n    parsed = tagger.parse(text)\n    tokens = []\n    for line in parsed.splitlines():\n        if line == \"EOS\" or line == \"\":\n            continue\n        surface = line.split(\"\\t\")[0]\n        tokens.append(surface)\n    \n    return tokens\n\ndef compute_f1_score(y_true: list, y_pred: list) -> tuple:\n    \"\"\"\n    Precision、Recall、F1スコアを計算する関数\n    \"\"\"\n    y_true_counter = Counter(y_true)\n    y_pred_counter = Counter(y_pred)\n\n    intersection = y_true_counter & y_pred_counter\n    true_positives = sum(intersection.values())\n\n    precision = true_positives / sum(y_pred_counter.values()) if y_pred_counter else 0\n    recall = true_positives / sum(y_true_counter.values()) if y_true_counter else 0\n\n    if precision + recall == 0:\n        f1 = 0.0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n\n    return precision, recall, f1\n\ndef extract_text_with_documentai(project_id: str, location: str,\n                                 processor_id: str, file_path: str,\n                                 mime_type: str) -> str:\n    \"\"\"\n    Google Document AIを使用してテキストを抽出する関数\n    \"\"\"\n    docai_client = documentai.DocumentProcessorServiceClient(\n        client_options=ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\", credentials_file=\"./key.json\")\n    )\n    resource_name = docai_client.processor_path(project_id, location, processor_id)\n\n    with open(file_path, \"rb\") as image:\n        image_content = image.read()\n    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n    request = documentai.ProcessRequest(name=resource_name, raw_document=raw_document)\n    result = docai_client.process_document(request=request)\n    document_object = result.document\n    return document_object.text\n\ndef extract_text_with_gpt4o(image_path: str, openai_api_key: str) -> str:\n    \"\"\"\n    gpt-4oを使用してテキストを抽出する関数\n    \"\"\"\n    openai_client = openai.Client(api_key=openai_api_key)\n    base64_image = encode_image(image_path)\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"あなたは非常に優秀なOCRです。画像から**すべて**の文字列を抽出してください。\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n                    },\n                },\n            ],\n        },\n    ]\n    response = openai_client.beta.chat.completions.parse(\n        model=\"gpt-4o\",\n        messages=messages,\n        response_format=OCRResult,\n    )\n    ocr_result = json.loads(response.choices[0].message.content)\n    return \"\".join(ocr_result['extracted_texts'])\n\ndef extract_text_with_gemini(image_path: str, gemini_api_key: str) -> str:\n    \"\"\"\n    gemini-1.5-pro-002を使用してテキストを抽出する関数\n    \"\"\"\n    genai.configure(api_key=gemini_api_key)\n    model = genai.GenerativeModel(model_name = \"gemini-1.5-pro-002\")\n    prompt = \"あなたは非常に優秀なOCRです。画像から**すべて**の文字列を抽出してください。\"\n    response = model.generate_content(\n        [{'mime_type':'image/png', 'data': encode_image(image_path)}, prompt],\n        generation_config=genai.GenerationConfig(\n            response_mime_type=\"application/json\",\n            response_schema = OCRResult,\n        ),\n        request_options={\"timeout\": 600},  # タイムアウト\n    )\n\n    ocr_result = json.loads(response.text)\n    return \"\".join(ocr_result['extracted_texts'])\n\nfrom PIL import Image\n\ndef display_image(image_path, title=None):\n    \"\"\"\n    指定した画像を表示する関数\n    \"\"\"\n    image = Image.open(image_path)\n    plt.figure(figsize=(8, 8))\n    plt.imshow(image)\n    plt.axis('off')  # 軸を非表示\n    if title:\n        plt.title(title, fontsize=16)\n    plt.show()\n\ndef main_process():\n    project_id = \"\"\n    location = \"\"\n    processor_id = \"\"\n\n    openai_api_key = \"\"\n    gemini_api_key = \"\"\n\n    # 評価結果の格納\n    evaluation_results = {\n        'Google Document AI': [],\n        'gpt-4o': [],\n        'gemini-1.5-pro-002': []\n    }\n\n    # 各画像に対する処理\n    for i, (image_path, ground_truth_text) in enumerate(zip(sample_images, ground_truth_texts)):\n        print(f\"Processing Image {i+1}: {image_path}\")\n        # plot image\n        display_image(image_path, title=f\"Image {i+1}\")\n\n        # 正解テキストのトークン化\n        y_true = tokenize_japanese(ground_truth_text)\n\n        # Google Document AIでテキスト抽出\n        mime_type = 'image/png'\n        document_text = extract_text_with_documentai(\n            project_id=project_id,\n            location=location,\n            processor_id=processor_id,\n            file_path=image_path,\n            mime_type=mime_type\n        )\n        print(document_text.replace(\"\\n\", \"\"))\n        y_pred_docai = tokenize_japanese(document_text)\n        precision_docai, recall_docai, f1_docai = compute_f1_score(y_true, y_pred_docai)\n        evaluation_results['Google Document AI'].append({\n            'precision': precision_docai,\n            'recall': recall_docai,\n            'f1': f1_docai\n        })\n\n        # gpt-4oでテキスト抽出\n        ocr_text_gpt4o = extract_text_with_gpt4o(image_path, openai_api_key)\n        print(ocr_text_gpt4o)\n        y_pred_gpt4o = tokenize_japanese(ocr_text_gpt4o)\n        precision_gpt4o, recall_gpt4o, f1_gpt4o = compute_f1_score(y_true, y_pred_gpt4o)\n        evaluation_results['gpt-4o'].append({\n            'precision': precision_gpt4o,\n            'recall': recall_gpt4o,\n            'f1': f1_gpt4o\n        })\n\n        # gemini-1.5-pro-002でテキスト抽出\n        ocr_text_gemini = extract_text_with_gemini(image_path, gemini_api_key)\n        print(ocr_text_gemini)\n        y_pred_gemini = tokenize_japanese(ocr_text_gemini)\n        precision_gemini, recall_gemini, f1_gemini = compute_f1_score(y_true, y_pred_gemini)\n        evaluation_results['gemini-1.5-pro-002'].append({\n            'precision': precision_gemini,\n            'recall': recall_gemini,\n            'f1': f1_gemini\n        })\n    return evaluation_results\n\n    # 結果の出力\n    for model_name, results in evaluation_results.items():\n        print(f\"\\nResults for {model_name}:\")\n        total_precision = 0\n        total_recall = 0\n        total_f1 = 0\n        for i, res in enumerate(results):\n            print(f\"Image {i+1}: Precision={res['precision']:.4f}, Recall={res['recall']:.4f}, F1={res['f1']:.4f}\")\n            total_precision += res['precision']\n            total_recall += res['recall']\n            total_f1 += res['f1']\n        avg_precision = total_precision / len(results)\n        avg_recall = total_recall / len(results)\n        avg_f1 = total_f1 / len(results)\n        print(f\"Average: Precision={avg_precision:.4f}, Recall={avg_recall:.4f}, F1={avg_f1:.4f}\")\n```\n\n### 検証用Gradio実装\n\n```python\nfrom typing import List, Dict\nfrom pydantic import BaseModel, Field\nimport gradio as gr\nimport base64\nimport cv2\nimport numpy as np\nfrom google.cloud import documentai\nfrom google.api_core.client_options import ClientOptions\nimport openai\nimport google.generativeai as genai\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport Levenshtein\nfrom dataclasses import dataclass, field\n\nclass ExtractedElement(BaseModel):\n    id: int = Field(..., description=\"ID number of the extracted element\")\n    text: str = Field(..., description=\"The extracted text content\")\n\nclass OCRResult(BaseModel):\n    extracted_elements: List[ExtractedElement]\n\n@dataclass\nclass DocumentElement:\n    text: str\n    bbox: List[Dict[str, int]]  # [{\"x\": x1, \"y\": y1}, ..., {\"x\": x4, \"y\": y4}]\n    id: int\n    candidates: List[str] = field(default_factory=list)\n    confidence_score: float = 0.0\n\ndef encode_image(image_path: str) -> str:\n    with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n\ndef extract_text_with_documentai(project_id: str, location: str, processor_id: str,\n                                 file_path: str, mime_type: str) -> List[DocumentElement]:\n\n    docai_client = documentai.DocumentProcessorServiceClient(\n        client_options=ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\", credentials_file=\"./key.json\")\n    )\n\n    resource_name = docai_client.processor_path(project_id, location, processor_id)\n\n    with open(file_path, \"rb\") as image:\n        image_content = image.read()\n\n    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n\n    request = documentai.ProcessRequest(name=resource_name, raw_document=raw_document)\n\n    result = docai_client.process_document(request=request)\n\n    document = result.document\n    elements = []\n    page = document.pages[0]\n    doc_text = document.text\n\n    for i, token in enumerate(page.paragraphs):  # token or paragraph\n        layout = token.layout\n\n        # テキストを取得\n        if i == 0:\n            text = doc_text[:token.layout.text_anchor.text_segments[0].end_index]\n        else:\n            segment = token.layout.text_anchor.text_segments[0]\n            text = doc_text[segment.start_index:segment.end_index]\n\n        bbox = [{\"x\": vertex.x, \"y\": vertex.y} for vertex in layout.bounding_poly.vertices]\n\n        elements.append(DocumentElement(\n            text=text.strip(),\n            bbox=bbox,\n            id=i,\n            candidates=[text.strip()]\n        ))\n\n    return elements\n\ndef draw_boxes_and_ids(image_path: str, elements: List[DocumentElement]) -> str:\n    image = Image.open(image_path)\n    draw = ImageDraw.Draw(image)\n\n    try:\n        font = ImageFont.truetype(\"fonts/ipagp.ttf\", 20)\n    except:\n        font = ImageFont.load_default()\n\n    for element in elements:\n        polygon = [(vertex[\"x\"], vertex[\"y\"]) for vertex in element.bbox]\n        draw.polygon(polygon, outline=\"red\", width=2)\n\n        x1, y1 = polygon[0]\n        draw.text((x1 - 20, y1 - 20), str(element.id), fill=\"red\", font=font)\n\n    temp_path = \"temp_annotated.png\"\n    image.save(temp_path)\n    return temp_path\n\ndef highlight_elements_by_score(image_path: str, elements: List[DocumentElement]) -> str:\n    image = Image.open(image_path).convert(\"RGBA\")\n    overlay = Image.new('RGBA', image.size, (0, 0, 0, 0))\n    draw = ImageDraw.Draw(overlay)\n\n    for element in elements:\n        score = element.confidence_score\n        color_intensity = int((1 - score) * 255)\n        fill_color = (255, 255 - color_intensity, 255 - color_intensity, 100)\n\n        polygon = [(vertex[\"x\"], vertex[\"y\"]) for vertex in element.bbox]\n        draw.polygon(polygon, fill=fill_color)\n\n    combined = Image.alpha_composite(image, overlay)\n    temp_path = \"temp_highlighted.png\"\n    combined.convert(\"RGB\").save(temp_path)\n    return temp_path\n\n\n\ndef extract_text_with_gpt4o(image_path: str, openai_api_key: str) -> List[ExtractedElement]:\n    client = openai.Client(api_key=openai_api_key)\n    base64_image = encode_image(image_path)\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"画像から要素を抽出し、各要素のIDと文字列を抽出してください。\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/png;base64,{base64_image}\"\n                    }\n                }\n            ]\n        }\n    ]\n\n    response = client.beta.chat.completions.parse(\n        model=\"gpt-4o\",\n        messages=messages,\n        response_format=OCRResult\n    )\n\n    result = OCRResult.parse_raw(response.choices[0].message.content) # 一旦model_validateは使わず\n    return result.extracted_elements\n\ndef extract_text_with_gemini(image_path: str, gemini_api_key: str) -> List[ExtractedElement]:\n    genai.configure(api_key=gemini_api_key)\n    model = genai.GenerativeModel('gemini-1.5-pro-002')\n\n    prompt = \"画像から要素を抽出し、各要素のIDと文字列を抽出してください。\"\n\n    response = model.generate_content(\n        [\n            {'mime_type': 'image/png', 'data': encode_image(image_path)},\n            prompt\n        ],\n        generation_config=genai.GenerationConfig(\n            response_mime_type=\"application/json\",\n            response_schema=OCRResult\n        )\n    )\n\n    result = OCRResult.parse_raw(response.text)\n    return result.extracted_elements\n\ndef calculate_confidence_score(candidates: List[str]) -> float:\n    if not candidates:\n        return 0.0\n\n    total_distance = 0\n    comparisons = 0\n\n    for i in range(len(candidates)):\n        for j in range(i + 1, len(candidates)):\n            distance = Levenshtein.distance(candidates[i], candidates[j])\n            max_length = max(len(candidates[i]), len(candidates[j]))\n            normalized_distance = 1 - (distance / max_length) if max_length > 0 else 0\n            total_distance += normalized_distance\n            comparisons += 1\n\n    return total_distance / comparisons if comparisons > 0 else 0.0\n\ndef crop_image(image_path: str, bbox: List[Dict[str, int]]) -> Image:\n    image = Image.open(image_path)\n    xs = [vertex[\"x\"] for vertex in bbox]\n    ys = [vertex[\"y\"] for vertex in bbox]\n    min_x, max_x = min(xs), max(xs)\n    min_y, max_y = min(ys), max(ys)\n    cropped = image.crop((min_x, min_y, max_x, max_y))\n    return cropped\n\ndef process_image(image_path: str, project_id: str, location: str, processor_id: str,\n                  openai_api_key: str, gemini_api_key: str):\n    # OCR\n    elements = extract_text_with_documentai(\n        project_id, location, processor_id, image_path, \"image/png\"\n    )\n    # バウンディングボックスとIDを描画\n    annotated_image_path = draw_boxes_and_ids(image_path, elements)\n\n    # gpt-4oとgeminiでもテキスト抽出\n    gpt4_elements = extract_text_with_gpt4o(annotated_image_path, openai_api_key)\n    gemini_elements = extract_text_with_gemini(annotated_image_path, gemini_api_key)\n\n    for element in elements:\n        element.candidates = [element.text]\n\n        for gpt4_elem in gpt4_elements:\n            if gpt4_elem.id == element.id:\n                element.candidates.append(gpt4_elem.text)\n                break\n\n        for gemini_elem in gemini_elements:\n            if gemini_elem.id == element.id:\n                element.candidates.append(gemini_elem.text)\n                break\n\n        element.confidence_score = calculate_confidence_score(element.candidates)\n\n    # ハイライト画像を作成\n    highlighted_image_path = highlight_elements_by_score(image_path, elements)\n\n    output_elements = []\n\n    # 言語モデルに入力したバウンディングボックス付き画像を追加\n    annotated_image = Image.open(annotated_image_path)\n    output_elements.append([annotated_image, \"バウンディングボックス付き画像\"])\n\n    # スコアでハイライトされた画像を追加\n    highlighted_image = Image.open(highlighted_image_path)\n    output_elements.append([highlighted_image, \"スコアでハイライトされた画像\"])\n\n    # 各要素のクロップ画像とキャプションを追加\n    for element in elements:\n        cropped_image = crop_image(image_path, element.bbox)\n        caption = f\"ID: {element.id}, スコア: {element.confidence_score:.2f}, 候補: \" + \", \".join(element.candidates)\n        output_elements.append([cropped_image, caption])\n\n    # テーブルデータを作成\n    table_data = []\n    for element in elements:\n        table_data.append([\n            element.id,\n            round(element.confidence_score, 2),\n            \", \".join(element.candidates)\n        ])\n\n    return output_elements, table_data\n\ndef create_gradio_interface():\n    with gr.Blocks() as interface:\n        gr.Markdown(\"## 確信度つきOCR\")\n\n        with gr.Row():\n            input_image = gr.Image(type=\"filepath\", label=\"Input Image\")\n\n        with gr.Row():\n            submit_btn = gr.Button(\"画像を処理\")\n\n        output_gallery = gr.Gallery(\n            label=\"結果\",\n            show_label=True,\n            elem_id=\"gallery\",\n            columns=3,\n            height=\"auto\"\n        )\n\n        output_table = gr.Dataframe(\n            headers=[\"ID\", \"スコア\", \"候補\"],\n            label=\"ID、スコア、候補の一覧\",\n            datatype=[\"number\", \"number\", \"str\"],\n            wrap=True\n        )\n\n        def process_and_return(img):\n            outputs, table = process_image(\n                img,\n                project_id=\"\",\n                location=\"\",\n                processor_id=\"\",\n                openai_api_key=\"\",\n                gemini_api_key=\"\"\n            )\n            return outputs, table\n\n        submit_btn.click(\n            fn=process_and_return,\n            inputs=[input_image],\n            outputs=[output_gallery, output_table]\n        )\n\n    return interface\n\nif __name__ == \"__main__\":\n    interface = create_gradio_interface()\n    interface.launch()\n\n```\n",
      "coediting": false,
      "comments_count": 0,
      "created_at": "2024-12-05T07:58:40+09:00",
      "group": null,
      "id": "c7173648aea662451dbd",
      "likes_count": 50,
      "private": false,
      "reactions_count": 0,
      "stocks_count": 27,
      "tags": [
        {
          "name": "OCR",
          "versions": []
        },
        {
          "name": "LMM",
          "versions": []
        },
        {
          "name": "GPT-4o",
          "versions": []
        }
      ],
      "title": "確信度を出してくれるOCRを作ってみる！",
      "updated_at": "2024-12-31T11:29:52+09:00",
      "url": "https://qiita.com/sakasegawa/items/c7173648aea662451dbd",
      "user": {
        "description": "",
        "facebook_id": "",
        "followees_count": 2,
        "followers_count": 2831,
        "github_login_name": null,
        "id": "sakasegawa",
        "items_count": 43,
        "linkedin_id": "",
        "location": "",
        "name": "",
        "organization": "",
        "permanent_id": 2958603,
        "profile_image_url": "https://s3-ap-northeast-1.amazonaws.com/qiita-image-store/0/2958603/cdfed59e44e5d2598edbd97db569f0ad715bdf7d/x_large.png?1667890567",
        "team_only": false,
        "twitter_screen_name": "gyakuse",
        "website_url": ""
      },
      "page_views_count": null,
      "team_membership": null,
      "organization_url_name": null,
      "slide": false,
      "semantic_similarity": 0.8220961689949036,
      "quality_score": 22,
      "python_code_score": 6,
      "python_code_blocks": 3
    }
  ]
}